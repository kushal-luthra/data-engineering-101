{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Data Engineering - 101","text":""},{"location":"#index","title":"Index","text":"<ul> <li>Concepts<ul> <li>Data Warehousing Concepts</li> <li>ELK</li> <li>Database Vs Datawarehouse Vs Datalake | ETL Vs ELT</li> </ul> </li> <li>OOPs concepts<ul> <li>OOPs in Python </li> </ul> </li> <li>SQL<ul> <li>SQL Performance Tuning Page</li> <li>SQL Performance Tuning Summary</li> <li>Common Table Expressions (CTEs)</li> <li>SQL Analytical Functions</li> <li>SQL Practise Questions</li> </ul> </li> <li>Storage Layer <ul> <li>Row-based Vs Column-based File Formats </li> <li>Text-based File Formats </li> <li>Big Data File Formats </li> <li>File Compression Techniques in Big Data Systems</li> </ul> </li> <li>PyTest overview <ul> <li>PyTest overview </li> </ul> </li> <li>SOLID Principles <ul> <li>Introduction to SOLID Principles </li> <li>Single Responsibility Principle </li> <li>Open Closed Principle </li> <li>Liskov Substitution Principle </li> <li>Interface Seggregation Principle </li> </ul> </li> <li>MLOps <ul> <li>Challenges and Evolution of Machine Learning  <ul> <li>Introduction to Machine Learning </li> <li>Benefits of Machine Learning </li> <li>MLOps Fundamentals </li> <li>DevOps and DataOps Fundamentals </li> </ul> </li> <li>MLOps Fundamentals  <ul> <li>Problems that MLOps solves </li> <li>MLOps Components </li> <li>MLOps Toolbox </li> </ul> </li> <li>MLOps Stages <ul> <li>MLOps Stages</li> </ul> </li> </ul> </li> <li>Productivity Hacks<ul> <li>Cold Mail Tactics</li> <li>LinkedIn Growth</li> <li>Top 10 hacks to be a Bad developer</li> <li>Jeff Bezos on Amazon Leadership principles</li> <li>About Me</li> </ul> </li> <li>New Rules of the Game <ul> <li>New Rules of the Game - an Introduction</li> <li>New Rules of Work</li> <li>New Rules of Learning</li> <li>New Rules of Life Author : Kushal Luthra </li> </ul> </li> </ul>"},{"location":"aboutme/","title":"About Me","text":"<p>Kushal is currently Lead Engineer at Airtel Africa Digital Labs (BigData &amp; Analytics Team). He is the Lead for Business Intelligence product, which is being built from scratch, and is aimed to be scalable (handles data that can be in Gigabytes and Terabytes) and replicable across 14 OpCos.</p> <p>Efficient team leader and mentor with a history of managing multiple squads.  </p> <p>Expert in providing data driven strategic approach and creating reusable and scalable models.  </p> <p>Building strategic assets to set up a long term sustainable operational and capability model in technology strategy arena. </p> <p>He has extensive experience that spans across various technologies, including Python, Spark(PySpark), SQL, Hive, Hadoop, Apache Hudi, Airflow, Sqoop, Gitlab, BItBucket, CICD. </p> <p>He also has experience of migrating Data Solutions on legacy system to Big Data Stack. He has built data pipelines from scratch, with focus on data frameworks.</p> <p>Domain : Retail and Telecom Distributed Computing: Hadoop, HDFS, Yarn, Spark Programming Languages: Python, Scala Operating System: Linux, Unix Development Tools: JIRA Databases: Postgres, MongoDB, Oracle Exadata Methodologies: Agile/Scrum</p> <p>Open to hearing about exciting information/opportunities in meaningful industries, more tech connections and both mentor/mentee relationships.</p>"},{"location":"MLOPs/1%20-%20Challenges%20and%20Evolution%20of%20ML/1%20-%20Introduction%20to%20ML/","title":"Introduction to Machine Learning","text":""},{"location":"MLOPs/1%20-%20Challenges%20and%20Evolution%20of%20ML/1%20-%20Introduction%20to%20ML/#introduction-to-machine-learning","title":"Introduction to Machine Learning","text":"<p> As we already know, artificial intelligence has evolved considerably over the last few years.  It started with the field of artificial intelligence in which the first more rudimentary artificial intelligence models were developed.  It was between the 1950s-1980s, the field of machine learning emerged.  That's when supervised and unsupervised learning machine learning models are developed.  In addition, throughout these years is when machine learning gained strength and began to be applied.   Finally, deep learning was developed in 2010.  Deep learning is a type of machine learning that trains a computer to perform tasks as humans do.  These tasks include speech recognition, image identification or video analysis.   At this time, machine learning was booming and applied in different fields.  However, during all these years, some challenges have persisted.   According to a survey by algorithmic, 85% of models fail to deploy to production.  - 55% of companies have never implemented a machine learning model.  - These problems have caused 85% of machine learning projects to fail.   Some main reasons are the lack of talent, the lack of processes that can manage change, and the absence of automated systems.  It is well known that there is a shortage of people trained in this field.  A single data scientist can take a few weeks to develop a model.  However, putting the same model into production can take months of work for a team of developers.  Why? Because model must be scalable, reproducible and collaborative to be deployed in production.   You have to package the developments, establish the dependencies between the libraries, add parameters, execute scripts, and automate this process.  It is also necessary to scale it so that it can meet the demands of users.   To do this, you must add a load balancer, partition the data and adjust different model configurations.   You also have to parallelize the model's training to support CPU, especially when we want to train the model with large amounts of data.   Finally, you have to monitor the results and make a record.   Thanks to this, we ensure that our model continues to work correctly over time.  It is also necessary to generate different versions of the model.   Finally, you have to automate the process.  You have to create workflows and make it possible to update the models and many other transformations.   How is all this done?  Thanks to the MLOps and new trend has appeared in recent years.  </p>"},{"location":"MLOPs/1%20-%20Challenges%20and%20Evolution%20of%20ML/1%20-%20Introduction%20to%20ML/#mlops-as-an-answer-to-challenges-of-ml","title":"MLOPs as an answer to challenges of ML","text":"<p>Here's Gartner's Trend Graph.  This graph analyzes all the trends that have appeared related to the field of data.  If we look closely at this curve, we can identify the trend of MLOps.   MLOps was born to streamline the process of putting our machine learning models into production. </p>"},{"location":"MLOPs/1%20-%20Challenges%20and%20Evolution%20of%20ML/2%20-%20Benefits%20of%20ML/","title":"Benefits of Machine Learning","text":"<p>Despite the difficulties faced by machine learning, the benefits are considerable.  According to multiple surveys, organizations that put machine learning models into production experienced a profit margin increase of 3 to 15%.   Here we can see the evolution of this benefit.   We see that around 2020 or 2019, the benefit was $23 billion.  However, between 2024 and 2025, we will see an exponential increase in profit.  It is due to the rapid adoption of machine learning that unlocks will enable. </p>"},{"location":"MLOPs/1%20-%20Challenges%20and%20Evolution%20of%20ML/3%20-%20MLOPs%20Fundamentals/","title":"MLOps Fundamentals","text":""},{"location":"MLOPs/1%20-%20Challenges%20and%20Evolution%20of%20ML/3%20-%20MLOPs%20Fundamentals/#what-is-mlops","title":"What is MLOPs?","text":"<p>  In the traditional view of model development, we only focused on model training. With the MLOPs, other components such as data collection, variable transformation, data validation, model management or service infrastructure have more importance.  </p>"},{"location":"MLOPs/1%20-%20Challenges%20and%20Evolution%20of%20ML/3%20-%20MLOPs%20Fundamentals/#mlops-process","title":"MLOPs Process","text":"<p>Developing a model using MLOPs follows this process.   It has 5 main parts.   The first one is the discovery of use cases. It means discovering the most critical aspects of the business and identifying its needs.  Then there would be data engineering, pipeline development, production, startup and monitoring.   These five phases are composed of different sub-parts.   For example, in the use case discovery, we have business understanding which seeks to understand the  needs of the business.  The use case identification seeks to identify different ways to solve the needs of the business.  Data Understanding analyzes the available data and identifies what other data needs to be collected.  The part of the feasibility study where it is determined if with the data you have, it is possible to develop the models.   Then we would move on to the part that is most commonly known as Model Development.  Here, the data is transformed and cleaned.  Different algorithms are trained and the best algorithm is identified and optimized.  Then its performance is evaluated and finally put into production.   In the last phase, we would put the model into production.  To do this, we would have to automate the deployment of the model.   We would also have to monitor the performance of the model. If it starts to give worse results, we could retrain it and redeploy it. </p>"},{"location":"MLOPs/1%20-%20Challenges%20and%20Evolution%20of%20ML/4%20-%20DevOps%20and%20DataOps%20Fundamentals/","title":"DevOps and DataOps Fundamentals","text":""},{"location":"MLOPs/1%20-%20Challenges%20and%20Evolution%20of%20ML/4%20-%20DevOps%20and%20DataOps%20Fundamentals/#devops-and-dataops","title":"DevOps and DataOps","text":"<p>DevOps is an iterative approach to deploying software applications in production, and MLOps borrows the same principles from DevOps by applying them to machine learning and ML.   MLOps allows machine learning models to be brought to production iteratively.   Whether it's DevOps or MLOps, the ultimate goal is the same - greater quality and control of software applications and machine learning models sent to production.   Not only have we applied the term DevOps to machine learning, but we have also applied it to data.   The DevOps applied to the data is what is known as DataOps.  DataOps involves a set of rules that ensure the availability of high quality data for model training.   On the one hand, we have the part of machine learning.  In this part, we not only develop the different models, but also the acquisition of data, understanding business needs and developing initial models.   Then we would have the DevOps part.  This part applies continuous testing, integration and development.  Continuous integration and continuous deployment are practices for automating integration and deployment.   Let's say we have completed the development of our machine learning model and have all our code in a version control repository because we work as a team, many people work on different parts of the code.  We must integrate all changes somehow and implement the latest code in all our development and production environments.  It is when we apply continuous integration and continuous deployment.   Finally, we would have the part of ops related to operate.  In this part, we have the continuous delivery, the data feedback loop and the monitoring of the model.   The data feedback loop helps the model learn from its mistakes by returning incorrectly predicted data.  Without this, a machine learning model tends to worsen its metrics over time, as it does not adapt to changes in the data.   As for monitoring, it is just as important in software development as it is in data science.  We need to monitor the model to keep users satisfied constantly.  </p>"},{"location":"MLOPs/1%20-%20Challenges%20and%20Evolution%20of%20ML/4%20-%20DevOps%20and%20DataOps%20Fundamentals/#roles-in-mlops","title":"Roles in MLOps","text":"<p>  When we applied all the MLOPs methodologies, we went from having a single role, which would be the data scientist to have many more roles with MOPS.   The data scientist would worry about training the model and knowing why the model is not giving good results.   Then we would have the data science manager.  He is in charge of evaluating if it is time to retrain the model or if it needs additional changes.   The product manager is responsible for identifying the customer's needs and publicizing the model's limitations.   The data engineer is responsible for obtaining the data from different sources, extracting them and correcting them.   Then we would have the quality manager (Compliance).   The quality manager is the one who makes sure that the model we are using is suitable and safe.   The support part is responsible for helping the business to apply this model.   The business stakeholder is responsible for valuing the model in the company.   Finally, the user utilizes the model to obtain predictions.  Also, he will compare the model's results with his knowledge of the business to know if it is reliable. </p>"},{"location":"MLOPs/2%20-%20MLOps%20Fundamentals/%201%20-%20Problems%20that%20MLOps%20solves/","title":"Problems that MLOps solves","text":""},{"location":"MLOPs/2%20-%20MLOps%20Fundamentals/%201%20-%20Problems%20that%20MLOps%20solves/#challenges-addressed-by-mlops","title":"Challenges addressed by MLOps","text":""},{"location":"MLOPs/2%20-%20MLOps%20Fundamentals/%201%20-%20Problems%20that%20MLOps%20solves/#versioning","title":"Versioning","text":"<p>The tools and techniques defined in MLOps make data scientists work easier.  On the one hand, it addresses the issue of versioning.  Version control is common in software engineering with tools like GitHub or Git.  In addition to code control, you must control other things like model data and artifacts.  Model and data versioning ensures that experiments are reproducible, and we can replicate the same results across different iterations.  </p>"},{"location":"MLOPs/2%20-%20MLOps%20Fundamentals/%201%20-%20Problems%20that%20MLOps%20solves/#model-tracking","title":"Model Tracking","text":"<p>Another problem it solves is the monitoring of the performance of the model models, and production can degrade over time.  It is due to the differences between training data and test data.  It is known as Data Drift.  If we monitor the performance of a model, we can identify and solve those problems.  </p>"},{"location":"MLOPs/2%20-%20MLOps%20Fundamentals/%201%20-%20Problems%20that%20MLOps%20solves/#feature-generation-feature-engineering","title":"Feature Generation / Feature Engineering","text":"<p>Finally, the last problem that ML solves is that of feature engineering.  Feature engineering can be a time-consuming task and computing resources.  The MLOps application allows functions to be generated only once and can be used as many times as needed.  It frees up the data scientists to focus on designing and testing the model. </p>"},{"location":"MLOPs/2%20-%20MLOps%20Fundamentals/2%20-%20MLOps%20Components/","title":"MLOps Components","text":""},{"location":"MLOPs/2%20-%20MLOps%20Fundamentals/2%20-%20MLOps%20Components/#parts-of-mlops","title":"Parts of MLOps","text":"<p>Here we can see where each of the parts of MLOps is located.  We start with the value proposition, then data versioning, data analysis and experimentation, code repository, pipeline orchestration, model registration and versioning, production deployment, model serving and finally model and application monitoring.   In the value proposition, we will identify different machine learning use cases.   Then we identify the available data sources, analyze them version, and register the data sets.  Once we have done this, we move on to the development and training of the model.  Here, the feature stores are generated, the model is registered and versioned and the metadata is stored.  Finally, the application is generated and monitored in the production startup part.  </p> <p> </p> <p> Even though the ML ops field is quite broad, we will look at the most critical parts.  </p>"},{"location":"MLOPs/2%20-%20MLOps%20Fundamentals/2%20-%20MLOps%20Components/#feature-store","title":"Feature Store","text":"<p>The feature store is a critical part of ML ops.  We must store the functions used in the training of a model.  Ensures that there is no duplication in the creation of functions.  Features can also be searched and used to build other models or analyze the data.  Functions are also versioned.  It ensures that you can revert to a previous version of the feature.  </p>"},{"location":"MLOPs/2%20-%20MLOps%20Fundamentals/2%20-%20MLOps%20Components/#data-versioning","title":"Data Versioning","text":"<p>Data versioning ensures reproducibility in model creation.  In addition to functions, you can also store the data set used to train a model.  It is also essential during the audit to identify the data sets used.  </p>"},{"location":"MLOPs/2%20-%20MLOps%20Fundamentals/2%20-%20MLOps%20Components/#metadata-store","title":"Metadata Store","text":"<p>Another essential part is the metadata store.  In machine learning, it is vital to store the configuration and metadata of the model.  It is essential to increase reproducibility.  Some artifacts that we have to record is the random seed used to split the data.  We can store other metadata, for example, model evaluation metrics, hyper parameters or configuration files.  </p>"},{"location":"MLOPs/2%20-%20MLOps%20Fundamentals/2%20-%20MLOps%20Components/#model-versioning","title":"Model Versioning","text":"<p>Model version control is vital because it lets you switch between models in real time.  Apart from that, multiple models can be used to monitor performance.  Version control is also critical from the governance point of view of model and compliance.  </p>"},{"location":"MLOPs/2%20-%20MLOps%20Fundamentals/2%20-%20MLOps%20Components/#model-registration","title":"Model Registration","text":"<p>Model registration should not be mistaken for model versioning.  Once we have trained the model, we should store it as a new record.  Each model in the registry will have a version.  In addition, each model is stored with its hyper parameters, metrics version of functions used and version of the training data set.  </p>"},{"location":"MLOPs/2%20-%20MLOps%20Fundamentals/2%20-%20MLOps%20Components/#model-serving","title":"Model serving","text":"<p>Once the model is versioned and registered, we will move on to the model service part.  </p> <p>It is nothing more than deploying the model to serve users.  When we create model consumption endpoints, we can use those endpoints to get predictions.  A model container could also integrate an application.  However, if we want to use the model in several applications simultaneously, it is better to use endpoints with APIs.   Another alternative is to deploy the models on edge devices with tiny machine learning.  </p>"},{"location":"MLOPs/2%20-%20MLOps%20Fundamentals/2%20-%20MLOps%20Components/#model-monitoring","title":"Model Monitoring","text":"<p>We should monitor models for possible deviations and biases.  The deviation occurs when the statistical characteristics between training data and new data change unexpectedly.  Therefore, the performance of the model degrades.  We can detect these problems early by monitoring the statistical properties of both training and prediction data.  In addition, production bias occurs when the deployed model has a different performance than the local model.   Errors can be caused during the training process, service errors or discrepancies in the data.  </p>"},{"location":"MLOPs/2%20-%20MLOps%20Fundamentals/2%20-%20MLOps%20Components/#model-recycling-retraining","title":"Model Recycling / Retraining","text":"<p>Another part of ML ops would be model recycling, also known as model retraining.  Models can be retrained for two reasons.  The first reason is to improve performance.  The second reason is when new training data is available.  </p>"},{"location":"MLOPs/2%20-%20MLOps%20Fundamentals/2%20-%20MLOps%20Components/#cicd","title":"CI/CD","text":"<p>If we detect newly available data, it should trigger model retraining.  It is where continuous integration and implementation come in.  Continuous integration and continuous deployment ensure that models are built and deployed frequently.  Continuous delivery ensures that code is frequently merged into a central repository where automated builds and tests are applied in machine learning.  This would involve testing the code and the resulting models.   It also involves creating a container of the models so that users can make use of that model. </p>"},{"location":"MLOPs/2%20-%20MLOps%20Fundamentals/3%20-%20MLOps%20Toolbox/","title":"MLOps Toolbox","text":"<p>MLOps Toolbox</p> <p></p> <p></p> <p>We will learn about the essential MLOps libraries in the data exploration and model training part. </p> <p>We can use different Development Tools such as Jupyter Notebooks, PyCharm or Visual Studio. These are some of the most used.  Some other libraries are no longer used. For example Spyder.   We will usually use GitHub, GitLab or Bitbucket for the Versioning of the code.   For Data Labeling, we will use tools such as V7 or Label Box.  V7 is a powerful automated annotation platform for computer vision.  V7 combines dataset management, image annotation, video annotation and AutoML model training to complete labeling tasks automatically.   For the Visualization and Exploration of data, we can use visualization libraries such as bokeh, matplotlib or Seaborn.   As for Feature Engineering, this will depend on the type of model and the programming languages we use.  Usually we use the scikit-learn Library to perform feature engineering with Python, but we also have other libraries such as the featuretools or the feast library.   Once it is done, we will move on to Model Training.  The conventional libraries for training models are scikit-learn for Machine Learning and TensorFlow and Keras for deep learning.   Once the model is developed, we will move on to Model Debugging.  For this we can use TensorBoard, interpretML or SHAP.    We would use libraries such as Optima, Tune or KerasTuner to optimize this model.   In ML ops, it is vital to register the models.  Therefore, we would go on to perform the Model Track.  One of the best known and newest book stores is ML Flow, but we also have others like Comet, ClearML and TensorBoard.   Once we have tracked the experiment and the versioning, we will move on to the Model Packaging.  Here we can use Kubeflow, mlflow, ONNX or BentoML.   ONNX provides an open source format for AI models, both deep learning and traditional ML enabling interoperability between different frameworks.   Once the model was packaged, we would go to model serving.  Model serving seeks to make the model accessible so the user can use it.  We can use Amazon Sage Maker, FastAPI or BentoML.   Once we have deployed a model in production, we must orchestrate the workflow.  To do this we can use Apache airflow, ZenML or Kubeflow.   For the monitoring of the model, we can use Seldon or Verta.  To ensure that the model continues to work over time, we must monitor it.   As we had mentioned, it is not enough to version the model.  You also have to version the data.  The most commonly used library for data versioning is DVC.  We also have the bookstores of Pachyderm, Quilt and Comet, among others.   Finally, we may want to develop a model for an application process.  In this case, we can resort to streamlit or Dash.  Both tools allow you to create machine learning web applications in Python with few lines of code.   Once this is finished, we also have the option to develop a POC of an application.  It can be developed quickly with a streamlit library, a library specially designed for science and data to generate visual applications with very little code and integrates seamlessly with Python. </p>"},{"location":"MLOPs/3%20-%20MLOps%20Stages/1%20-%20MLOps%20Stages/","title":"MLOps Stages","text":""},{"location":"MLOPs/3%20-%20MLOps%20Stages/1%20-%20MLOps%20Stages/#mlops-stages","title":"MLOps Stages","text":"<p>Here we will explain the different stages we must follow to complete the MLW Ops cycle.   Here we have a global vision of all those stages.   We would start in the first stage by implementing version control for the model and the data.   Next, we will start automating the process of machine learning model development with auto machine learning tools.  Also in this stage, we maintain version control of the model data and code.   In the third stage, we will add model serving.  Model serving could be done through an API, web or mobile application.   Finally, in the final stage, we would maintain the above components and include the models monitoring, governance and retraining.  It will ensure that the production model continues to function correctly over time.  </p>"},{"location":"MLOPs/3%20-%20MLOps%20Stages/1%20-%20MLOps%20Stages/#stage-1-data-collection-and-preparation","title":"Stage 1: Data Collection and Preparation","text":"<p>  The first step consists of data collection and preparation.   As we already know, there is no machine learning without data.  The part of feature engineering and data cleaning is one of the most relevant phases when training a model data processing is required.   Typically, data science teams need access to different historical data.  This data provides from different data sources and different formats.   Therefore, it is necessary to catalog and organize the data.   In addition, we cannot use raw data to train a model.   Also, developing individual processing for each model on the same data makes no sense.  On the contrary, developing a catalog and feature storage is necessary if we use identical data for different models.  We have different solutions for this feature storage.   Thanks to feature store data transformation, functions can be processed automatically without manual intervention.  Also, we will define data collection and transformations only once.   Finally, features will be stored in a shared catalog.   Above are some of the most important libraries to develop a feature store.  The most used is Feast.  DVC is also commonly used, although it also supports model registration and model versioning.  </p>"},{"location":"MLOPs/3%20-%20MLOps%20Stages/1%20-%20MLOps%20Stages/#stage-2-automated-development","title":"Stage 2: Automated Development","text":"<p>  For this stage, we will implement the automation of developments.   Generally, the development of machine learning models follows the same process.  - We start with the exploratory analysis of the data.  - Then we conduct feature engineering and data cleaning, model training, and finally model evaluation and testing.   Much of this process can be automated with auto machine learning tools, and that's what we would do in the second stage.   Keep in mind that all executions must be versioned and recorded.  We have to store information about the training, data, metadata and model results.  </p>"},{"location":"MLOPs/3%20-%20MLOps%20Stages/1%20-%20MLOps%20Stages/#stage-3-create-ml-services","title":"Stage 3: Create ML Services","text":"<p>  Then came stage 3, where we would put our models into production to create different machine learning services.   Once we have trained a model, this model must be integrated with the enterprise application or front end services.  It implies that we implement the model without interrupting the service.   To achieve this, pipelines will incorporate the following steps.  - The first one will be data collection, validation and feature engineering.  - Then APIs will be developed to serve as model endpoints.  Also, we can serve the model through applications. </p>"},{"location":"MLOPs/3%20-%20MLOps%20Stages/1%20-%20MLOps%20Stages/#stage-4-model-monitoring-governance-and-retraining","title":"Stage 4: Model Monitoring, Governance and Retraining","text":"<p>  The final implementation stage of the ML cycle is model monitoring, governance and retraining.  As mentioned, model monitoring is a central component in MLOps.   This monitoring allows us to keep the models updated and predict with the highest quality.  It is the only way to guarantee the model's validity in the long term.   Here we have an example of a monitoring and retraining of the model.  In this case, it is a classification model where they are monitoring the F1.   We can see that in the first few months F1 was relatively high, around 0.8.  It indicates that we had a good model.   As the months passed, F1 declined.   Once the model goes below the threshold of 0.55, the model will be automatically retrained.   Then in October, we would train the model again after model retraining.  Its performance reached an F1 of almost one.   We can see that the model was losing quality, but thanks to alerts and automatic retraining, the model will continue working well in the long term. </p>"},{"location":"New%20Rules%20of%20the%20game/Introduction/","title":"New Rules of the Game - an Introduction","text":""},{"location":"New%20Rules%20of%20the%20game/Introduction/#introduction","title":"Introduction","text":"<p>You've probably been following the rules of work that were established generations ago, but you've also probably had the nagging feeling that in a rapidly changing world, a lot of those rules no longer apply.    What nobody has told you is that there was a completely new set of rules for how you work, how you learn, and how you live your life.   And you have the opportunity to take advantage of those new rules, so you can not only be effective in your work, but you can love what you do.  </p>"},{"location":"New%20Rules%20of%20the%20game/Introduction/#whats-causing-the-change","title":"What's causing the change?","text":"<p>  The pace of change in the world of work is accelerating.   It seems that every week brings new word of a particular job that's about to go the way of the Dodo. Some of us may want to go back to what we remember as a simpler time. When many people had jobs for their entire careers, others of us are energized by the possibilities of such a dynamic world of work.   We can put the responsibility squarely on the shoulders of technology and globalization. These twin juggernauts have changed how we work, where we work and when we work. And increasingly, they're changing the rules of work.   Now it's not that technology hasn't had a profound impact on work in the past.   As futurist Ross Dawson has said, human history is all about the automation of work, right from the plough through to the spinning jenny, through to the automobile, through to any number of other inventions. Now, technology has taken away the jobs of lamplighters, switchboard operators and street sweepers.   But in the past, over time, economies have been resilient enough and innovation has been widespread enough that those who lost work in a disrupted field were often able to find work again.  So the big question today is, is it different this time? How are automation and globalization changing the very nature of work?    In the past, the pace of change has usually been stretched out over years or decades.   But information technology is unlike any other kind of technology that has come before it. With the advent of robots and machine learning, often called artificial intelligence or AI, technology has the ability to create more technology in ways that were previously the realm of science fiction.   As we become an increasingly digital economy, we are undergoing a shift in the world of work that's as fundamental as the transition from an agricultural to an industrial economy.   And it's happening at a blinding pace. This kind of big shift creates a lot of challenges in the work arena, but it also creates a range of opportunities if we can just understand what the new rules are. </p> <p> Credits : Gary Bolles - Program</p>"},{"location":"New%20Rules%20of%20the%20game/New%20Rules%20of%20Learning/","title":"New Rules of Learning","text":""},{"location":"New%20Rules%20of%20the%20game/New%20Rules%20of%20Learning/#rule-1-become-a-lifelong-learner","title":"Rule 1 : Become a Lifelong Learner","text":"<p>The new rules of work require us to have the new rules of learning.  Our industrial era model of education that's designed to manufacture learners on a mass scale is no longer up to the task of preparing young people for adult life in a time of rapid change.   Instead, we need a completely new model of learning that lasts throughout our entire lives and helps us to continually adapt to a rapidly changing world.    In his book, \"The Three Boxes of Life and How To Get Out of Them\", Dick Bolles said that we have three major components of our lives, a big chunk of learning at the beginning, then a big chunk of work, and finally a big chunk of leisure and what I call the period formerly known as retirement.   He advocated the need to completely change that model of education from a single box at the beginning of our lives to a model of continuous self-education.   So that's rule number one of the new rules of learning. You must become a lifelong learner.    As adults, we accept the work world that will constantly change. So our knowledge of the world has to continually change as well.   We must think of ourselves as lifelong learners and we have to become adept at navigating our individual paths to work opportunities.   That's probably an uncomfortable reality for many, especially for those who grew up working in long-term jobs or fields.   But if you're going to thrive in disruptive times, you need to think and act completely differently.   So let's take a look at a few strategies.    First, you need to continually have learning goals.   Each of us has to continually develop a set of topics that fascinates us.   The shelf life of knowledge is becoming shorter and shorter, so we need to set those goals for ourselves on the kinds of knowledges that we want to grow and the kinds of experiences we want to develop.   Second, it's really critical to do an annual self-inventory because we're constantly growing and gaining new experiences, we have to understand what makes each of us unique, what makes us tick on an ongoing basis.   We have annual health checkups with a doctor, we also need annual life design checkups, when we assess how our work matches up with our capabilities and goals.   This also guides the choices we make about our future work and what we'll need to learn and want to learn to help us get there.    Finally, you need to develop transferrable and self-management skills, in addition to your knowledges.   So much of formalized learning is about developing those knowledges, but we also have to look for learning opportunities that emphasize developing the skills that are transferable and that we use to manage ourselves.   In the digital era, where so much information is available online those transferable skills are what will allow us to continually adapt.    Well, lifelong learning probably sounds a little exhausting, doesn't it? Does it stop at some point?   Sure, depending on the kind of work you choose, you could find a field or work that changes more slowly than others.   But it needs to be emphasized that lifelong learning isn't a chore, it's an opportunity.   There are so many exciting opportunities to learn in so many different arenas.   It shouldn't be an issue of trying to find something that excites you.   The challenge really should be trying to choose between all the different things that you wish that you can learn.  </p>"},{"location":"New%20Rules%20of%20the%20game/New%20Rules%20of%20Learning/#rule-2-become-a-just-in-time-learner","title":"Rule 2 : Become a just-in-time Learner","text":"<p>Back when life and work were a little more predictable, you could immerse yourself in learning a field for a long time.   But you're going to find you have to increasingly look for ways to learn what's needed just before you have to perform the work.   So that's new learning rule number two, you must become a just-in-time learner.    Now let's take the example of computer programming, which is considered one of the hot jobs of today.   A 4-year software engineering degree at a well-known school could cost upwards of $300,000, yet it comes with no guarantee of future work opportunities.   It turns out that increasing number of online and in-person code camps charge fees ranging from free to $24,000 for programs ranging from four to 48 weeks.   And if they're successfully completed, they come with a guarantee to be hired, or you get your money back, which would you choose?    Of course, not all students will be directed enough to choose such just-in-time learning opportunities.   And a traditional view would be to point out that a code camp graduate might lose a number of things such as the chance to grow up on a campus, build lifelong friendships, or gain exposure to a range of classes that could help spawn critical thinking.   Every single one of those is a tremendous benefit for those that could afford it, but the number of persons that can either pay such fees or take on that kind of debt is rapidly shrinking.    In fact, just-in-time learning isn't just for those who are of college age. You could choose to take one of those targeted learning programs at virtually any age, any point in your life.   Udacity, calls these nano degrees. And such targeted learning experiences are just one solution to a rapidly changing world of work.   By allowing workers to become quickly trained in new fields of compensated work, targeted learning offers one answer to the needs of workers who might otherwise be unemployed.   A lot of just-in-time learning comes in increments in smaller learning experiences. If you need to learn a specific task such as doing pivot tables in a spreadsheet, or solving a thorny strategic problem, then a more targeted learning opportunity like a single online course is a great idea.   Of course, for many people, a two, four, or eight year college degree is absolutely the right decision, no matter what age.   The 2 most important questions to ask yourself are is the kind of environment where you learn best, and can you afford it, both in terms of the time it takes, and the expense of a traditional degree?  </p>"},{"location":"New%20Rules%20of%20the%20game/New%20Rules%20of%20Learning/#rule-3-have-a-portfolio-of-learning","title":"Rule 3 : Have a portfolio of learning","text":"<p>We've talked about the range of engagement in our work, from mild interest, all the way through to finding work that we feel is part of our purpose for being here on the planet.   We've also talked about having a portfolio of work, a range of work-related activities that together allow us to maintain our desired lifestyle.   In the same way that each of us will increasingly have a portfolio of work, we're also likely to have a portfolio of learning, and that's rule number three of the new rules of learning.   Think of the range of topics that you're attracted to.   Let's say you're mildly interested in website design, or organizational development.   You might read an article or two on the subject every now and then.   On the other end of the spectrum, let's say you have a consuming passion for archeology or movie production.   You look for every opportunity to learn more about those topics, grilling people you meet at parties and obsessively researching everything you can find.   As the world of work continues to change rapidly, having a portfolio of learning becomes a critical asset.   Think of yourself as continually looking for those topics that spark your curiosity.   - How does something work?   - What kind of jobs do people do in that field?   - What does it take to find work in that arena?  </p> <p>Now, as part of this portfolio of learning, it's important to have at least one or two ideas for subjects that could lead to future work activities for you.   There may be a particular field that fascinates you so much, you might want to immerse yourself in learning more about it.    Now, for some people, this may sound like a big obligation, after all, you already learned a trade or got a degree years ago. Why should you have to go learn a completely different field? Especially if you enjoy what you're doing now? The answer is, everything changes.  Work situations change, you change.   If you're going to be able to adapt to all those changes, you need to treat learning as a never ending journey.   So, as a lifelong learner, here's some ways you can be investing your time the best.   - To start, read obsessively about topics that interest you.   - You can also take online courses, and you can take in-person courses.   - Find the time to do informational interviews with people in fields that interest you.   - You can also participate in training experiences with one or more of the organizations you work with.   - Consider engaging in one or more hobbies, which one day may become paid work for you.   - And be sure to participate in leisure activities, like travel or sports.  </p> <p>Together, these will define your portfolio of learning, which will go hand-in-hand with your portfolio of work.  Credits : Gary Bolles - Program</p>"},{"location":"New%20Rules%20of%20the%20game/New%20Rules%20of%20Life/","title":"New Rules of Life","text":""},{"location":"New%20Rules%20of%20the%20game/New%20Rules%20of%20Life/#rule-1-plan-for-leisure-like-you-plan-for-work-and-learning","title":"Rule 1: Plan for leisure like you plan for work and learning","text":"<p>As technology allows us to work and learn from anywhere, work can come to consume your life, if you let it.   The same goes for learning, especially formal learning like in-person courses. And if you're working and you're in school, well, good luck trying to have any leisure time.   Now, we don't use the common catch phrase work-life balance because that means something different to everyone. But there's no question that work is increasingly a major source of stress in many people's lives.   And that creates a raft of challenges that only become more difficult as the pace of change accelerates around us.   We can't let work overtake our entire lives.   We need to make sure that we dial back the amount of time that we put into our work and intentionally plan for time to learn and to enjoy ourselves.   So that's rule number one of the new rules of life - Plan for your leisure time just like you plan for your work and your learning.   Let's talk about some strategies for making sure you proactively ensure that you can manage life as a priority.   First, in the same way that you block out time on your calendar for work appointments, block out time for appointments with your friends, your family and yourself. That's a good way to keep from filling every waking moment with work. If you maintain those open times on your calendar, you're much more likely to do more activities that aren't just work related.   Second, try to batch work time together, learning time together and leisure time together.   One of the reasons we can feel so stressed in our lives is that we often don't honor our relaxation time by taking it in those blocks.   If we only catch small pieces of leisure time every now and then, we can feel that our stress level isn't easily reduced.   Take it many vacation days whenever possible, so you can have that block of free time.   Third, if you have any flexibility in the work you do, you're working freelance or part-time, or you have a manager who lets you work from home at times.   Find the hours that you're most productive for work and block out the other times when you're less productive for your leisure activities.   For example, I work from home and I'm a night owl, so I code at night. And I try to do things like meeting with fascinating people or going for exercise during the day.   One strategy that some people like is to look for opportunities where the lines start to blur between work and learning and life. For example, when you go to a conference, you are working, you are always learning something new and you are always meeting new people and enjoying myself.  So, which is it? Work, learning, leisure or all three.  But many people prefer to have their lives segmented. So to these people work shouldn't overlap with their personal lives. That's great too.   Just make sure you're reserving enough time for the activities that aren't defined by work </p>"},{"location":"New%20Rules%20of%20the%20game/New%20Rules%20of%20Life/#rule-2-retirement-is-time-for-renewal","title":"Rule 2: Retirement is time for renewal","text":"<p>  The three boxes of life, a phrase coined by Dick Bolles, are learning, work and leisure.   Now the third box, the traditional box of leisure after we're done with our schooling and with our careers, is what I call the period formerly known as Retirement.   People are living longer. The average life expectancy in the India is over 68 years, and that's the average, so many of us are just going to live in longer.   We're also on average a lot healthier at an advanced age, so many of us are going to be working longer as well.   In some cases, it will be because we feel energized and motivated by the work we're doing and we see no reason to stop.   In other cases, many haven't saved anywhere near enough for retirement so they're going to continue to work out of necessity.   That's why I say rule number two of the new rules of life is, retirement should be renewal.    Treat that period formerly known as retirement as a chance to do a range of activities that renew your outlook on work, on learning and on life.   Whatever your rationale, it's important to think of that later stage as an opportunity just as you have a portfolio of work and a portfolio of learning, you could have a portfolio of activities later in your life.   At that stage, you'll have a tremendous amount of experience to pass onto others.   Well, that's a great opportunity to look for different ways to keep yourself active and engaged with work, whether it's a full or part-time job, sitting on the board of a for-profit or a non-profit company, consulting, mentoring younger workers, doing volunteer work or all the above.   Now for some people, a traditional retirement is the goal. One can understand if you put off doing some of the things that really interests you like traveling to places around the world, or if you've been working in a job that doesn't energize you for most of your career.   But for many of us, it's not clear why we would actually want to stop working in our later years, there are too many exciting things to do for work and to learn.   That's why for many people, retirement time must be renewal time.    So what's the most effective strategy that you can use to proactively approach your later years?   Treat it like any career transition, and first do your self-inventory. Look at your own unique skills and knowledges, you should have plenty of them by the time you reach traditional retirement age.   Make sure to list the range of activities that you'd find interesting and include all those leisure activities like travel that you want to include.    Now for many of us, what will be the most important is the combination of our fiscal health and our physical health.  Fiscal health is the knowledge about your financial resources and whether that can support or limit your possible activities.   More money saved for retirement, more options. Less money saved, well, you probably need to get more creative with your planning.   For example, if you want to travel but you aren't sure you have the money you need, consider doing work that pays for travel activities, such as writing travel articles, or maybe being a travel guide.   In the same way your activities might be supported or limited by your physical health.   You may have to consider certain kinds of work that are tailored to your physical condition at the time.   Again, doing a complete self inventory of your skills, your interests and your capabilities is key.   And when you look at issues like compensation, that's where a complete fiscal plan will help you to understand what kind of income you need and when you'll need it.   And when you look at working conditions, that's where you can think about the kinds of support you might need for any physical limitations that you need to adjust for, but be sure to plan for the long haul.   As said earlier, many of us are going to be living and working a lot longer.  </p>"},{"location":"New%20Rules%20of%20the%20game/New%20Rules%20of%20Life/#rule-3-make-time-to-give-back","title":"Rule 3: Make time to give back","text":"<p>  One of the most important of all the new rules has to do with why? Why are you working? Why are you learning? Why do you engage in leisure time?   The first answer we hear is, invariably, to meet your own needs. You'll have tremendous satisfaction if you're doing the kinds of activities that you most enjoy.   Of course, you need to eat, you need a roof over your head, and you need to be healthy, all of which are more possible if you're doing fulfilling work.    The second reason why is for your friends and family. They also need to eat, they need a roof over their heads, and they need to be healthy, and you can support their lives through your work.   The third reason is to help others, people who aren't part of your family or friends, but people who need help, people you may not even know.   And that's the focus of the third new rule of life, give back.    It is not suggested that you should be looking for ways to help all of humanity. Nor that you quit your job and just go volunteer someplace for no pay, although I wouldn't stop you from doing either of those things.   What is being suggested is that as you figure out how to continually adapt to the new rules of work, of learning, and of life, there will be others who aren't quite as adept at it as you are.   So here's your opportunity.    As you navigate this brave, new, constantly changing world, you have the chance to help other people.   Suppose, for example, you've learned how to become an adaptive problem solver.   You manage a portfolio of work where you always have an interesting set of projects that together pay what you need to do to have your ideal lifestyle.   In a constantly changing world that's a huge accomplishment.   You could be helping others to learn how to proactively solve problems too.   Or suppose you become a lifelong learner, naturally on you own.   You're curious about many a different things.  You're always taking courses and researching topics that fascinate you and you're continually discovering new opportunities for work.   That's something you can teach others as well.   Remember, there are still many people who were following the old rules and who aren't achieving life as they'd hoped to, as the landscape of work has changed dramatically.   You can help others to become agile navigators of the new rules as well.   Credits : Gary Bolles - Program</p>"},{"location":"New%20Rules%20of%20the%20game/New%20Rules%20of%20Working/","title":"New Rules of Work","text":""},{"location":"New%20Rules%20of%20the%20game/New%20Rules%20of%20Working/#rule-1-do-the-work-you-love","title":"Rule 1: Do the work you love","text":"<p>  It used to be okay to settle for work that didn't excite you, but if you're going to increasingly compete with others for work, then you're going to need to do work that motivates you.   So, rule number one of the new rules of work is, do the work that you love.   One of the most fundamental understandings of ourselves is that we each have a completely unique mix of skills, knowledges, traits, experiences and interests. Nobody else on the planet has exactly the same characteristics as you. And that unique mix means that there's a range of different kinds of work that you could be doing.   Dick Bolles, author of \"What Color Is Your Parachute?\", the enduring career manual, has found that the key to being motivated at work is in finding or in creating a match between your unique characteristics and the work that you do.   Over and over again, we've seen that the better that match is, between what's true about you and your work, the better your satisfaction and motivation.    So why is motivation more important in a world of constant change? There are several important reasons.   First, workers will increasingly stand out because they're motivated. Think of motivation as a spectrum. On one end, we're completely disinterested in the work that we do. We show up, we do it, but we have zero motivation. But on the other end, we jump out of bed in the morning. We're energized by the opportunity to do the work that we love. Now in a more static world with nine-to-five jobs and repetitive work, it was possible to do work that didn't motivate you very much. But you're going to increasingly find that being highly motivated by the work that you do will not only make you stand out, it will give you the drive to continually excel at what you do.   It's absolutely your choice if you don't want to be motivated at work, but you're going to find it will increasingly put you at a disadvantage.   </p> <p>  Second, workers who are doing the work that they love are far more likely to be more creative, more innovative, and more proactive in doing their work.   That increases your ability to perform your work and to repeatedly find new ways to solve problems.   Now, using your fingers as circles, if your left hand circle is all of the skills, knowledges and work environment characteristics that make you happy in your work and your right-hand circle is your current job, how much of an overlap do those two circles have? And how much do you want them to have? The greater the overlap, the more agile you'll be in continually doing the work you're motivated to do. And that will be one of your greatest assets in a constantly changing world of work. </p> <p></p>"},{"location":"New%20Rules%20of%20the%20game/New%20Rules%20of%20Working/#rule-2-be-an-adaptive-problem-solver","title":"Rule 2: Be an adaptive problem solver","text":"<p>  Our constantly changing world isn't going to slow down anytime soon. So, need to be an adaptive problem solver.  </p> <p>Let's start by looking at the fundamentals of work.   Why do we get paid for the work that we do? It's to solve problems and to generate results.   How do we solve problems? We perform tasks, and how do we perform those tasks? We use our skills.   So here's the sequence, we use our skills to perform tasks and solve problems to generate results.   For example, a fast food company has problems like getting food into the hands of happy customers and getting paid for it.   Tasks such as taking orders and making change are performed using skills like listening, handling money, making change and so on.   And the result should be that food gets delivered into the hands of a happy customer. And money gets deposited into a cash register.    Or a consulting company has problems, like helping clients develop complex market strategies, tasks such as conducting research programs and engaging in strategy design are performed using skills like analyzing, interviewing, synthesizing and writing. And the result is a happy client who has a new strategy.   In a rapidly changing world, successful workers will increasingly be the ones who seek out problems to be solved and who come up with creative ways to solve them and to generate results.   But the problems we're asked to solve tomorrow probably won't be the problems we're solving today. That's why we need to be adaptive. The work world will constantly change. So we'll need to learn how to continually adapt.   Even once we picked up a kind of work we very much want to perform. We have to be prepared to do a completely different kind of work in the future.    Now, what does it mean to be adaptive? Here are three characteristics.   Adaptive workers are continually aware and they're always on the lookout for changes in their work environment. They ask questions and they're constantly trying to understand the changing dynamics of their work.   Adaptive workers are continually innovating. They don't fall in love with any particular way of doing something. Instead, they're constantly looking for new ways to solve problems.   Adaptive workers are continually learning. They don't assume they know everything about their work. In a constantly changing work environment, they know there will always be something new to learn.   Let's face it, if you thrive on stability, you'll need to look even harder in the future for work situations that don't change on a regular basis.   Now, the good news is that in a world of constant change, we can increasingly use technology to help us solve those problems.   Technology can amplify our skills, allowing us to do things we couldn't necessarily do on our own.   So technology itself can help us to be far more adaptive.  </p>"},{"location":"New%20Rules%20of%20the%20game/New%20Rules%20of%20Working/#rule-3-think-like-an-entrepreneur","title":"Rule 3: Think like an entrepreneur","text":"<p>Now being passive may have been okay in the time of the old rules of work. If you weren't especially engaged in your work, and you didn't care all that much about the problems you were solving, maybe it was acceptable that you didn't have to be very proactive, but those days are increasingly behind us.    Instead, whether we work in a large company or we're a start-up of one, or something in between, we need to think like entrepreneurs, which is the third new rule of work.    So what do entrepreneurs do?   They identify problems proactively, they don't wait for problems to come to them.    This is about dynamically binding around problems, bringing your unique skills to bear on identifying and solving challenges and roadblocks, then pulling others into the process of generating results.    Entrepreneurs make decisions based on the information that they have at hand, they don't wait until they have the perfect information they'd like to have.    Entrepreneurs also gather as much information as they can, in a brief period of time, then they act and they do their best not to second guess their decisions after the fact.    But, perhaps, the most important trait of entrepreneurs is that they take smart risks.    They put themselves in a position where they could fail, and when they do fail they don't treat it as a life-changing problem, they treat it as a badge of courage, and then they go on to the next challenge, incorporating what they've learned, so they don't fail at the same thing again.    As investor and entrepreneur Esther Dyson says, \"Make new mistakes.\"     In large companies though, making new mistakes is one of the hardest things to do. Unless the organization supports a culture of risk-taking, a single mistake could brand you as a failure.    That's why these people are called Intrapreneurs.    People who act entrepreneurially in large organizations deserve a tremendous amount of credit for putting themselves in situations where they're taking visible risks.    John Hagel, at the Lloyd Center for the Edge, says that it's hardest to do this kind of entrepreneurial activity in the center of the organization, that is in a division or a department that's in the middle of the company's core business.    It's far easier to act entrepreneurially in a new venture or in a small department that's not at the center of the organization, but no matter where you work in your organization you'll need to look for ways that you can be more proactive.    And we call this kind of proactivity - Agency, taking initiative, attacking new problems, coming up with new solutions.    This is how workers are agent-ful, delivering new value to the organization and gaining more satisfaction in their work.     Finally, thinking like an entrepreneur doesn't necessarily mean you should go out and start a company, though it doesn't mean you shouldn't either.    What it does mean is that you need to be more of a risk-taker, and you're going to find the results will be tremendously rewarding.  </p>"},{"location":"New%20Rules%20of%20the%20game/New%20Rules%20of%20Working/#rule-4-work-with-others-to-solve-a-pool-of-problems","title":"Rule 4: Work with others to solve a pool of problems","text":"<p>  The old rules of work meant you didn't always have to collaborate with other people. Even if you were part of a team, you might've worked somewhat independently of others workers.   But in a constantly changing work environment, you and your fellow team members will increasingly need to work together to solve problems.   You're going to find you need to collaborate across departments and divisions across organizational boundaries and with people all over around the world.   So, rule number four of the new rules of work, work with others to solve a pool of problems.    In order to be an adaptive problem solver, what you'll increasingly find is that you and your coworkers will need to work together as a team of problem solvers.   So, rather than being a set of individual workers who operate in parallel, think instead of the pool of problems that your team is charged with solving.   What if you approach that set of problems as a team dynamically allocating the tasks to be solved between yourselves so that each person's unique skills could be brought to bear on the most appropriate problems, that's what it means to be a pool of problem solvers.   Now, how can you know the kinds of people with whom you'd work best? Well, there are numerous approaches. Psychologists call this the Person Environment fit.   That essentially means that you have a set of unique characteristics and so does where you work and especially the kinds of people with whom you work.   Since few things in life are ever perfect person-environment fit is really about optimization, which is trying to continually adjust to increase the match between you and your environment.   One theory of person-environment fit is known as your Holland Code. That code can give you some pretty good insights about the kind of people-environment that you'd like to most work at.   Now there's another approach that's called the Myers Briggs Type Indicator, which is a schema based on psychologist Carl Young's archetypes. And that gives you a four letter code that helps you to understand the kinds of ways you solve problems and how you interact with other people. Once you know that Myers Briggs Code, you can get a good idea of the kinds of people with whom you work best, and the kinds of people you probably have some more difficulty communicating and collaborating with.   You can think of this process as finding your tribe, the kinds of people with whom you feel the most affinity at work.   Working with your tribe can bring a tremendous amount of satisfaction because you feel you're working with the people who think and act like you and reinforcing your own skills and knowledge.   But don't, over-index on your tribe. Don't create a situation where all you see are the kinds of people who think like you.   Researchers show that the best ideas come from heterogeneous groups with lots of ideas from people who think far more differently.   It's great to work with people you think of as your tribe, but if you're going to be a member of a team of problem solvers make sure you have plenty of chances to collaborate with people who are dramatically different from your tribe as well. </p>"},{"location":"New%20Rules%20of%20the%20game/New%20Rules%20of%20Working/#rule-5-have-a-portfolio-of-work","title":"Rule 5: Have a portfolio of work","text":"<p>In a world of rapid change, we need a new model that helps each of us to continually grow and develop and to continually find new sources of opportunities for compensated work, what we used to call a job.   Without our necessarily realizing it, our actual relationship with work is changing.    When our relationship to work is binary, we either have a job or we don't, we usually have predictable activities and earnings.   But when rapidly changing times means we have a fuzzier relationship with work, perhaps requiring multiple sources of income from a variety of activities.   I call this a portfolio of work.    In the same way that an investor often has a distributed portfolio, some safe investments, some riskier investments, an important strategy for dealing with the uncertainty in the world of work is to have more than one activity at any one time. And the most entrepreneurial of workers will have a range of activities they do to generate income, and to perform the kinds of activities that they really enjoy.   So what are all the different ways that you can work? You could be full-time long-term or full-time temporary.   Or you could work part-time long-term or part-time temporary.   You could also be on call or doing project-based or piecework.    Maybe you could be doing app-based work, such as driving for Lyft or doing tasks for TaskRabbit.   You might be doing what's known as asset-based work, such as sharing something you own on Airbnb, selling and buying assets on something like eBay, or creating and selling wares on something like Etsy.   Perhaps you might be participating in an online work co-op.   You could also be starting your own company on the side.   Or you might be doing several of these all at the same time.    Now, of course, jobs as we know them will continue to exist for a long time. But for many people, there will be strong incentives to work in a different manner.   Alternatives to traditional jobs have existed for years. But the incentives to use those alternatives are increasing.   Think of each of these alternatives as parts of your portfolio of work.    A portfolio of work is inevitable for each of us and it's already in practice by many workers.   For those who thrive on variety and change, a portfolio of work is ideal. Life is varied. There's a constant flow of new challenges.   But for those who thrive on stability and predictability, a portfolio can mean stress and uncertainty.   Even if you only have one job, you probably already have at least one additional work-related activity in your portfolio.   You might have a hobby that could turn into work. You might have an idea for a startup that you're working on.   Or you might help out with projects with friends or at a nonprofit, which could turn into compensated work some day.   If for no other reason, think of a work portfolio as a kind of insurance. In a constantly changing world, there's no guarantee that your current work situation is going to remain static.   Now, to some people, this is a really exciting opportunity, which means constant variety and change, while to others, it sounds like a lot of work.   If you already are good at things like time management and communicating with people in your network, you'll find that you're naturally drawn to this kind of work context.   But you may find you'll need to hone these kinds of skills if you haven't practiced them much until now.   Just as entrepreneurs are always trying to anticipate problems before they arrive, it's going to be increasingly useful to expand your portfolio of work and to continually look for new ways to do the kinds of work that you love. </p>"},{"location":"New%20Rules%20of%20the%20game/New%20Rules%20of%20Working/#rule-6-build-and-tend-your-network","title":"Rule 6: Build and tend your network","text":"<p>  In a more aesthetic world, you probably didn't need much of a network of contacts. If you only change jobs every decade or so, you didn't need to go through many other people to find that work, but that's an old rule.   The new rules of work mean you'll increasingly need a network of connections to do your work and to find new work.    One of the most influential research papers on networks comes from a sociologist by the name of Mark Granovetter, who wrote a paper called The Strength of Weak Ties. His thesis was that some of the greatest value from our connections with other people is at the edge of our networks, not at the core.   The people we know well, those who are at the center of our network are likely to hear about the same opportunities that we are and the more likely are potential competitors than they are our allies, at least when we're looking for work.   Today, we maintain a lot of weak ties through online social media, like Facebook and LinkedIn, but having weak ties doesn't mean that you should build your network without being intentional about the connections that you make.   You could have a broad network, but make sure that even the weaker connections are quality connections.    As the walls of organizations become more pliable, people are more frequently moving in and out of work situations.   That means that you'll frequently be meeting a variety of people, sometimes briefly, sometimes in deeper contact.    It's essential that you build that network over time, maintaining connections with those with whom you share interests and experiences.   It also means you should take care when adding to your network.   If you don't really know someone, if you've never met them and you don't really know anyone who knows them, it's still possible you want them in your network.   But it's important to be as intentional as possible.    When you make a decision to add someone. Remember too, that while you'll probably make many of your connections online, the best relationships are actually cemented in person.   So it's important to seek out opportunities to see people in your network face to face, that will increase the quality of the connection between you.   It'll also make it more likely that you'll understand what's important to others in your network, so you can make it a two way street.   Just as you may someday need the help of people in your network, you'll want to know how you can be helpful to them.    It may help you to think of your network as a garden. You need to grow it, nurture it, and do your best to keep it healthy and thriving.   Beyond your own skills and abilities in a world of constant change, your network is the greatest asset you'll have.  </p>"},{"location":"New%20Rules%20of%20the%20game/New%20Rules%20of%20Working/#rule-7-do-work-driven-by-meaning","title":"Rule 7: Do work driven by meaning","text":"<p>Any of us, no matter what age, has the opportunity to look for that meaning and to seek out what we believe to be our purpose.   Of course, if you don't feel it's important to have your work be a source of purpose in your life, it's completely your choice.   But it's my fervent hope for you, though, that you find meaning and purpose in some other part of your life outside your work.    It's never been more possible for a single passionate individual or group of passionate individuals to have a positive impact on the world.   Look, there's too many important things to do, and there aren't enough people and time to do them.   That's why we all need to look for that meaning and purpose in our work, and you can do it too. </p> <p> Credits : Gary Bolles - Program</p>"},{"location":"OOPs/OOPs%20in%20Python/","title":"OOPs in Python","text":""},{"location":"OOPs/OOPs%20in%20Python/#oops-in-python","title":"OOPS in python","text":""},{"location":"OOPs/OOPs%20in%20Python/#key-terminologies","title":"Key terminologies","text":"<ul> <li>Classes are logical collection of attributes and methods;</li> <li>Objects are instance of class.</li> <li>Actions performed are covered in Methods</li> <li>To access attribute of your class, use self parameter</li> <li>Process of creating your object is called object instantiation</li> </ul> <p>Everything in python is an object.  eg - mylist = [1,2,3] --if we do type() on this, it returns o/t as 'class list'. That means, list is a class in python, and above mylist is an object of class list. So, if you call function, say, append, then it invokes method of class list. mylist.append(4) -&gt; [1,2,3,4]</p>"},{"location":"OOPs/OOPs%20in%20Python/#attributes","title":"Attributes","text":"<p>Attributes - an attribute is a property that further defines a class.</p>"},{"location":"OOPs/OOPs%20in%20Python/#what-is-a-class-attribute","title":"What is a class attribute?","text":"<ul> <li>a class attribute are attributes that are shared across all instances of a class. eg - no. of working hours. </li> <li>they are created either as a part of the class or by using className.attributeName.</li> <li>so if edited or modified, then reflected for all object instances. How to edit - to access them, use ClassName.ClassAttributeName and change value.  eg -</li> </ul> <pre><code>class Employee:\n    HoursOfWork = 40\n\nx=Employee()\nx.HoursOfWork --&gt; output = 40\n\nNow edit HoursOfWork --&gt;\nEmployee.HoursOfWork = 30\nx.HoursOfWork --&gt; output = 30\n</code></pre> <p>We can also edit class attribute specific for an object instead for class as a whole.  eg -</p> <pre><code>class Employee:\n    HoursOfWork = 40\n\nx=Employee()\nx.HoursOfWork --&gt; output = 40\n\nNow edit HoursOfWork --&gt;\nx.HoursOfWork = 50\nx.HoursOfWork --&gt; output is 50 --&gt; here we created another instance attribute called HoursOfWork(class attribute remains unchanged)\n\ny=Employee()\ny.HoursOfWork --&gt; output = 40\n</code></pre>"},{"location":"OOPs/OOPs%20in%20Python/#what-is-instance-attribute","title":"What is Instance attribute?","text":"<p>Instance attribute are attributes that are specific to each instance of a class.  eg - </p> <pre><code>class Employee:\n    HoursOfWork = 40\n\nx=Employee()\nx.name = \"ram\"  --&gt; creates an instance attribute for object 'x'\nx.name --&gt; output = ram\n\ny=Employee()\ny.name --&gt; give error coz no instance attribute associated with 'y'\n</code></pre> <p>They are created using objectName.attributeName   So, if edited or modified, then reflected only for that object instance.  </p>"},{"location":"OOPs/OOPs%20in%20Python/#how-self-parameter-is-handled","title":"How self parameter is handled?","text":"<p>The method call objectName.methodName() is interpreted as className.methodName(objectName), and this parameter is referred to as 'self' in method definition.  </p>"},{"location":"OOPs/OOPs%20in%20Python/#methods","title":"Methods","text":"<p>Attributes - an attribute is a property that further defines a class. Method - a method is a function within a class which can access all the attributes of a class and perform a specific task.</p>"},{"location":"OOPs/OOPs%20in%20Python/#what-is-instance-method","title":"What is Instance Method?","text":"<p>Methods that accept self parameter as default parameter.  They are used to modify instance attributes. </p>"},{"location":"OOPs/OOPs%20in%20Python/#what-is-a-static-method","title":"What is a static method?","text":"<p>Methods that do not modify instance attributes of a class are called Static Methods.   But they can be used to modify Class Attributes. </p>"},{"location":"OOPs/OOPs%20in%20Python/#what-is-init-method","title":"What is init() method?","text":"<p>init() method is the initializer in python.  It is called when an object is instantiated.  All the attributes of the class should be initialized in this method to make your object a fully initialized object. </p> <p>It has the default parameter as the object are static methods.  </p>"},{"location":"OOPs/OOPs%20in%20Python/#self-variable","title":"self Variable","text":"<p>When you call any class method, then by default, first argument that is passed is class object name.  eg - </p> <pre><code>class Employee:\n    def employeeDetails()\n        pass\n\n\nemployeOne = Employee()\nemployeOne.employeeDetails() is same  as employeeOne.emplpyeDetails(employeeOne)\n</code></pre> <p>By using self parameter, you are referring to attributes of that object, and the values persist until lifespan of program or until you manually delete the object. Eg -</p> <pre><code>class Employee:\n    def employeeDetails(self):\n        self.name = \"kush\"\n        print(\"name = \", self.name)\n\n        age = 30 --&gt; since we didnt use \"self\", the value age is not available to other methods\n        print(\"age = \",age)\n\n    def printEmployeeDetails(self):\n        print(\"my name = \", self.name)\n        print(\"my age = \",age) --&gt; here it will throw error since \"age\" is not available to other functions. Its scope is restricted to method employeeDetails.\n\n</code></pre>"},{"location":"OOPs/OOPs%20in%20Python/#static-methods-and-instance-methods","title":"Static Methods and Instance Methods","text":"<p>Instance Methods - They are methods of our class that make use of self parameter and modify instance attributes of our class. </p> <p>Static Methods- - They are methods that do not take default self parameter.   - To differentiate between static and instance method, we use decorator  @staticmethod </p> <p>Decorators are functions that take another function as input and extend their functionality.  They are denoted by starting them with '@' symbol.   For example - @staticmethod is a decorator that takes the function as message, extends its functionality  and ignores the binding of the object.  </p> <pre><code>class Employee:\n    @staticmethod\n    def WelcomeMessage():\n        print(\"welcome to our organization\")\n</code></pre> <p>init() method  - init() method is a special method in python which is used to initialize an object.  eg -</p> <pre><code>class Employee:\n\n    def setName(self):\n        self.name=\"ram\"\n\n    def printName(self):\n        print(self.name)\n\n\n\nx=employee()\nx.printName() --&gt; it gives error coz we havent initialized name.\n</code></pre> <ul> <li>init() method is first method to be called in a class.</li> </ul> <pre><code>class Employee:\n    def __init__(self, name):\n        self.name = name\n\n    def printName(self):\n        print(self.name)\n\nx=employee(\"ram\")\nx.printName() --&gt; output is Ram\n\ny=employee(\"shyam\")\ny.printName() --&gt; output is shyam\n</code></pre>"},{"location":"OOPs/OOPs%20in%20Python/#abstraction-and-encapsulation","title":"Abstraction and Encapsulation","text":"<p>Abstraction is the process of hiding implementation details from the user.  Encapsulation is done to achieve abstraction.  You use method of class via abstraction.  </p>"},{"location":"OOPs/OOPs%20in%20Python/#inheritance","title":"Inheritance","text":"<ul> <li>allows code re-usability.</li> </ul>"},{"location":"OOPs/OOPs%20in%20Python/#type-1-single-level-inheritance","title":"Type 1 : single level inheritance","text":"<p>eg 1 - </p> <pre><code>dad = good baseball player. \nson = good baseball player.\n</code></pre> <p>eg 2 - </p> <pre><code>class Apple:\n    manufacturer = \"Apple Inc.\"\n    contactWebsite = \"www.apple.com/contact\"\n\n    def contactDetails(self):\n        print(\"To contact us, log on to \", self.contactWebsite )\n\nclass MacBook(Apple):\n    def __init__(self):\n        self.yearOfManufacture = 2017\n\n    def manufactureDetails(self):\n        print(\"This macbook was manufactured in year {} by {}\"\n        .format(self.yearOfManufacture, self.manufacturer)\")\n\nmymacbook = MacBook()\n\n# output = This macbook was manufactured in year 2017 by Apple Inc.\nmymacbook.manufactureDetails\n\n# To contact us, log on to www.apple.com/contact\nmymacbook.contactDetails \n</code></pre>"},{"location":"OOPs/OOPs%20in%20Python/#type2-multiple-inheritances","title":"Type2: Multiple Inheritances","text":"<p>eg 1 - </p> <pre><code>dad = good baseball player.\nmom = good cook. \nson = good baseball player and good cook\n</code></pre> <p>eg 2 - </p> <pre><code>class OperatingSystem:\n    multitasking = True\n    name = \"Windows OS\"\n\nClass Apple:\n    website = \"www.apple.com\"\n    name = \"Apple OS\"\n\nClass MacBook(OperatingSystem, Apple):\n    def __init__(self):\n        if self.multitasking is True:\n\n            # output = this is a multitasking system. Visit www.apple.com for more details.\n            print(\"this is a multi tasking system. Visit {} for more details\".format(self.website))\n\n            #output = Window OS --why - coz we inherited OperatingSystem class first, and so variable present in both classes will be picked up from first class.\n            print(\"name = \".self.name)\n\n\n\nClass NewMacBook(Apple, OperatingSystem):\n    def __init__(self):\n        if self.multitasking is True:\n            # output = this is a multitasking system. Visit www.apple.com for more details.\n            print(\"this is a multi tasking system. Visit {} for more details\".format(seld.website))\n\n            # output = Apple OS --why - coz we inherited Apple class first, and so variable present in both classes will be picked up from first class. \n            print(\"name = \".self.name)\n\n\n</code></pre>"},{"location":"OOPs/OOPs%20in%20Python/#type-3-multi-level-inheritance","title":"Type 3: multi level inheritance","text":"<p>eg 1- </p> <pre><code>grandfather = good baseball player\nfather = good soccer player \nson = good baseball and soccer player\n</code></pre> <p>eg 2- </p> <pre><code>class MusicalInstruments:\n    numberOfMajorKeys = 12\n\nclass StringInstruments(MusicalInstruments):\n    typeOfWood = \"ToneWood\"\n\nclass Guitar(StringInstruments):\n    def __init__(self):\n        self.numberOfStrings = 6\n        print(\"this guitar consists of {} strings. It is made of {} and it can play {} keys\"\n        .format(self.numberOfStrings, self.typeOfWood, self.numberOfMajorKeys))\n\nguitar = Guitar()\noutput - \n    This guitar consists of 6 strings. It is made of ToneWood and can play 12 keys\n</code></pre>"},{"location":"OOPs/OOPs%20in%20Python/#access-specifiers-public-private-and-protected","title":"Access Specifiers - public, private and protected","text":"<ul> <li>public - </li> <li>your class, derived class, and further sub-classes derived from derived class --all can access values in main class.</li> <li>protected - </li> <li>only your class and derived class members can access data and methods of your class.</li> <li>private- </li> <li>only your class can access methods and attributes of your class.</li> </ul> <p>To declare attribute or method as -      1. public - MemberName      2. Protected - _memberName --&gt; single underscore.      3. Private - __MemberName --&gt; double underscore. </p> <pre><code>class Car:\n    numberOfWheels = 4\n    _color = \"Black\"\n    __YearOfManufacture = 2018 #python saves it as _Car__yearOfManufacture i.e _ClassName__AttributeName\n\nClass BMW(Car):\n    def __init__(self):\n        print(\"Protected attrubte color : \", self._color)\n\ncar=Car()\nprint(\"Public Attribute numberOfWheels: \", self.car.NumberOfWheels) --&gt; output =  Public Attribute NumberOfWheels: 4\n\nbmw = BMW() --&gt; output =  Protected attribute color : Black\n\nprint(\"Private Attribute yearOfManufacture: \", car.__yearOfManufacture  ) --&gt; output = error\nprint(\"Private Attribute yearOfManufacture: \", car._Car__yearOfManufacture) --&gt; output = Private attribute YearOfManufacture : 2018\n</code></pre>"},{"location":"OOPs/OOPs%20in%20Python/#polymorphism","title":"PolyMorphism","text":"<p>It is characteristic of an entity to be able to be present in more than one form.  eg - \"+\" operator when applied to 2 numbers returns their sum, but whe applied to 2 strings, returns their concatenation.  </p>"},{"location":"OOPs/OOPs%20in%20Python/#overriding-and-super-method","title":"Overriding and Super() method","text":"<p>Overriding = redefine base class method in your derived class.  </p> <p>super() function allows you to transfer control back to base class in a derived class.  It is used in case you want to access method in base class. </p> <pre><code>class Employee:\n    def setNumberOfWorkingHours(self):\n        self.NumberOfWorkingHours = 10\n\n\n    def displayNumberOfWorkingHours(self):\n        print(self.NumberOfWorkingHours)\n\n\nemployee = Employee()\nemployee.setNumberOfWorkingHours()\nprint(\"Number of working hours of emplyee = \", end = ' ')\nemployee.displayNumberOfWorkingHours()\n\noutput -\nNumber of working hours of Employee = 10\n\n\nclass Trainee(Employee):\n    def setNumberOfWorkingHours(self):\n        self.NumberOfWorkingHours = 20\n\n\n    def resetNumberOfWorkingHours(self):\n        super().setNumberOfWorkingHours()\n\ntrainee = Trainee()\n\ntrainee.setNumberOfWorkingHours()\nprint(\"Number of working hours of Trainee = \", end = ' ')\ntrainee.displayNumberOfWorkingHours()\n\n\ntrainee.resetNumberOfWorkingHours()\nprint(\"Number of working hours of Trainee after reset = \", end = ' ')\ntrainee.displayNumberOfWorkingHours()\n\n\noutput -\nNumber of working hours of Trainee = 20\nNumber of working hours of Trainee after reset = 10\n</code></pre>"},{"location":"OOPs/OOPs%20in%20Python/#diamond-problem","title":"Diamond Problem","text":"<p>Here we have 4 classes - A, B, C and D  - A is base class  - B anc C derived from A - D is derived from B and C</p>"},{"location":"OOPs/OOPs%20in%20Python/#case-1-method-will-not-be-overridden-in-class-b-and-class-c","title":"Case 1: Method() will Not be overridden in Class B and Class C","text":"<pre><code>class A:\n    def method(self):\n        print(\"this method belongs to class A\")\n\n    pass\n\n\nclass B(A):\n    pass\n\nclass C(A):\n    pass\n\nclass D(B,C):\n    pass\n\nd = D()\nd.method()\n\noutput --&gt;\nThis method belongs to class A\n</code></pre>"},{"location":"OOPs/OOPs%20in%20Python/#case-2-method-will-be-overridden-in-class-b-but-not-in-class-c","title":"Case 2: Method() will be overridden in Class B but not in Class C","text":"<pre><code>class A:\n    def method(self):\n        print(\"this method belongs to class A\")\n\n    pass\n\nclass B(A):\n    def method(self):\n        print(\"this method belongs to class B\")\n\nclass C(A):\n    pass\n\nclass D(B,C):\n    pass\n\nd = D()\nd.method()\noutput --&gt; This method belongs to class B\n\n</code></pre>"},{"location":"OOPs/OOPs%20in%20Python/#case-3-method-will-be-overridden-in-class-c-but-not-in-class-b","title":"Case 3: Method() will be overridden in Class C but not in Class B","text":"<pre><code>class A:\n    def method(self):\n        print(\"this method belongs to class A\")\n\nclass B(A):\n    pass\n\nclass C(A):\n    def method(self):\n        print(\"this method belongs to class C\")\n\nclass D(B,C):\n    pass\n\nd = D()\nd.method()\noutput --&gt; This method belongs to class C\n</code></pre>"},{"location":"OOPs/OOPs%20in%20Python/#case-4-method-will-be-overridden-in-both-class-b-and-class-c","title":"Case 4: Method() will be overridden in both Class B and Class C","text":"<pre><code>class A:\n    def method(self):\n        print(\"this method belongs to class A\")\n\nclass B(A):\n    def method(self):\n        print(\"this method belongs to class B\")\n\nclass C(A):\n    def method(self):\n        print(\"this method belongs to class C\")\n\nclass D(B,C):\n    pass\n\nd = D()\noutput -&gt; This method belongs to class B\nwhy this output -&gt; \nMethod Resolution Order -- in case of derived class from multiple classes, method picked from 1st class to which it is obtained;\n\n\nHad we defined class D as below, output would have been different. \nclass D(C,B):\n    pass\n\nd = D()\nd.method() --&gt; output = This method belongs to class C\n</code></pre>"},{"location":"OOPs/OOPs%20in%20Python/#operator-overloading","title":"Operator Overloading","text":"<p>There are internal functions of python.  They are overridden in following way -   eg - overloading addition operator,  the \"+\" operator when   - applied to 2 numbers returns their sum.  - applied to 2 strings, returns their concatenation.</p> <pre><code>class Square:\n    def __init__(self, side):\n        self.side = side\n\n\nsq1 = Square(5)\nsq2 = Square(10)\n\n# output -&gt; ERROR\n# Why -&gt; sq1 + sq2 = error coz '+' operator doesn't work on object of class square\nprint(\"the sum of sides of both the squares = \", sq1 + sq2)  \n\nSo, we need a mechanism to add sides of square and return value.\nThat should allow us to add above 2 square objects as well.\n\nSolution -&gt;\noverload special method add() in your class\n\nclass Square:\n    def __init__(self, side):\n        self.side = side\n\n    def __add__(square1, square2):\n        return( (4 * square1.side) + (4 * square2.side) )\n\nsq1 = Square(5)\nsq2 = Square(10)\n\n# output -&gt; 60\nprint(\"the sum of sides of both the squares = \", sq1 + sq2)\n\n</code></pre>"},{"location":"OOPs/OOPs%20in%20Python/#abstract-base-class","title":"Abstract Base Class","text":"<p>An Abstract Base Class is a base class which doesn't have a definition of its own.  It has abstract methods which forces its implementation in derived classes  </p> <p>To declare a class as abstract base class, it needs to be declared as instance of ABCMeta class of python.  eg -</p> <pre><code># we import ABCMeta and abstractmethod from abc module of python\n\nfrom abc import ABCMeta, abstractmethod\n\nclass shape(metaclass = ABCMeta):\n\n    #decorator abstractmethod tells that given method is required to be defined in derived classes.\n    @abstractmethod    \n    def area(self):\n        return 0\n\n    #here we say that method area() does not have a defn of its own; but it needs to be defined in derived classes.\n\n\nclass Square(shape):\n    side = 4\n\n    def area(self):\n        print(\"area of square\", self.side * self.side)\n\nclass Rectangle(Shape):\n    width = 5\n    breadth = 10\n\n    def area(self):\n        print(\"area of rectangle \" , self.width * self.length)\n\nsq = Square()\nrc = Rectangle()\n\nsq.area()\nrc.area()\n</code></pre> <p>Note - if no definition of method area() in class Square and even if we don't call sq.area() --&gt; we get error</p> <p>An abstract can only be inherited in derived classes, but cannot be instantiated.  If we try to create object of abstract class, we get error.  </p> <pre><code>sh = Shape()\n</code></pre> <p>Summary   - polymorphism = ability of an entity to be able to exist in more than one form.    - overriding = form of polymorphism in which we redefine method of base class in a derived class.     - By doing this we change behaviour of a base class method.      - How -</p> <pre><code>    class BaseClass:\n           def BaseClassMethod():\n               #define behaviour\n    class DerivedClass(BaseClass):\n           def baseClassMethod():\n               #redefine behaviour\n</code></pre> <ul> <li> <p>Operator Overloading - defining a special method for an operator within your class to handle the operation between the objects of that class is called Operator Overloading.</p> </li> <li> <p>Abstract Base Class - a base class which consists of abstract methods that should be implemented in its derived class is called abstract base class.  Syntax -</p> </li> </ul> <pre><code>    from abc import ABCMeta, abstractmethod\n\n    class baseClass(metaclass = ABCMeta):\n\n        @abstractmethod\n        def abstractMethod(self):\n            return\n</code></pre>"},{"location":"SOLID%20Principles/Interface%20Seggregation%20Principle/","title":"Interface Segregation Principle","text":""},{"location":"SOLID%20Principles/Interface%20Seggregation%20Principle/#introduction","title":"Introduction","text":"<p>  'I' stands for Interface Segregation Principle. This is often abbreviated as ISP.  This principle says - \"No client should be forced to depend on methods it does not use\".   To explain this, as always, lets start with a real world analogy.  Assume you are working in an office which has around 200 employees.  You have a bunch of printers , scanners and fax machines available for the employees to use.  As a software developer, you have been asked to represent these devices using object oriented concepts in code.  You have to design some interfaces too, to ensure a certain level of uniformity among these devices.   You start walking around the office floor, and you spot THIS device.This is a multi-function all-in-one Xerox WorkCentre, that has a printer, scanner, copier, and a fax machine all built into one.   You think that this could be a good starting point.  You feel that if you can design a common interface based on this all-in-one device, that will give you a good start.  So lets see this in code.   First, you design an interface named IMultiFunction.  This interface defines methods that represent various functions.   We have the print() and getPrintSpoolDetails() methods that are associated with printing.  We have the scan() and scanPhoto() methods that are associated with scanning.  The fax() and internetFax() methods deal with fax related functions.   Now you create a concrete class that represents this particular device - The Xerox WorkCentre.  You make it implement the IMultiFunction interface. So you will have to implement all the 6 methods().   All good so far.  </p> <p>Now you walk over to the next couple of rooms in your office, and encounter 2 different machines.   First is an HP printer-scanner device.  Its not as multi-faceted as the Xerox WorkCentre but it can still do a couple of functions - printing and scanning.  For it, we implement the print(), getPrintSpoolDetails(), scan() and scanPhoto() methods.  What about the other methods in the interface?  If you are implementing an interface, you HAVE to implement ALL methods in it.  So what we will do is we will implement the fax() and internetFax() method and give it a blank implementation.   This machine does not have to ability to send or receive fax, which is why, we have to leave these fax related methods unimplemented.   Second one, is a Canon printer device and has printing functions alone.  So we will create a class to represent this device and make it implement the IMultiFunction interface again.  We will implement the print() and getPrintSpoolDetails() methods.  But then, we will have to implement all the other methods as well and leave the implementations blank.  So scan(), scanPhoto(), fax(), internetFax() , leave it all blank.  </p> <p>Now you start seeing the problem. </p> <p>Unimplemented methods are almost always indicative of a poor design.   This goes against the Interface Segregation Principle which says:\"No client should be forced to depend on methods it does not use\".   So what is the problem if we don't fix this? Why is this a bad design?   We leave the methods blank - What's the big deal?  </p> <p>Assume you have a employee portal application in your office that needs to directly access these devices.  So you have to provide these device classes packaged as a library, to a group of programmers who are working on this employee portal.   One programmer wants to send a fax, he sees the fax() method on your Canon Device class, and invokes it.  He does not care to look inside the method, and understand that it is a blank implementation.  He does not need to, in fact.   All the programmer sees is the fax() method on the Canon class, assumes that this device has the ability to send a fax, and invokes it from the employee portal.  This is calling for trouble, and the code will definitely break, because this method implementation is blank.   So this is the reason why this is a bad design.   And this is why Interface Segregation Principle recommends to avoid such designs. </p>"},{"location":"SOLID%20Principles/Interface%20Seggregation%20Principle/#restructing-the-code-to-follow-isp","title":"Restructing the Code to follow ISP","text":"<p>  Continuing our ongoing example, we have one generic interface, and 3 classes that implement it.  Not all methods make sense to all classes, so some have blank method implementations.  So the easiest way to fix this is to split the Big Interface into smaller interfaces   So lets take the generic interface IMultiFunction and split it into 3 interfaces - IPrint, IScan and IFax interfaces.   Now, lets change the classes too accordingly.  So the Xerox WorkCentre will implement all 3 interfaces. Remember, all object oriented programming languages allow a class to implement multiple interfaces.  The HP device class will implement the IPrint and IScan interfaces.  So, we can delete the fax() and internetFax() blank implementations now from this class.  Finally the Canon device will implement the IPrint interface alone, as it can do only printing related functions.  So, we can remove all the remaining blank implementations from the Canon device class.   So you can see that the classes have become much cleaner now.  No more blank implementations of methods.   If we decide to package these classes as a library, and pass it on to a programmer, then there is no ambiguity - A programmer can only call the defined methods in a class.  So, we have solved the problem by segregating the interfaces.This is one way of following the Interface Segregation Principle.   Now, if you feel that there are certain functions that are common between a printer, scanner, and a fax machine, you can have a parent interface with the common methods.   And you can have the IPrint, IScan, and IFax interfaces extend the parent interface adding specific methods. </p>"},{"location":"SOLID%20Principles/Interface%20Seggregation%20Principle/#techniques-to-identify-violations","title":"Techniques to identify violations","text":"<p>Lets first look at some standard techniques to identify violations of the interface segregation principle.</p> <ol> <li> <p>Fat Interfaces - Fat interfaces mean interfaces with high number of methods. If you recall our IMultiFunction interface from our introductory session, it had around 6 methods to deal with multifarious functions like printing, scanning, and fax. This is almost always indicative of a violation of the Interface Segregation Principle.</p> </li> <li> <p>Interfaces with Low Cohesion. Lets again talk in the context of our IMultiFunction interface.  We had methods for fax and photoScan. Now fax and photoScans are two entirely different functions. One deals with transmitting a printed piece of paper over a telephone line, whereas the other deals with digitally capturing the colors in a photograph. In other words, there is Low Cohesion between the methods in the IMultiFunction interface.</p> </li> </ol> <p>This is another indication that we are breaking the Interface Segregation Principle in all probability.</p> <ol> <li>empty implementations of methods. Again , we saw this in our introductory session where some classes had to leave certain method implementations as blank. Blank implementations of interface methods is almost always a violation of the interface segregation principle.</li> </ol> <p>So these are some of the ways you can identify violations of the interface segregation principle.</p>"},{"location":"SOLID%20Principles/Interface%20Seggregation%20Principle/#relationship-with-other-solid-principles","title":"Relationship with other SOLID principles","text":"<p>Next, lets explore the relation that this principle has, with other solid principles.  Lets look at our problem solution page.   After we split the interface, we can now say that one interface has one responsibility, or a single responsibility.  So the IPrint interface has only a single responsibility, i.e. it holds print related method definitions.  Similarly, the IScan interface has only a single responsibility of holding scanning related method definitions.  So we are inadvertently following the Single Responsibility Principle.    Also, after we segregated the interfaces, we are indirectly following the Liskov Substitution Principle as well.  Because now , in places where we use , for instance, the IPrint interface, we could substitute it with the CanonPrinter class instead.   So as you can see , most of the SOLID principles are intricately linked to one another.  SOLID principles complement each other, and work together in unison to achieve the common purpose of well-designed software. </p>"},{"location":"SOLID%20Principles/Introduction/","title":"Introduction to SOLID Principles","text":""},{"location":"SOLID%20Principles/Introduction/#what-are-solid-principles","title":"What are SOLID Principles?","text":""},{"location":"SOLID%20Principles/Introduction/#why-solid","title":"Why SOLID?","text":"<p>These are simple but cardinal design principles, that enables us to create elegant and robust software.  Cost of bad software increases over time. </p>"},{"location":"SOLID%20Principles/Liskov%20Substitution%20Principle/","title":"Liskov Substitution Principle","text":""},{"location":"SOLID%20Principles/Liskov%20Substitution%20Principle/#introduction","title":"Introduction","text":"<p>'L' stands for Liskov Substitution Principle. This is often abbreviated as LSP.  It is named after a computer scientist Barbara Liskov who explained this principle in a book that she authored. </p> <p>The principle says \"Objects should be replaceable with their subtypes without affecting the correctness of the program\" </p> <p>To explain this, we will start with inheritance, which is a basic feature of any object oriented programming language. </p>"},{"location":"SOLID%20Principles/Liskov%20Substitution%20Principle/#is-a-relationship","title":"'Is-A' relationship","text":"<p>Inheritance is also referred to as the 'Is-A' relationship.  </p> <p>Take the example of a car class.  A hatchback extends a car class. So we say hatchback 'is-a' car. </p> <p>Let's take another one: We have a bird class, and ostrich extends bird. So Ostrich 'is-a' bird.  Another one: Fuel and gasoline. So gasoline extends fuel, or gasoline 'is-a' fuel. </p> <p>On the face of it, all of this sounds perfect and sounds like text-book examples. </p> <p>But there are hidden problems with this approach, which may not seem obvious at first. </p> <p>Among these 3 examples, the one in the middle has a hidden problem i.e. Ostrich is-a bird. </p> <p>So Ostrich is a bird. And by all means of real-world classification, an ostrich is a bird alright. </p> <p>As per the biological classification too, ostrich is a bird.  But, here's an interesting fact, an ostrich cannot fly. </p> <p>Let's see this in code now.  </p> <p>So we have a Bird class with a fly method.  Then, we have an Ostrich class that extends the Bird class.  Because the fly() method does not make sense for an Ostrich, what the Ostrich class does is, it overrides the fly method from the Bird class and leaves it unimplemented.  Unimplemented methods are almost always indicative of a design flaw.  Now, the statement Ostrich 'is-a' bird might still be correct. </p> <p>But if we apply the Liskov Principle here, which says: Objects should be replaceable with their subtypes without affecting the correctness of the program.    This test fails - because you cannot use an Ostrich object in all the places where you use the Bird object.  If you do so, and someone calls the fly() method on the Bird object, your program will fail. </p> <p>So the Liskov Substitution Principle requires a test standard that is more strict than the 'is-a' test. </p> <p>It wants us to move away from the 'is-a' way of thinking. </p> <p> </p> <p>This is a line often associated with the Liskov Substitution Principle: If it looks like a duck and quacks like a duck, but it needs batteries, you probably have the wrong abstraction!  Instead, the Liskov way of thinking should be: Objects should be replaceable with their subtypes without affecting the correctness of the program. </p>"},{"location":"SOLID%20Principles/Liskov%20Substitution%20Principle/#breaking-the-hierarchy","title":"Breaking the Hierarchy","text":"<p>Here we are going to examine the Liskov Substitution Principle in depth.  We will take 2 examples and analyze and find out the design flaws in each one of them.  Then we will apply the Liskov Substitution Principle and improve the design.  </p> <p>One example is discussed here, and we use the approach 'Breaking the Hierarchy'. </p> <p>In a given case, we have a generic Car class.  We have a racing car, the ones from the Formula One racing.  Racing car extends car. </p> <p>Let's see what the code looks like now.  </p> <p> </p> <p>The Car class has one method getCabinWidth() that returns the cabin width of the car. </p> <p>The RacingCar class overrides the getCabinWidth() function and leaves it unimplemented. </p> <p>Why? That's because racing cars have a set of specifications some of which might not match that of a generic car. </p> <p>In a generic car, we call the width as cabin width.  But in a racing car, there is no cabin. The interior space is called a cockpit.  So racing cars do not have a cabin width. But they have a cockpit width.  So the racing car implements a getCockpitWidth() method accordingly.  </p> <p>Now, lets create some objects of Cars and RacingCars and play around with it. </p> <p> </p> <p>In order to do this, we will look at a utils class called CarUtils.  So CarUtils instantiates 3 objects with reference Type Car.  Note that even though the reference for all 3 objects is Car,2 of them are generic car instances, the 3rd one is a racing car instance. </p> <p>We insert all 3 car reference objects into an ArrayList and name it myCars. </p> <p>Next we iterate through the car list and we print out the cabin width of each car. </p> <p>The first two objects are Car Objects. So the getCabinwidth method works fine.  But in the third iteration, the object is a RacingCar object.  And RacingCar overrides the getCabinWidth() method and leaves it unimplemented.  So in the third iterations, this code will not work correctly, because of an unimplemented method. </p> <p>So this is the design hole that has been exposed. </p> <p>So how do you fix this?   We have to strike at the root, which is the inheritance itself.  So we have to break the inheritance.  RacingCar should no longer extend Car.  Instead, we'll come up with a common parent for both.  So we create a new class called Vehicle which is a very generic class which can represent any modes of transportation including a truck, a boat, or an airplane.  Then we make both Car and RacingCar extend this Vehicle class. </p> <p>We need to restructure the code also accordingly. </p> <p>Let's see how that can be done.  First is the vehicle class, which has one method, called getInteriorWidth().  Note that it is neither cabin width nor cockpit width, but a much more generic abstraction called interior width.  Next we have the Car class that extends Vehicle. The 'Car' class overrides the getInteriorWidth(),which in turn calls its getCabinWidth() method internally. </p> <p>Next we have the RacingCar class that also extends Vehicle. </p> <p>The 'RacingCar' class also overrides the same getInteriorWidth(), which in turn calls its getCockpitWidth() method internally.  So that's the code restructuring that is needed to follow the new hierarchy. </p> <p>Finally, lets look at the same CarUtils class again.  We have renamed it to VehicleUtils by the way.  Again, we have the first two car objects and the third RacingCar object. </p> <p>First two iterations, the getInteriorWidth() method gets called on the car objects. The Car objects, in turn,call their getCabinWidth method.  In the third and last iteration, the getInteriorWidth() method gets called on the RacingCar objects, which in turn, call their getCockpitWidth method.  So all calls work correctly. And the whole thing is dynamic.  Because of the way we designed the hierarchy, we can simply iterate through any number of objects of reference type Vehicle and get the interior width,without ever bothering to know if they are Car objects or RacingCar objects. </p> <p>So the solution that we applied to this pattern of problems is called 'Breaking the hierarchy'. </p> <p>There is one more pattern where we apply the Liskov Substitution Principle in a different way. </p>"},{"location":"SOLID%20Principles/Liskov%20Substitution%20Principle/#tell-dont-ask","title":"Tell, Don't Ask","text":"<p>Case Study -&gt;  We have a generic Product class.  We also have an InHouseProduct class that extends the Generic Product class.  Just think of it from the perspective of an e-commerce website like Amazon.  Amazon sells a number of products on its website, mostly from third party sellers. But Amazon has its own set of products too, which they manufacture InHouse (Amazon Basics). </p> <p>Assume ,all Products get a base discount of 20%.  But, if it's an In-House Product, it gets 1.5 times the existing base discount.  This is the hierarchy that we need to implement. </p> <p>Lets start with an initial version.  </p> <p> </p> <p>We have a product class, which has a discount variable, and a getDiscount() method which returns the discount percentage.  Then we have the InHouseProduct class, which has a applyExtraDiscount method, that simply takes the existing discount and multiplies it by 1.5.  Note that the InHouseProduct does NOT override the getDiscount() method, so it will simply inherit the getDiscount() method from its parent, the Product class.  We will now use a bunch of these product and InHouseProduct objects in another class. This class is named PricingUtils.   It instantiates 3 objects, 2 of them generic products, and the third one an InHouse Product.  Note that all 3 are of type reference Product.  Next, we iterate through this list of objects which we just created.   For the first two products, the 'if' condition returns false, because they are not InHOuseProducts, and so the instanceof operator returns false.  So it skips what is inside the 'if' condition.  And it simply prints the discount for each of the two products. For the third product though, because the product is an instance of InHouseProduct, it gets inside the 'if' condition.  Then it calls the applyExtraDiscount() method which multiples the existing discount by 1.5.  Then it exits the 'if' loop and prints the final modified discount.   What we are seeing now in this Utils class, is NOT a good design. This is against the Liskov Substitution Principle.  We should have been able to deal with all the objects as Product objects itself instead of typecasting to InHouseProduct for some of them.  So how can restructure the code to achieve this?   First, we will override the getDiscount() method in the InHouseProduct class.  The overridden getDiscount() method, in turn, will call the applyExtraDiscount() method.   Now, we'll do the corresponding change in PricingUtils.  No change to the product instantiations.   The only difference is that the instanceof check is now gone.   We do not need to bother if the objects are instances of Generic Product or instances of InHouseProduct.  By doing this, the Liskov substitution test gets passed now.  So objects can be substituted by their subtypes without affecting the correctness of the program, in this particular example.   So we have the product reference here, but it contains a InHouseProduct, which is a subtype of Product.   Still the program works as expected, and its correctness is not affected.  This is a second pattern of problems, where the Liskov Substitution Principle is applied.   The solution to this pattern is called 'Tell, Don't Ask'.   In the initial version of code,we are ASKING or enquiring about the subtype , from INSIDE the Utils class.  But, lets go the final version now. Here, we are TELLING the Utils class, the Utils class does not need to ask anything. This is what we mean by 'Tell,Don't Ask'.   So, to sum up, we saw two patterns of problems, the solution to the first one was to break the hierarchy as we saw in the RacingCar example.  And the solution to the second one is to restructure the code, so as to follow the 'Tell, don't ask' rule. </p>"},{"location":"SOLID%20Principles/Open%20Closed%20Principle/","title":"Open Closed Principle","text":""},{"location":"SOLID%20Principles/Open%20Closed%20Principle/#introduction","title":"Introduction","text":"<p>'O' stands for Open Closed Principle . This is often abbreviated as OCP. </p> <p>As per OCP, \"Software components should be closed for modification, but open for extension\"  So this presents kind of a conundrum: How can something be open and closed at the same time?  To explain this, we will take the help of a real world analogy.  Most of you should be familiar with this device - this is the Nintendo Wii, a very popular gaming console, though its discontinued now. </p> <p> </p> <p>So when you buy the Wii console, what you get is the console itself which is the CPU tower kind of thing, and a basic remote controller. You can see both in the picture now. </p> <p>Wii also manufactures a number of accessories that can go with the Wii. For instance, there's Wii Zapper, which is a good accessory for playing FPS or first person shooter games in Wii.Setting this up is pretty simple. You just place the basic remote controller into a cradle inside the zapper and you are all set. </p> <p>So the Zapper has now added a really useful feature to the Wii. </p> <p>Another accessory is the Wii steering wheel, which is a really good accessory to have, if you are into racing games. </p> <p>The steering wheel is also setup in the same way. You just put the remote controller into the cradle and you are all set. </p> <p>Now think about this: In order to add the Zapper feature and the steering wheel feature, did we do any visible change to the CPU or the basic controller? </p> <p>No. Nothing.It was just plug n play. </p> <p>Think about a situation where Wii wanted you to move a jumper on the console's motherboard, if you wanted to add n accessory. </p> <p>That would have been so not cool, right? </p> <p>So when the Wii console came out of the factory, it came out as 'Closed for modification'.  The makers of Wii did not want their customers to go around opening their consoles on their own.  But still they made it possible for customers to add accessories and thereby add extensions or new features to the Wii.  This did NOT happen by accident. This happened because the engineers at Wii wanted this product to behave this way, and did a really good job designing it.  So, to sum up, the Wii was designed in such a way, that it is closed for modification, but open for extension. </p> <p>That was a simple analogy that I thought of to explain the concept of open closed principle.  </p> <p>Let's get back to software design now. </p> <p> </p> <p> So when we say software components should be closed for modification and open for extension, this is what we mean:   Closed for modification means: New features getting added to the software component,should NOT have to modify existing code.  Open for extension means: A software component should be extendable to add a new feature or to add a new behaviour to it. </p> <p>So, even though the term open-closed might sound like a conundrum, this is what it really means. </p>"},{"location":"SOLID%20Principles/Open%20Closed%20Principle/#real-world-analogies-and-code-snippets","title":"Real world analogies and code snippets","text":"<p>Lets look at a code example in this session. </p> <p>'One State' is an insurance company that primarily deals with health insurance.  The insurance calculations are coded as a Java library. This is one code snippet taken from the library.   This piece of code shows how the premium discounts are calculated. </p> <p>We have an InsurancePremiumDiscountCalculator class that has a calculatePremiumDiscountPercent. </p> <p>This method takes in as argument a HealthInsuranceCustomerProfile object. </p> <p>Lets see what this class is: So HealthInsuranceCustomerProfile has a isLoyalCustomer() method which returns a true if the current customer is a loyal customer. If not, it simply returns false. So this HealthInsuranceCustomerProfile object is the input for the calculatePremiumDiscountPercent method. </p> <p>The calculate method invokes the isLoyalCustomer() method on the incoming HealthInsuranceCustomerProfile object and does further discount calculations based on whether the customer is loyal or not. </p> <p>So far, so good. </p> <p>Come tomorrow, One State company acquires another insurance company which is primarily into Vehicle Insurance. </p> <p>They change their tagline accordingly. For all your health AND vehicle insurance needs. </p> <p>So we now have to support vehicle insurance discounts as well. </p> <p>One State has decided that the discount calculation is going to be the same, that is , it is going to be based on loyalty always, regardless of whether its health, vehicle, or any other type of insurance.   Okay, so to handle this, we add a new class into our design. VehicleInsuranceCustomerProfile.  This is just like theHealthInsuranceCustomerProfile class.It has a isLoyal() method which returns a boolean. </p> <p>So, are we done? No, this is where the problems begin. </p> <p>We now have to modify the Calculator class, because the calculate method currently takes in a HealthInsuranceCustomerProfile object.  We want it to take in a VehicleInsuranceCustomerProfile object as well.  The only way out is to add a new overloaded method which takes in a VehicleInsuranceCustomerProfile object. </p> <p>This is just the beginning. What if we want to handle home insurance too? </p> <p>We will need to add code again to this Calculator class. So, why is this not good? </p> <p>Because in order to add a new feature, we are having to touch existing code, which goes against our Open Closed Principle. The existing code is supposed to be closed for modification. </p> <p>Let's refactor our design and see if we can solve this problem.   We will revert our Calculator class back. We will create a new interface named CustomerProfile.  The interface defines only one method: isLoyalCustomer  We will make both our CustomerProfile classes implement this common interface. </p> <p>The beauty of this design lies in how it handles future extensions.Assume, One State enters home insurance business as well. So we will need to create a HomeInsuranceCustomerProfile object.  We make it implement the common CustomerProfile interface.   We do NOT need to touch the Calculator class at all. </p> <p>All we did is : add a new class by implementing an existing interface. </p> <p>Neither the calculator class, nor the interface nor any of the existing classes had to be modified.  See how this makes the process of adding extensions much cleaner.  What we saw now was an example of the Open Closed Principle. When we started out with this example, it was not following the open closed principle. So we picked holes in the design.  The refactoring that we did, made it conform to the open closed principle. </p> <p>After we did it, we saw how the design became much more robust, to handle future extensions in a more elegant way.So that was an example for the Open Closed Principle.  </p>"},{"location":"SOLID%20Principles/Open%20Closed%20Principle/#key-takeaways","title":"Key takeaways","text":""},{"location":"SOLID%20Principles/Open%20Closed%20Principle/#what-are-our-key-takeaways","title":"What are our Key takeaways?","text":"<p>S,o what is the principal benefit that we got from the new design in the previous example? </p> <ol> <li> <p>Ease of adding new features.  This can translate to cost savings.  Let's see how. Assume we do not follow the Open Closed Principle. Then for all additional features, we will end up having to modify the existing code.  The more the number of changes we introduce to the existing code, the more time we need to spend on testing and quality assurance, to make sure we did not introduce any bug into the existing code.  If we follow the open closed principle, it's not that we do not have test at all. We still need to test.   But testing a new piece of code is much easier than having to run a whole regression test suite on the existing code.  So to sum up , ease of adding new features , leads to minimum cost of developing and testing that is involved. </p> </li> <li> <p>Though not quite obvious is that we unknowingly did quite a bit of decoupling when we revamped our design.  So after we made our design conform to the Open Closed Principle, we ended up with components that were more loosely coupled with one another. So we unknowingly followed the Single Responsibility Principle as well! </p> </li> </ol> <p>So a key point here is that the SOLID principles are all intertwined and interdependent.  They are most effective when they are combined together.  So it is important that you get a wholesome view of all the SOLID principles.  </p>"},{"location":"SOLID%20Principles/Open%20Closed%20Principle/#a-word-of-caution","title":"A word of caution","text":"<p>Do not follow the open Closed Principle blindly.  You will end up with a huge number of classes that can complicate your overall design.  For e.g. if you were to fix a bug, and if you think existing code has to be modified in order to fix the bug effectively, then go do it.  Do not think about revamping the design just to fix the bug.  But if you see repeated occurrences of certain kinds of bugs, and you think that revamping your design can help reduce it, then by all means, do it. </p> <p>So it's a subjective, rather than an objective decision, to decide when and where to apply the Open Closed Principle to your design. </p>"},{"location":"SOLID%20Principles/Open%20Closed%20Principle/#code-examples","title":"Code examples","text":"<p>Its the same example we did in previous session.  </p>"},{"location":"SOLID%20Principles/Open%20Closed%20Principle/#wrap-up","title":"Wrap up","text":"<p>So the open closed principle says: \"Software components should be closed for modification, but open for extension\" Closed for modification means that New features getting added to the software component, should NOT have to modify existing code. </p> <p>And at the same time, Open for extension means: A software component should be extendable to add a new feature or to add a new behaviour to it. </p> <p>We saw an example where we picked holes in our design and revamped it to make it follow the open closed principle.  We saw that the benefits of following this principle can lead to considerable cost savings in the long run.  We also saw how the OPen closed principle and single responsibility principle can work together to achieve a better design. </p> <p>Finally, we also cautioned against creating a huge number of classes by applying open closed principle without applying any thought. This can be dangerous. </p>"},{"location":"SOLID%20Principles/Single%20Responsibility%20Principle/","title":"Single Responsibility Principle","text":""},{"location":"SOLID%20Principles/Single%20Responsibility%20Principle/#introduction","title":"Introduction","text":"<p>S stands for Single Responsibility Principle, often abbreviated and referred to as SRP.  So what is this principle?   The Single Responsibility Principle says that 'Every software  component should have one and only one responsibility' </p> <p>When we say software component, if we are talking in the context of an object oriented programming language like Java, the first thing that comes to our mind is a Java class. </p> <p>But it is to be noted that the term software component could also refer to a method or a function or even a module. </p> <p>So the rule states that a software component should have only one responsibility. </p> <p>Let me bring up a picture to explain this.  </p> <p>This is Swiss Army Knife. As you know,a swiss army knife is a combination of a number of useful tools, each one with a distinct purpose.  You can have anything from a can opener to a pair of mini scissors or even a screw-driver inside a Swiss Army knife. </p> <p>Although a Swiss Army knife is a versatile tool and much sought after and appreciated, when it comes to software, the rules change. </p> <p>If you think of the Swiss Army Knife as a software component, it violates the Single Responsibility Principle, for the reason that it has multiple responsibilities. </p> <p>So then what does SRP recommend?  If I can put a visual to it, this is it.   So this is a knife that has a single responsibility.  It can only be used to cut. You cannot use it as a can opener, you cannot use it as a screw-driver, you can only use it to cut.  This is a very high level introduction of the Single Responsibility Principle.  </p>"},{"location":"SOLID%20Principles/Single%20Responsibility%20Principle/#cohesion","title":"Cohesion","text":"<p>Take a look at this class.</p> <pre><code>public class Square {\n\n    private boolean highResolutionMonitor = true;\n    private int side = 5;\n\n    public int calculateArea() {\n        return side * side; // side\u00b2 - side ^ 2;\n    }\n\n    public int calculatePerimeter() {\n        return side * 4;\n    }\n\n    public void draw() {\n        if (highResolutionMonitor) {\n            // Render a high resolution image of a square\n        } else {\n            // Render a high normal image of a square\n        }\n    }\n\n    public void rotate() {\n        // Rotate the image of the square clockwise to\n        // the required degree and re-render\n    }\n}\n</code></pre> <p>The name of the class is Square.  It has 4 methods:  -   calculateArea(), -   calculatePerimeter() -   draw() -   rotate()</p> <p>The calculateArea and calculatePerimeter functions do exactly what they are supposed to do, they calculate the area and perimeter of a square given the length of its side.</p> <p>The draw() function renders the image of the square on the display.   It has multiple code flow, depending on what type of display is being used.  </p> <p>The rotate() function rotates the image of the square and re-renders it on the display.  </p> <p>In the context of this code snippet, we will learn about a new concept termed Cohesion.  </p> <p>Cohesion , in the software world is defined as the degree to which the various parts of a software component are related.  </p> <p>Here, you see garbage that looks unsegregated.    You cannot really come up with a relation between all the contents of this garbage can. It has a wide variety of items like plastic cans, beer bottles, paper waste etc.  </p> <p>But let's look at how it looks, after it gets segregated. Very nice.  </p> <p> </p> <p>Take a look at the yellow bin for plastics.   There are a number of plastic bottles inside the bin.   The bottles are NOT alike.   But if you look at all the contents of the yellow bin, they have a common relation.   They are all made up of plastic.  </p> <p>If we apply the definition of cohesion here, which says that cohesion is the degree of relation, we could say that the contents of the unsegregated waste bin have a low cohesion, and the contents of each of the segregated waste bins have a high cohesion.  </p> <p>Let's get back to our code snippet and apply the same principle here.   What do you make of the methods inside the Square class?  </p> <p>The methods calculateArea and calculatePerimeter are closely related, in that they deal with the measurements of a square.   So there is a high level of cohesion between these two methods.  </p> <p>The draw() method and the rotate() method deal with rendering the image of the square in a certain way on the display.   So there is a high level of cohesion between these two methods as well.  </p> <p>But if you take all of the methods as a whole, the level of cohesion is low.   For instance , the calculatePerimeter() method is not closely related to the draw() method as they deal with entirely different responsibilities.  </p> <p>So we are going to do some shuffling so as to increase the level of cohesion.  </p> <p>We take these draw and rotate methods and move them to a different class - SquareUI.  </p> <p>By doing this, even though I have split the methods into two classes, I have increased the level of cohesion in each of the classes.  </p> <p>All the two methods inside the Square class are now closely related, as both of them deal with the measurements of the square.  </p> <p>All the two methods inside the SquareUI class are now closely related, as both of them deal with the graphic rendering of the square.  </p> <p>So one aspect of the Single Responsibility Principle is that, we should always aim for high cohesion within a component.   Component means class in this case.  </p> <p>If there is high cohesion between all methods of a class, we can assign a single responsibility to all the methods as a whole.   For e.g. for the first class Square, we can safely say that the responsibility of the Square class as a whole is to deal with the measurements related to a square.  </p> <p>Similarly , for the second class SquareUI, we can safely say that the responsibility of the SquareUI class as a whole is to deal with rendering the image of a square.  </p> <p>So we just saw how aiming for higher cohesion can help us move towards conforming to the single responsibility principle.  </p>"},{"location":"SOLID%20Principles/Single%20Responsibility%20Principle/#coupling","title":"Coupling","text":"<p>Like cohesion, there is another concept that we need to look at - Coupling.   Coupling is defined as the level of inter-dependency between various software components.  </p> <p>Take a look at these two trains.   </p> <p>What do you notice about the width of the tracks?  One is Standard Gauge Rail, and other is Broad Gauge Rail.   - Standard Gauge is 1.4 m wide.   - Broad Gauge is 1.6m wide.  </p> <p>The difference in width makes it two entirely different railway systems. For e.g. the width of the train for standard gauge is different from that of broad gauge. One train cannot move on a track of a different gauge. In other words, a train is tightly coupled to its track.  </p> <p>Tight coupling may be a necessity in railways, but in software, tight coupling is an undesirable feature.  </p> <p>Why? See below example -   </p> <p>So Coupling is defined as the level of inter-dependency between various software components.  </p> <p>Let's see a code snippet now.   Here's a class Student.  </p> <p>One of the methods inside the Student class is the save() method.    The 'save' method will convert the student class into a serialized form and persist it into a Database.  </p> <p>You can see that this method deals with a lot of low level details related to handling record insertion into a database.  </p> <p>Let's assume the database you are using now is MySQL. Sometime in the future, if you decide to go with a NoSQL database like,say, MongoDB, most of this code will need to change.   So you can see that the Student class is tightly coupled with the database layer we use at the back end, just like the train is tightly coupled to the track.  </p> <p>The Student class should ideally deal with only basic student related functionalities like getting student id, date of birth, address etc.   The Student class should NOT be made cognizant of the low level details related to dealing with the back end database.   So tight coupling is bad in software.  </p> <p>So, How do we fix this?    We'll take the database related code, and we'll move it into a new Repository class.   Then we'll refer to this Repository method from inside the Student class.   By doing so, we have removed the tight coupling and made it loose.  </p> <p> So now if we change the underlying database, the Student class does NOT need to get changed and recompiled.   You only need to change the Repository class.   If you look at this in terms of responsibilities, the Student class has the responsibility of dealing with core student related data.   And the Repository class has a single responsibility of dealing with database operations.   So by removing tight coupling, and making the coupling loose, we are again abiding by the Single Responsibility principle.  </p> <p>So to sum up -  - we looked at two concepts - Cohesion and Coupling.  - We saw how low cohesion is bad.  - Single Responsibility Principle always advocates higher cohesion.  - We also saw how tight coupling is bad. Single Responsibility Principle always recommends loose coupling.  - So always aim for Higher Cohesion and Loose Coupling. </p>"},{"location":"SOLID%20Principles/Single%20Responsibility%20Principle/#reason-for-change-single-responsibility-principle-revisited","title":"Reason for change : Single Responsibility Principle revisited","text":"<p>The principle ,that we studied so far, states that 'Every software component should have one and only one responsibility'   In place of the term responsibility, we are going to put a new phrase 'reason to change'.   So now it reads 'Every software component should have one and only one reason to change'.   So what's this new phrase 'reason to change'?   In the words of the Greek Philosopher - Heraclitus. \"The only thing that is constant is change\". In other words, change is inevitable.   This quote applies to the software world as well.  </p> <p>Software is never dormant. It always keeps changing.   Let us explain this with an example.  </p> <p>We'll start with the same Student class that we used for our previous session, the original version of it.  </p> <p>Assume that this Student class is part of a software module which is already in production.   There could be multiple reasons for a software component to change in the future -   - A change in the student id format, as suggested by school management.   - A change in the student name format, as necessitated by a state law.   - A change in the database back end, as advised by the technical team. Another one.  </p> <p>So, we have 3 reasons to change.   What does the Single Responsibility Principle say?   As per the new definition, 'Every software component should have one and only one ~~responsibility~~ reason to change'    Why? What's the problem if our class has more reasons to change?  </p> <p>If a software component has multiple reasons to change, then the frequency of changes to it will increase.   Every change to a software component opens up the possibility of introducing bugs into the software.   So if there are frequent changes to a software component, the probability of introducing a bug goes up.   This would require more time and effort to be spent on re-testing the software after the changes are made, because we need to make sure we catch all the bugs before we release the modified version of the software.   More time and effort, means more money.  </p> <p> </p> <p>So the difference between following and not following the Single Responsibility Principle could be a considerable financial impact!   And this applies, not just to the Single Responsibility Principle, but to all other SOLID principles as well.  </p> <p>So, back to our code snippet...   </p> <p>How do we fix this? What can we do about the multiple reasons to change?   We'll repeat the same move we did in our last session.   We'll take the database operations out and move it to a separate Repository class. Because we split the classes, lets split the 'reasons to change' as well.    So the Student class is left with 2 reasons to change.   And the Repository class has one reason to change.  </p> <p></p> <p>2 is still a problem, isn't it. We are supposed to have only one reason to change, right?   Technically, yes. But if the reasons are closely related to one another,  you can go ahead and combine them.</p> <p>So if we examine the 2 reasons to change for the Student class closely, one is related to student id, and another is related to student name. We could combine both these and say 'changes to student profile'.  </p> <p>So now both our classes follow the new definition of the Single Responsibility Principle which says 'Every software component should have one and only one reason to change'   So this new definition which replaces Responsibility with Reason to Change is just a new perspective.   At the end of the day, the action which we took to fix is the same, in the previous session and the current session.   So, these two definitions are pretty much the same.  </p>"},{"location":"SOLID%20Principles/Single%20Responsibility%20Principle/#code-exercise","title":"Code exercise","text":"<p>Given below code, we find 3 reasons to change -&gt;   </p> <p>We take out 2 methods -&gt; employeeRepository and TaxCalculation.   </p> <p> </p> <p>Now our final code looks like this -&gt;   </p> <p>So we have applied the single responsibility principle here.   But a word of caution is that, do not keep creating a huge number of classes just like that.   For instance, it is a bad idea to create separate classes to handle employee id, separate class to handle employee name etc.   If you can group the responsibilities together in a sensible way,then do it.   Else, you will end up having a huge number of classes thereby adding unnecessary complexity to your code.   So follow the principle, but use your discretion and make an informed judgment.  </p>"},{"location":"SQL/docs/Common%20Table%20Expressions/","title":"Common Table Expressions (CTEs)","text":""},{"location":"SQL/docs/Common%20Table%20Expressions/#common-table-expressions-ctes","title":"Common Table Expressions (CTEs)","text":""},{"location":"SQL/docs/Common%20Table%20Expressions/#what-are-common-table-expressions-ctes","title":"What are Common Table Expressions (CTEs)?","text":"<p>A Common Table Expression (CTE) is the result set of a query which exists temporarily and for use only within the context of a larger query.  Much like a derived table, the result of a CTE is not stored and exists only for the duration of the query. </p> <p>Common Table expression (CTE) is a temporary named result set that you can reference within a SELECT, INSERT, UPDATE, or DELETE statement.  You can also use a CTE in a CREATE a view, as part of the view\u2019s SELECT query. </p>"},{"location":"SQL/docs/Common%20Table%20Expressions/#how-are-ctes-helpful","title":"How are CTEs helpful?","text":"<p>CTEs, like database views and derived tables, enable users to more easily write and maintain complex queries via increased readability and simplification.  This reduction in complexity is achieved by deconstructing ordinarily complex queries into simple blocks to be used, and reused if necessary, in rewriting the query.   Example use cases include:  - Needing to reference a derived table multiple times in a single query  - An alternative to creating a view in the database  - Performing the same calculation multiple times over across multiple query components </p>"},{"location":"SQL/docs/Common%20Table%20Expressions/#how-to-create-cte","title":"How to create CTE?","text":"<p>We can define CTEs by adding a WITH clause directly before SELECT, INSERT, UPDATE, DELETE, or MERGE statement.  The WITH clause can include one or more CTEs separated by commas.  The following syntax can be followed: </p> <pre><code>[WITH  [, ...]]  \n\n::=\ncte_name [(column_name [, ...])]\nAS (cte_query) \n</code></pre> <p>After you define your WITH clause with the CTEs, you can then reference the CTEs as you would refer any other table.   However, you can refer a CTE only within the execution scope of the statement that immediately follows the WITH clause. After you\u2019ve run your statement, the CTE result set is not available to other statements.</p>"},{"location":"SQL/docs/Data-Warehousing-basics/","title":"Data Warehouse","text":""},{"location":"SQL/docs/Data-Warehousing-basics/#what-is-data-warehouse","title":"What is Data Warehouse?","text":"<p>Put simply, it\u2019s a central place where data is stored for the purpose of analysis and reporting. The data may be collected from a variety of sources. It\u2019s organised to provide complete data for users that can be easily understood in a business context. Different from databases in that it\u2019s purpose is for analysis</p> <p>-In companies, we use dimensional modelling to design our data warehouses. Dimensional modelling always uses two types of tables \u2013 facts and dimensions.</p> <p>a. Fact Tables- Contain the measurements, metrics or facts of a business process i.e. Transactions (items &amp; baskets) Each fact will include measures (e.g. spend) and context data. Describe the measurements/facts in a transaction table \u2013 what was bought/how much it cost, etc. </p> <p>b. Dimension Tables- Stores attributes that describe the objects in a fact table i.e. Stores, Products, Customers</p> <p>They are linked together \u2013 this is a relational model - a. Primary key -  Is the attribute or a set of attributes in an entity whose value(s) guarantee only one tuple (row) exists for each value. The fact table will also include foreign keys that relate each fact to our dimension tables.</p> <p>b. Foreign key  The primary key of another table referenced here. Each entity in a dimension table will contain an attributes that describe that entity. There will also be a key that is used to join the dimension table to the fact table.</p>"},{"location":"SQL/docs/Data-Warehousing-basics/#characteristics-of-data-warehouse","title":"Characteristics of Data warehouse","text":"<ul> <li>Data warehouse is a database which is separate from operational database which stores historical information also.</li> <li>Data warehouse database contains transaction(OLTP) as well as analytical data(OLAP).</li> <li>Data warehouse helps higher management  to take strategic as well as tactical decisions using historical or current data.</li> <li>Data warehouse helps consolidated historical data analysis.</li> <li>Data warehouse helps business user to see the current trends to run the business.</li> <li>Data warehouse is used for reporting and data analysis purpose.</li> </ul>"},{"location":"SQL/docs/Data-Warehousing-basics/#types-of-data-warehouse-systems","title":"Types of Data warehouse systems","text":"<p>a. Data Mart Data Mart is a simplest set of Data warehouse which is used to focus on single functional area of the business.</p> <p>b. Online Analytical Processing (OLAP) - Refer  OLAP is used at strategic level and contains aggregated data, covering number of years. The key purpose to use OLAP system is to reduce the query response time and increase the effectiveness of reporting. So, data is denormalized. OLAP uses Star-Schema,Snowflakes schema or Fact-Dimensions. OLAP database stores aggregated historical data in multidimensional schema. eg - summaries data.</p> <p>c. Online Transactional Processing (OLTP) - Refer  It is operational database, maintaining large number of small daily transactions like insert,update and delete. Data is normailzed. OLTP uses Entity Relations. OLTP system maintains concurrency and it avoids the centralization so as to avoid the single point of failures. Data concurrency and integrity = focus.</p> <p>d. Predictive Analysis</p>"},{"location":"SQL/docs/Data-Warehousing-basics/#difference-between-data-warehouse-and-data-mart","title":"Difference between Data Warehouse and Data Mart","text":"<p>a. Definition - The Data Warehouse is a large repository of data collected from different organizations or departments within a corporation. The data mart is an only sub-type of a Data Warehouse. It is designed to meet the need of a certain user group.</p> <p>b. Focus - Data warehouse focuses on multiple business areas. Data mart focuses only on single subject area.</p> <p>c. Usage - DW - It helps to take a strategic decision. DM  - The data mart is used to take tactical decisions for growth of business.</p> <p>d. Type of system - DW - This is centralized system where one fact is at center surrounded by dimension tables. DM - Data mart system is de-centralized system</p> <p>e. data model - DW = top down DM - bottom up</p> <p>f. source - Data warehouse data comes from multiple heterogeneous data sources. Data mart data is data of only one business area.Many times it will come from only one data source.</p> <p>g. Implementation Time - Data warehouse contains all data which will come from multiple data sources. It will take time to build data warehouse. The Time to build data warehouse is months to years. Data mart is small data warehouse which will contain the data of only a single business area. The implementation time to build data mart is in months.</p>"},{"location":"SQL/docs/Data-Warehousing-basics/#data-warehousing","title":"Data Warehousing","text":"<p>-It\u2019s the process of TRANSFORMING data into information and making it available to users in a TIMELY enough manner to make a difference.</p>"},{"location":"SQL/docs/Data-Warehousing-basics/#different-use-cases-of-etl","title":"Different use cases of ETL","text":"<p>a. Data Warehousing - User needs to fetch the historical data as well as current data for developing data warehouse. The Data warehouse data is nothing but combination of historical data as well as transactional data. Its data sources might be different.User needs to fetch the data from multiple heterogeneous systems and load it in to single target system which is also called as data warehouse.</p> <p>b. Data Migration ETL tools are widely used in data migration projects.  If the organization is managing the data in oracle 10 g previously and now organization wants to go for SQL server cloud database then there is need to migrate the data from Source to Target. To do this kind of migration the ETL tools are very useful.  If user wants to write the code of ETL it is very time consuming process. To make this simple the ETL tools are very useful in which the coding is simple as compare to PL SQL or T-SQL code. So ETL process i very useful in Data migration projects.</p> <p>c. Data Integration Now a days big organizations are acquiring small firms.  Obviously the data source for the different organizations may be different.We need to integrate the data from one organization to other organization.  These kind of integration projects need the ETL process to extract the data,transform the data and load the data.</p> <p>d. Third Party data management- many a times company outsources process to different vendors. eg - telecom - billing managed by one and CRM by other vendor.  If CRM company needs some data from the company who is managing the Billing. That company will receive a data feed from the other company.  To load the data from the feed ETL process is used.</p>"},{"location":"SQL/docs/Data-Warehousing-basics/#etl","title":"ETL","text":""},{"location":"SQL/docs/Data-Warehousing-basics/#extract","title":"Extract","text":"<p>Extract source data from our client- Once the data we will receive has been agreed, it is transferred from the client to us via our secure FTP system called Axway. This data is known as source data. Once the data has been received, it is validated according to the retailer-specific rules which are outlined in the DIS. If the data does not reflect what is outlined in the DIS changes may need to be made \u2013 either by updating the DIS or requesting a resupply. Once we agree the data is in the correct format it can be read in. Data Extraction can be - Full or Partial (Delta).</p> <p>DIS - The data we receive is mapped in a document known as Data Interface Specification (DIS).</p> <p>QA check in Source / RAW layer- A key section of RAW is quality assurance (QA). We carry out standard checks to ensure data is \u201chealthy\u201d and without errors Main focus of the checks is the fact tables, and relation on the fact data with the key dimension tables. Checks include:  * Number of baskets in the basket and item tables  * Levels of spend  * Missing foreign/primary keys Any issues found are recorded and can either be resolved with the solution DSG or may require input from the client Once these checks are successfully completed the build moves forward into the PREP stage.</p>"},{"location":"SQL/docs/Data-Warehousing-basics/#transform","title":"Transform","text":"<p>Transform this client data to meet the operational and business needs of our internal database. Within prep we transform the client data into a standardized format within the guidelines of Marketplace What types of transformations do we perform? i. Reject bad data - Bad data i.e. record where key info is missing. What is rejected depends on the business rules. We keep a record of rejected data by extracting them to a separate table, mark the missing field.</p> <p>ii. Remove duplicates Duplicate data can have negative impact on results.  Important to understand if it is really a duplicate before removing.</p> <p>iii. Convert fields Convert from character to date and numeric fields where relevant e.g. spends/quantities.</p> <p>iv. Text manipulations Changes format e.g. change lookup value to descriptive form.</p> <p>v. Merges with other tables Merge lookups/useful fields that should be on specific table.</p> <p>vi. Aggregate data Sometimes need to roll up products in same basket or even create basket table - involves summations.</p> <p>vii. Rename fields Rename to make more meaningful \u2013 esp if in a foreign language.</p> <p>viii. Create standard fields Essential to marketplace, same naming conventions e.g. dib_bask_code.</p>"},{"location":"SQL/docs/Data-Warehousing-basics/#load","title":"Load","text":"<p>Load into our analytical data mart within Marketplace The load is automated, so you will not be expected to know exactly what occurs. Here is an overview: Inbound Outbound -extracts and updates required data into standard structure Staging - manage slowly changing dimensions, generate surrogate keys and  *create skeleton records</p> <p>SCD - a dimension is considered a SCD when its attributes remain almost constant over time, requiring relatively minor alterations to represent the evolved state. Surrogate Keys - system-generated and non-persistent integer keys which replace foreign keys. Skeleton records - Generated when a foreign key in a fact table does not have a match in the dimension table. A dummy or \u2018skeleton\u2019 record is created in the dimension table.</p> <p>There are following 3 Types of Data Loading Strategies : i. Initial load : Populating all the data tables from source system and loads it in to data warehouse table. ii. Incremental Load : Applying the ongoing changes as necessary in periodic manner. iii. Full Refresh : Completely erases the data from one or more tables and reload the fresh data.</p>"},{"location":"SQL/docs/Data-Warehousing-basics/#star-and-snowflake-schema","title":"Star and Snowflake schema","text":""},{"location":"SQL/docs/Data-Warehousing-basics/#star-schema","title":"Star Schema","text":"<p>In the star schema design, a single object (the fact table) sits in the middle and is radically connected to other surrounding objects (dimension lookup tables) like a star. Each dimension is represented as a single table. The primary key in each dimension table is related to a foreign key in the fact table.</p> <p>All measures in the fact table are related to all the dimensions that fact table is related to. In other words, they all have the same level of granularity.</p> <p>A star schema can be simple or complex. A simple star consists of one fact table; a complex star can have more than one fact table.</p>"},{"location":"SQL/docs/Data-Warehousing-basics/#snowflake-schema","title":"Snowflake Schema","text":"<p>It is an extension of star schema. In a star schema, each dimension is represented by a single dimensional table, whereas in a snowflake schema, that dimensional table is normalized into multiple lookup tables, each representing a level in the dimensional hierarchy.</p> <p>Adv - improvement in query performance due to minimized disk storage requirements and joining smaller lookup tables.</p> <p>Disadvantage- additional maintenance efforts needed due to the increase number of lookup tables.</p>"},{"location":"SQL/docs/Data-Warehousing-basics/#fact-table-granularity","title":"Fact Table Granularity","text":"<p>The first step in designing a fact table is to determine the granularity of the fact table. By granularity, we mean the lowest level of information that will be stored in the fact table.  This constitutes two steps:</p> <p>i. Determine which dimensions will be included - this depends on business process being targetted.</p> <p>ii. Determine where along the hierarchy of each dimension the information will be kept - This depends on requirements. Eg - if client wants hourly reports, then fact table will keep hour as lowest level of granularity. If daily reports are fine, then date_id is lowest level of granularity.</p> <p>The determining factors usually goes back to the requirements.</p>"},{"location":"SQL/docs/Data-Warehousing-basics/#fact-and-fact-table-types","title":"Fact And Fact Table Types","text":"<p>There are three types of facts: i. Additive: Additive facts are facts that can be summed up through all of the dimensions in the fact table. ii. Semi-Additive: Semi-additive facts are facts that can be summed up for some of the dimensions in the fact table, but not the others. iii. Non-Additive: Non-additive facts are facts that cannot be summed up for any of the dimensions present in the fact table.</p> <p>eg1 - Additive Fact - Consider a retailer fact table with following columns - - Date - Store - Product - Sales_Amount</p> <p>The purpose of this table is to record the sales amount for each product in each store on a daily basis.</p> <p>Sales_Amount is an additive fact, because you can sum up this fact along any of the three dimensions present in the fact table -- date, store, and product.</p> <p>eg2A - Semi-Additive Fact and Non-Additive Fact - Say we are a bank with the following fact table: - Date - Account - Current_Balance - Profit_Margin</p> <p>The purpose of this table is to record the current balance for each account at the end of each day, as well as the profit margin for each account for each day.</p> <p>Current_Balance and Profit_Margin are the facts.</p> <p>Current_Balance is a semi-additive fact, as it makes sense to add them up for all accounts (what's the total current balance for all accounts in the bank?), but it does not make sense to add them up through time (adding up all current balances for a given account for each day of the month does not give us any useful information).</p> <p>Profit_Margin is a non-additive fact, for it does not make sense to add them up for the account level or the day level.</p> <p>eg 2B - semi -additive - distinct customers who shopped in a day = semi additive. Across all stores, this number can be aggregated. For example, store A has 300 customers and store B has 200 customers. So total 500 customers. But cant add across date dimension. So no summation possible across days in a week.</p> <p>non-additive = %age loyalty transaction in a day. For example, store A has 30% sales as loyalty count, and store B has 40%. But we cant add these two figures to find overall loyalty sales.</p> <p>Based on the above classifications, there are two types of Fact TABLES: * Cumulative: This type of fact table describes what has happened over a period of time. For example, this fact table may describe the total sales by product by store by day. The facts for this type of fact tables are mostly additive facts. The first example presented here is a cumulative fact table. * Snapshot: This type of fact table describes the state of things in a particular instance of time, and usually includes more semi-additive and non-additive facts. The second example presented here is a snapshot fact table.</p>"},{"location":"SQL/docs/Data-Warehousing-basics/#slowly-changing-dimensions","title":"Slowly Changing Dimensions","text":"<p>The \"Slowly Changing Dimension\" problem is a common one particular to data warehousing. In a nutshell, this applies to cases where the attribute for a record varies over time. There are in general three ways to solve this type of problem, and they are categorized as follows:</p>"},{"location":"SQL/docs/Data-Warehousing-basics/#type-1","title":"Type 1","text":"<p>The new record replaces the original record.  No trace of the old record exists.  In other words, no history is kept. Advantage - * easiest to handle as no need to maintain history.</p> <p>Disadvantage- *History is lost. Cant track past behavior.</p> <p>So, Type 1 slowly changing dimension should be used when it is not necessary for the data warehouse to keep track of historical changes.</p>"},{"location":"SQL/docs/Data-Warehousing-basics/#type-2","title":"Type 2","text":"<p>A new record is added into the customer dimension table.  Therefore, the customer is treated essentially as two people. Both the original and the new record will be present.  The new record gets its own primary key.</p> <p>Advantages: - This allows us to accurately keep all historical information.</p> <p>Disadvantages: - This will cause the size of the table to grow fast.    In cases where the number of rows for the table is very high to start with, storage and performance can become a concern. - This necessarily complicates the ETL process.</p>"},{"location":"SQL/docs/Data-Warehousing-basics/#type-3","title":"Type 3","text":"<p>The original record is modified to reflect the change. We add more column to track change.  But this is feasible only if changes to be tracked are finite. For example, phone or address changes more than once will complicate things.</p>"},{"location":"SQL/docs/Data-Warehousing-basics/#data-integrity","title":"Data Integrity","text":"<p>Data integrity refers to the validity of data, meaning data is consistent and correct.  In the data warehousing field, we frequently hear the term, \"Garbage In, Garbage Out.\"  If there is no data integrity in the data warehouse, any resulting report and analysis will not be useful.</p> <p>In a data warehouse or a data mart, there are 3 areas of where data integrity needs to be enforced:</p> <p>a. Database level We can enforce data integrity at the database level.  Common ways of enforcing data integrity include:</p> <p>i. Referential integrity The relationship between the primary key of one table and the foreign key of another table must always be maintained.  For example, a primary key cannot be deleted if there is still a foreign key that refers to this primary key.</p> <p>ii. Primary key / Unique constraint Primary keys and the UNIQUE constraint are used to make sure every row in a table can be uniquely identified.</p> <p>iii. Not NULL vs. NULL-able For columns identified as NOT NULL, they may not have a NULL value.</p> <p>iv. Valid Values Only allowed values are permitted in the database.  For example, if a column can only have positive integers, a value of '-1' cannot be allowed.</p> <p>b. ETL process For each step of the ETL process, data integrity checks should be put in place to ensure that source data is the same as the data in the destination.  Most common checks include record counts or record sums.</p> <p>c. Access level We need to ensure that data is not altered by any unauthorized means either during the ETL process or in the data warehouse.  To do this, there needs to be safeguards against unauthorized access to data (including physical access to the servers), as well as logging of all data access history.  Data integrity can only ensured if there is no unauthorized access to the data.</p> <p>4F. Factless Fact Table A factless fact table is a fact table that does not have any measures. It is essentially an intersection of dimensions. On the surface, a factless fact table does not make sense, since a fact table is, after all, about facts. However, there are situations where having this kind of relationship makes sense in data warehousing.</p> <p>eg1 - student class attendance record. In this case, the fact table would consist of 3 dimensions: the student dimension, the time dimension, and the class dimension.  This factless fact table would look like the following: Fact Table \"school_attendance\"</p> date_id classId student_id 02-02-2020 1 101 02-02-2020 1 102 02-02-2020 1 103 ---------------------------------- The only measure that you can possibly attach to each combination is \"1\" to show the presence of that particular combination.  However, adding a fact that always shows 1 is redundant because we can simply use the COUNT function in SQL to answer the same questions. <p>eg 2 - online sales in CRV. columns - date_id, store_id, till_id, pos_id In essence it contains only 1 column = basket_key. If a basket is in this table, it means its online sale, else offline sale.</p> <p>eg3 - Promotion data. Table structure could be - date_id | store_id| promo_type| promo_id| basket_key</p> <p>promo_type = Promotion can be online, in-store, flat discount, coupon, voucher, etc. Above table contains info of promotion applied on a basket. No measurable fact exists here. But why needed ? Transaction data contains info of what item was sold on promotion. But promotion data contains information of all the promotion during the purchase period. That is, all products having promotion applied on them, including those which were not sold in spite of promotion. And so, this promotion table becomes pivotal even though it contains no measurable fact.</p> <p>Why need factless facts?  Factless fact tables offer the most flexibility in data warehouse design. For example, one can easily answer the following questions with this factless fact table:  * How many students attended a particular class on a particular day?  * How many classes on average does a student attend on a given day? Without using a factless fact table, we will need two separate fact tables to answer the above two questions. With the above factless fact table, it becomes the only fact table that's needed.</p>"},{"location":"SQL/docs/Data-Warehousing-basics/#junk-dimension","title":"Junk Dimension","text":"<p>There are columns in Fact table which can have only a few or 2 kind of values - true or false, 1 or 0, etc. eg = bulk Vs non-bulk online vs offline promo vs non-promo vs hybrid sale etc.</p> <p>From business point of view, capturing above info in Fact table is very important. Issue -having these info will only make our fact table bulky and eventually unmanageable.</p> <p>Soln - junk dimension. eg - CRV basket channel seg - shop_channel_code in 0,1,2 or 3 - covers both bulk/non-bulk and online/offline.</p> <p>this would reduce 2 columns in fact table to 1. we can expand scope of above column to include promo info, and in that way we replace 3 fact columns by 1. This will result in a data warehousing environment that offer better performance as well as being easier to manage.</p> <p>[reference] (https://www.1keydata.com/datawarehousing/junk-dimension.html)</p>"},{"location":"SQL/docs/SQL%20leetcode/","title":"SQL leetcode","text":""},{"location":"SQL/docs/SQL%20leetcode/#second-highest-salary","title":"Second Highest Salary","text":"<p>Link : https://leetcode.com/problems/second-highest-salary/ </p> <pre><code>Table: Employee\n\n\n+-------------+------+\n| Column Name | Type |\n+-------------+------+\n| id          | int  |\n| salary      | int  |\n+-------------+------+\nid is the primary key column for this table.\nEach row of this table contains information about the salary of an employee.\n</code></pre> <p>Write an SQL query to report the second highest salary from the Employee table. If there is no second highest salary, the query should report null. The query result format is in the following example.</p> <pre><code>Example 1:\n\nInput: \nEmployee table:\n+----+--------+\n| id | salary |\n+----+--------+\n| 1  | 100    |\n| 2  | 200    |\n| 3  | 300    |\n+----+--------+\nOutput: \n+---------------------+\n| SecondHighestSalary |\n+---------------------+\n| 200                 |\n+---------------------+\nExample 2:\n\nInput: \nEmployee table:\n+----+--------+\n| id | salary |\n+----+--------+\n| 1  | 100    |\n+----+--------+\nOutput: \n+---------------------+\n| SecondHighestSalary |\n+---------------------+\n| null                |\n+---------------------+\n</code></pre> <p>Ans </p> <pre><code>select  max(salary) as SecondHighestSalary from\n(select \n    salary, \n    dense_rank() over (order by salary desc) as sal_rank \n from employee\n ) \nwhere sal_rank=2;\n</code></pre> <p>Note -&gt; if Nth highest salary --&gt;  Link : https://leetcode.com/problems/nth-highest-salary/ </p> <pre><code>CREATE FUNCTION getNthHighestSalary(N IN NUMBER) RETURN NUMBER IS\nresult NUMBER;\nBEGIN\n    /* Write your PL/SQL query statement below */\n\n    select max(salary) into result\n    from \n        (\n        select salary, \n               dense_rank() over (order by salary desc) as sal_rank\n            from employee\n        ) where sal_rank=N;\n\n    RETURN result;\nEND;\n\n</code></pre>"},{"location":"SQL/docs/SQL%20leetcode/#find-median-given-frequency-of-numbers","title":"Find Median Given Frequency of Numbers","text":"<p>Link : https://leetcode.com/problems/find-median-given-frequency-of-numbers/  Table: Numbers </p> <pre><code>\n+-------------+------+\n| Column Name | Type |\n+-------------+------+\n| num         | int  |\n| frequency   | int  |\n+-------------+------+\nnum is the primary key for this table.\nEach row of this table shows the frequency of a number in the database.\n\n</code></pre> <p>The median is the value separating the higher half from the lower half of a data sample.  Write an SQL query to report the median of all the numbers in the database after decompressing the Numbers table. Round the median to one decimal point.  The query result format is in the following example. </p> <pre><code>Example 1:\n\nInput: \nNumbers table:\n+-----+-----------+\n| num | frequency |\n+-----+-----------+\n| 0   | 7         |\n| 1   | 1         |\n| 2   | 3         |\n| 3   | 1         |\n+-----+-----------+\nOutput: \n+--------+\n| median |\n+--------+\n| 0.0    |\n+--------+\nExplanation: \nIf we decompress the Numbers table, we will get [0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 3], so the median is (0 + 0) / 2 = 0.\n</code></pre> <p>Ans </p> <pre><code>select round(avg(num),1) as median\nfrom\n        (\n        select\n            num,\n            frequency,\n            lag(cum_sum,1,0) over (order by num) as prev_sum,\n            cum_sum,\n            medium_num\n        from\n                (select \n                    num,\n                    frequency,\n                    sum(frequency) over (order by num) as cum_sum,\n                    sum(frequency) over () / 2 as medium_num\n                from numbers)   a \n            ) b\nwhere medium_num between prev_sum and  cum_sum;\n</code></pre>"},{"location":"SQL/docs/SQL%20leetcode/#find-cumulative-salary-of-an-employee","title":"Find Cumulative Salary of an Employee","text":"<p>Table: Employee </p> <pre><code>+-------------+------+\n| Column Name | Type |\n+-------------+------+\n| id          | int  |\n| month       | int  |\n| salary      | int  |\n+-------------+------+\n(id, month) is the primary key for this table.\nEach row in the table indicates the salary of an employee in one month during the year 2020.\n\n</code></pre> <p>Write an SQL query to calculate the cumulative salary summary for every employee in a single unified table. </p> <p>The cumulative salary summary for an employee can be calculated as follows:  - For each month that the employee worked, sum up the salaries in that month and the previous two months. This is their 3-month sum for that month. If an employee did not work for the company in previous months, their effective salary for those months is 0.  - Do not include the 3-month sum for the most recent month that the employee worked for in the summary.  - Do not include the 3-month sum for any month the employee did not work.  - Return the result table ordered by id in ascending order. In case of a tie, order it by month in descending order. </p> <p>The query result format is in the following example. </p> <p>Example 1: </p> <pre><code>Input: \nEmployee table:\n+----+-------+--------+\n| id | month | salary |\n+----+-------+--------+\n| 1  | 1     | 20     |\n| 2  | 1     | 20     |\n| 1  | 2     | 30     |\n| 2  | 2     | 30     |\n| 3  | 2     | 40     |\n| 1  | 3     | 40     |\n| 3  | 3     | 60     |\n| 1  | 4     | 60     |\n| 3  | 4     | 70     |\n| 1  | 7     | 90     |\n| 1  | 8     | 90     |\n+----+-------+--------+\nOutput: \n+----+-------+--------+\n| id | month | Salary |\n+----+-------+--------+\n| 1  | 7     | 90     |\n| 1  | 4     | 130    |\n| 1  | 3     | 90     |\n| 1  | 2     | 50     |\n| 1  | 1     | 20     |\n| 2  | 1     | 20     |\n| 3  | 3     | 100    |\n| 3  | 2     | 40     |\n+----+-------+--------+\nExplanation: \nEmployee '1' has five salary records excluding their most recent month '8':\n- 90 for month '7'.\n- 60 for month '4'.\n- 40 for month '3'.\n- 30 for month '2'.\n- 20 for month '1'.\nSo the cumulative salary summary for this employee is:\n+----+-------+--------+\n| id | month | salary |\n+----+-------+--------+\n| 1  | 7     | 90     |  (90 + 0 + 0)\n| 1  | 4     | 130    |  (60 + 40 + 30)\n| 1  | 3     | 90     |  (40 + 30 + 20)\n| 1  | 2     | 50     |  (30 + 20 + 0)\n| 1  | 1     | 20     |  (20 + 0 + 0)\n+----+-------+--------+\nNote that the 3-month sum for month '7' is 90 because they did not work during month '6' or month '5'.\n\nEmployee '2' only has one salary record (month '1') excluding their most recent month '2'.\n+----+-------+--------+\n| id | month | salary |\n+----+-------+--------+\n| 2  | 1     | 20     |  (20 + 0 + 0)\n+----+-------+--------+\n\nEmployee '3' has two salary records excluding their most recent month '4':\n- 60 for month '3'.\n- 40 for month '2'.\nSo the cumulative salary summary for this employee is:\n+----+-------+--------+\n| id | month | salary |\n+----+-------+--------+\n| 3  | 3     | 100    |  (60 + 40 + 0)\n| 3  | 2     | 40     |  (40 + 0 + 0)\n+----+-------+--------+\n</code></pre> <p>Ans </p> <pre><code>select\n    id, \n    month,\n    sum(salary) over (partition by id order by month range between 2 PRECEDING and current row)  as salary\nfrom    \n    (\n    select \n        id,\n        month,\n        salary,\n        max(month) over (partition by id order by month rows between unbounded preceding and unbounded following) as max_month_for_employee     \n    from employee\n     )\n where month!=max_month_for_employee\n order by id, month desc;  \n</code></pre>"},{"location":"SQL/docs/SQL%20leetcode/#average-salary-departments-vs-company","title":"Average Salary: Departments VS Company","text":"<p>Link : https://leetcode.com/problems/average-salary-departments-vs-company/ </p> <pre><code>Table: Salary\n\n+-------------+------+\n| Column Name | Type |\n+-------------+------+\n| id          | int  |\n| employee_id | int  |\n| amount      | int  |\n| pay_date    | date |\n+-------------+------+\nid is the primary key column for this table.\nEach row of this table indicates the salary of an employee in one month.\nemployee_id is a foreign key from the Employee table.\n\n\nTable: Employee\n\n+---------------+------+\n| Column Name   | Type |\n+---------------+------+\n| employee_id   | int  |\n| department_id | int  |\n+---------------+------+\nemployee_id is the primary key column for this table.\nEach row of this table indicates the department of an employee.\n</code></pre> <p>Write an SQL query to report the comparison result (higher/lower/same) of the average salary of employees in a department to the company's average salary.  Return the result table in any order.  The query result format is in the following example.  Example 1: </p> <pre><code>Input: \nSalary table:\n+----+-------------+--------+------------+\n| id | employee_id | amount | pay_date   |\n+----+-------------+--------+------------+\n| 1  | 1           | 9000   | 2017/03/31 |\n| 2  | 2           | 6000   | 2017/03/31 |\n| 3  | 3           | 10000  | 2017/03/31 |\n| 4  | 1           | 7000   | 2017/02/28 |\n| 5  | 2           | 6000   | 2017/02/28 |\n| 6  | 3           | 8000   | 2017/02/28 |\n+----+-------------+--------+------------+\nEmployee table:\n+-------------+---------------+\n| employee_id | department_id |\n+-------------+---------------+\n| 1           | 1             |\n| 2           | 2             |\n| 3           | 2             |\n+-------------+---------------+\nOutput: \n+-----------+---------------+------------+\n| pay_month | department_id | comparison |\n+-----------+---------------+------------+\n| 2017-02   | 1             | same       |\n| 2017-03   | 1             | higher     |\n| 2017-02   | 2             | same       |\n| 2017-03   | 2             | lower      |\n+-----------+---------------+------------+\nExplanation: \nIn March, the company's average salary is (9000+6000+10000)/3 = 8333.33...\nThe average salary for department '1' is 9000, which is the salary of employee_id '1' since there is only one employee in this department. So the comparison result is 'higher' since 9000 &gt; 8333.33 obviously.\nThe average salary of department '2' is (6000 + 10000)/2 = 8000, which is the average of employee_id '2' and '3'. So the comparison result is 'lower' since 8000 &lt; 8333.33.\n\nWith he same formula for the average salary comparison in February, the result is 'same' since both the department '1' and '2' have the same average salary with the company, which is 7000.\n</code></pre> <p>Ans  </p> <pre><code>select distinct\n    pay_month,\n    department_id,\n    case \n        when avg_dept&lt;avg_company then 'lower'\n        when avg_dept=avg_company then 'same'\n        when avg_dept&gt;avg_company then 'higher'\n    end as comparison\nfrom    \n    (\n            select \n                department_id,\n                date_format(pay_date, '%Y-%m') as pay_month,\n                 avg(employee_salary) over (partition by pay_date  ) as avg_company,\n                 avg(employee_salary) over (partition by pay_date, department_id  ) as avg_dept\n            from    \n                (select\n                    s.employee_id,\n                    s.amount as employee_salary,\n                    s.pay_date,\n                    e.department_id   \n                from salary s \n                inner join employee e\n                on s.employee_id = e.employee_id) a\n    ) b     \n</code></pre>"},{"location":"SQL/docs/SQL%20leetcode/#sales-by-day-of-the-week","title":"Sales by Day of the Week","text":"<p>https://leetcode.com/problems/sales-by-day-of-the-week/ </p> <pre><code>Table: Orders\n\n+---------------+---------+\n| Column Name   | Type    |\n+---------------+---------+\n| order_id      | int     |\n| customer_id   | int     |\n| order_date    | date    | \n| item_id       | varchar |\n| quantity      | int     |\n+---------------+---------+\n(ordered_id, item_id) is the primary key for this table.\nThis table contains information on the orders placed.\norder_date is the date item_id was ordered by the customer with id customer_id.\n\n\nTable: Items\n\n+---------------------+---------+\n| Column Name         | Type    |\n+---------------------+---------+\n| item_id             | varchar |\n| item_name           | varchar |\n| item_category       | varchar |\n+---------------------+---------+\nitem_id is the primary key for this table.\nitem_name is the name of the item.\nitem_category is the category of the item.\n</code></pre> <p>You are the business owner and would like to obtain a sales report for category items and the day of the week.  Write an SQL query to report how many units in each category have been ordered on each day of the week.  Return the result table ordered by category.  The query result format is in the following example. </p> <p>Example 1: </p> <pre><code>Input: \nOrders table:\n+------------+--------------+-------------+--------------+-------------+\n| order_id   | customer_id  | order_date  | item_id      | quantity    |\n+------------+--------------+-------------+--------------+-------------+\n| 1          | 1            | 2020-06-01  | 1            | 10          |\n| 2          | 1            | 2020-06-08  | 2            | 10          |\n| 3          | 2            | 2020-06-02  | 1            | 5           |\n| 4          | 3            | 2020-06-03  | 3            | 5           |\n| 5          | 4            | 2020-06-04  | 4            | 1           |\n| 6          | 4            | 2020-06-05  | 5            | 5           |\n| 7          | 5            | 2020-06-05  | 1            | 10          |\n| 8          | 5            | 2020-06-14  | 4            | 5           |\n| 9          | 5            | 2020-06-21  | 3            | 5           |\n+------------+--------------+-------------+--------------+-------------+\nItems table:\n+------------+----------------+---------------+\n| item_id    | item_name      | item_category |\n+------------+----------------+---------------+\n| 1          | LC Alg. Book   | Book          |\n| 2          | LC DB. Book    | Book          |\n| 3          | LC SmarthPhone | Phone         |\n| 4          | LC Phone 2020  | Phone         |\n| 5          | LC SmartGlass  | Glasses       |\n| 6          | LC T-Shirt XL  | T-Shirt       |\n+------------+----------------+---------------+\nOutput: \n+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+\n| Category   | Monday    | Tuesday   | Wednesday | Thursday  | Friday    | Saturday  | Sunday    |\n+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+\n| Book       | 20        | 5         | 0         | 0         | 10        | 0         | 0         |\n| Glasses    | 0         | 0         | 0         | 0         | 5         | 0         | 0         |\n| Phone      | 0         | 0         | 5         | 1         | 0         | 0         | 10        |\n| T-Shirt    | 0         | 0         | 0         | 0         | 0         | 0         | 0         |\n+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+\nExplanation: \nOn Monday (2020-06-01, 2020-06-08) were sold a total of 20 units (10 + 10) in the category Book (ids: 1, 2).\nOn Tuesday (2020-06-02) were sold a total of 5 units in the category Book (ids: 1, 2).\nOn Wednesday (2020-06-03) were sold a total of 5 units in the category Phone (ids: 3, 4).\nOn Thursday (2020-06-04) were sold a total of 1 unit in the category Phone (ids: 3, 4).\nOn Friday (2020-06-05) were sold 10 units in the category Book (ids: 1, 2) and 5 units in Glasses (ids: 5).\nOn Saturday there are no items sold.\nOn Sunday (2020-06-14, 2020-06-21) were sold a total of 10 units (5 +5) in the category Phone (ids: 3, 4).\nThere are no sales of T-shirts.\n</code></pre> <p>Ans  </p> <pre><code>with cte_table as \n(select\n    b.item_category,\n    dayname(a.order_date) as day_of_week,\n    COALESCE(sum(a.quantity),0) as qty\nfrom \n    orders a \n    RIGHT join\n    items b\n    on a.item_id = b.item_id\ngroup by b.item_category, dayname(a.order_date)\n)\nselect\n    item_category as \"CATEGORY\",\n    sum(case when upper(day_of_week)='MONDAY' then qty else 0 end) as 'MONDAY',\n    sum(case when  upper(day_of_week)='TUESDAY' then qty else 0 end) as 'TUESDAY',\n    sum(case when  upper(day_of_week)='WEDNESDAY' then qty else 0 end) as 'WEDNESDAY',\n    sum(case when  upper(day_of_week)='THURSDAY' then qty else 0 end) as 'THURSDAY',\n    sum(case when  upper(day_of_week)='FRIDAY' then qty else 0 end) as 'FRIDAY',\n    sum(case when  upper(day_of_week)='SATURDAY' then qty else 0 end) as 'SATURDAY',\n    sum(case when  upper(day_of_week)='SUNDAY' then qty else 0 end) as 'SUNDAY'\n\nfrom cte_table\ngroup by item_category\norder by item_category;\n\n</code></pre>"},{"location":"SQL/docs/SQL%20leetcode/#report-contiguous-dates","title":"Report Contiguous Dates","text":"<p>https://leetcode.com/problems/report-contiguous-dates/ </p> <pre><code>Table: Failed\n\n+--------------+---------+\n| Column Name  | Type    |\n+--------------+---------+\n| fail_date    | date    |\n+--------------+---------+\nfail_date is the primary key for this table.\nThis table contains the days of failed tasks.\n\n\nTable: Succeeded\n\n+--------------+---------+\n| Column Name  | Type    |\n+--------------+---------+\n| success_date | date    |\n+--------------+---------+\nsuccess_date is the primary key for this table.\nThis table contains the days of succeeded tasks.\n</code></pre> <p>A system is running one task every day. Every task is independent of the previous tasks. The tasks can fail or succeed.  Write an SQL query to generate a report of period_state for each continuous interval of days in the period from 2019-01-01 to 2019-12-31.  period_state is 'failed' if tasks in this interval failed or 'succeeded' if tasks in this interval succeeded. Interval of days are retrieved as start_date and end_date.  Return the result table ordered by start_date.  The query result format is in the following example. </p> <p>Example 1: </p> <pre><code>Input: \nFailed table:\n+-------------------+\n| fail_date         |\n+-------------------+\n| 2018-12-28        |\n| 2018-12-29        |\n| 2019-01-04        |\n| 2019-01-05        |\n+-------------------+\nSucceeded table:\n+-------------------+\n| success_date      |\n+-------------------+\n| 2018-12-30        |\n| 2018-12-31        |\n| 2019-01-01        |\n| 2019-01-02        |\n| 2019-01-03        |\n| 2019-01-06        |\n+-------------------+\nOutput: \n+--------------+--------------+--------------+\n| period_state | start_date   | end_date     |\n+--------------+--------------+--------------+\n| succeeded    | 2019-01-01   | 2019-01-03   |\n| failed       | 2019-01-04   | 2019-01-05   |\n| succeeded    | 2019-01-06   | 2019-01-06   |\n+--------------+--------------+--------------+\nExplanation: \nThe report ignored the system state in 2018 as we care about the system in the period 2019-01-01 to 2019-12-31.\nFrom 2019-01-01 to 2019-01-03 all tasks succeeded and the system state was \"succeeded\".\nFrom 2019-01-04 to 2019-01-05 all tasks failed and the system state was \"failed\".\nFrom 2019-01-06 to 2019-01-06 all tasks succeeded and the system state was \"succeeded\".\n</code></pre> <p>Ans  </p> <pre><code>with \ncte1 as \n        (\n            select\n                'failed' as status,\n                date_format(fail_date,'%Y-%m-%d') as mydate\n            from \n            Failed\n            where fail_date between '2019-01-01' and '2019-12-31'\n            union \n            select\n                'succeeded' as status,\n                date_format(success_date,'%Y-%m-%d') as mydate\n            from \n            Succeeded\n            where success_date between '2019-01-01' and '2019-12-31'\n        ), \ncte2 as \n    (select \n            mydate,\n            status,\n            row_number() over (order by mydate)  -   dense_rank() over (partition by status order by mydate) as grp\n    from cte1\n    )\nselect\n    status as period_state, \n    min(mydate) as start_date,\n    max(mydate) as end_date\nfrom \ncte2\ngroup by grp , status \norder by  min(mydate); \n</code></pre>"},{"location":"SQL/docs/SQL%20leetcode/#user-purchase-platform","title":"User Purchase Platform","text":"<pre><code>Table: Spending\n\n+-------------+---------+\n| Column Name | Type    |\n+-------------+---------+\n| user_id     | int     |\n| spend_date  | date    |\n| platform    | enum    | \n| amount      | int     |\n+-------------+---------+\nThe table logs the history of the spending of users that make purchases from an online shopping website that has a desktop and a mobile application.\n(user_id, spend_date, platform) is the primary key of this table.\nThe platform column is an ENUM type of ('desktop', 'mobile').\n</code></pre> <p>Write an SQL query to find the total number of users and the total amount spent using the mobile only, the desktop only, and both mobile and desktop together for each date. Return the result table in any order. The query result format is in the following example.</p> <pre><code>Example 1:\n\nInput: \nSpending table:\n+---------+------------+----------+--------+\n| user_id | spend_date | platform | amount |\n+---------+------------+----------+--------+\n| 1       | 2019-07-01 | mobile   | 100    |\n| 1       | 2019-07-01 | desktop  | 100    |\n| 2       | 2019-07-01 | mobile   | 100    |\n| 2       | 2019-07-02 | mobile   | 100    |\n| 3       | 2019-07-01 | desktop  | 100    |\n| 3       | 2019-07-02 | desktop  | 100    |\n+---------+------------+----------+--------+\nOutput: \n+------------+----------+--------------+-------------+\n| spend_date | platform | total_amount | total_users |\n+------------+----------+--------------+-------------+\n| 2019-07-01 | desktop  | 100          | 1           |\n| 2019-07-01 | mobile   | 100          | 1           |\n| 2019-07-01 | both     | 200          | 1           |\n| 2019-07-02 | desktop  | 100          | 1           |\n| 2019-07-02 | mobile   | 100          | 1           |\n| 2019-07-02 | both     | 0            | 0           |\n+------------+----------+--------------+-------------+ \nExplanation: \nOn 2019-07-01, user 1 purchased using both desktop and mobile, user 2 purchased using mobile only and user 3 purchased using desktop only.\nOn 2019-07-02, user 2 purchased using mobile only, user 3 purchased using desktop only and no one purchased using both platforms.\n</code></pre> <p>Ans </p> <pre><code>with cte as (\nselect\n    spend_date,\n    user_id,\n    sum(case when platform='mobile' then amount else 0 end) as mobile_amount,\n    sum(case when platform='desktop' then amount else 0 end) as desktop_amount    \nfrom spending\n    group by spend_date,user_id\n\n),\ncte2 as (\nselect\n    spend_date,\n     user_id,\n    case \n        when  mobile_amount &gt; 0 and desktop_amount&gt;0 then 'both'\n        when  mobile_amount &gt; 0 and desktop_amount=0 then 'mobile'\n        when  mobile_amount = 0 and desktop_amount&gt;0 then 'desktop'\n        end as platform,\n    (mobile_amount+desktop_amount) as total_amount\nfrom cte\ngroup by spend_date , user_id\n),\ncte3 as \n(\n    SELECT distinct(spend_date), 'desktop' platform \n    FROM Spending\n    UNION\n    SELECT distinct(spend_date), 'mobile' platform \n    FROM Spending\n    UNION\n    SELECT distinct(spend_date), 'both' platform \n    FROM Spending\n)\nselect \n    cte3.spend_date,\n    cte3.platform,\n    coalesce(sum(cte2.total_amount),0) as total_amount,\n    count(cte2.user_id) as total_users\nfrom cte3\nleft join cte2\non \n    cte3.spend_date=cte2.spend_date\n    and \n    cte3.platform=cte2.platform\ngroup by  spend_date,\nplatform;\n</code></pre>"},{"location":"SQL/docs/sql-analytical-functions/","title":"SQL Analytical Functions","text":"<p>Source</p>"},{"location":"SQL/docs/sql-analytical-functions/#over-partition-by-order-by","title":"over (partition by / order by )","text":"<p>eg -</p> <pre><code>SELECT o.region_id region_id, o.cust_nbr cust_nbr,\n\n SUM(o.tot_sales) tot_sales,\n\n SUM(SUM(o.tot_sales)) OVER (PARTITION BY o.region_id) region_sales\n\nFROM orders o\n\nWHERE o.year = 2001\n\nGROUP BY o.region_id, o.cust_nbr;\n</code></pre>"},{"location":"SQL/docs/sql-analytical-functions/#ranking-functions","title":"Ranking Functions","text":"<p>source</p>"},{"location":"SQL/docs/sql-analytical-functions/#row_number-rank-and-dense_rank","title":"ROW_NUMBER, RANK and DENSE_RANK","text":"<ul> <li>ROW_NUMBER - Returns a unique number for each row starting with 1. For rows that have duplicate values, numbers are arbitrarily assigned.</li> <li>DENSE_RANK - Assigns a unique number for each row starting with 1, except for rows that have duplicate values, in which case the same ranking is assigned.</li> <li>RANK - Assigns a unique number for each row starting with 1, except for rows that have duplicate values, in which case the same ranking is assigned and a gap appears in the sequence for each duplicate ranking. eg 1 -</li> </ul> <pre><code>\nSELECT region_id, cust_nbr, \n\n  SUM(tot_sales) cust_sales,\n\n  RANK( ) OVER (ORDER BY SUM(tot_sales) DESC) sales_rank,\n\n  DENSE_RANK( ) OVER (ORDER BY SUM(tot_sales) DESC) sales_dense_rank,\n\n  ROW_NUMBER( ) OVER (ORDER BY SUM(tot_sales) DESC) sales_number\n\nFROM orders\n\nWHERE year = 2001\n\nGROUP BY region_id, cust_nbr\n\nORDER BY sales_number;\n\n\nREGION_ID   CUST_NBR  CUST_SALES SALES_RANK SALES_DENSE_RANK SALES_NUMBER\n\n---------- ---------- ---------- ---------- ---------------- ------------\n\n         8         18    1253840         11               11           11\n\n         5          2    1224992         12               12           12\n\n         9         23    1224992         12               12           13\n\n         9         24    1224992         12               12           14\n\n        10         30    1216858         15               13           15\n\n</code></pre> <p>eg 2 - The following query generates rankings for customer sales within each region rather than across all regions.  Note the addition of the PARTITION BY clause:</p> <pre><code>SELECT \n  region_id, cust_nbr, \n\n  SUM(tot_sales) cust_sales,\n\n  RANK( ) OVER (PARTITION BY region_id  ORDER BY SUM(tot_sales) DESC) sales_rank,\n\n  DENSE_RANK( ) OVER (PARTITION BY region_id ORDER BY SUM(tot_sales) DESC) sales_dense_rank,\n\n  ROW_NUMBER( ) OVER (PARTITION BY region_id ORDER BY SUM(tot_sales) DESC) sales_number\n\nFROM orders\n\nWHERE year = 2001\n\nGROUP BY region_id, cust_nbr\n\nORDER BY region_id, sales_number;\n\nREGION_ID    CUST_NBR CUST_SALES SALES_RANK SALES_DENSE_RANK SALES_NUMBER\n\n---------- ---------- ---------- ---------- ---------------- ------------\n\n         5          4    1878275          1                1            1\n\n         5          2    1224992          2                2            2\n\n         5          5    1169926          3                3            3\n\n         5          3    1161286          4                4            4\n\n         5          1    1151162          5                5            5\n\n         6          6    1788836          1                1            1\n\n         6          9    1208959          2                2            2\n\n</code></pre>"},{"location":"SQL/docs/sql-analytical-functions/#handling-nulls","title":"Handling NULLs","text":"<p>All ranking functions allow you to specify where in the ranking order NULL values should appear.  This is accomplished by appending either NULLS FIRST or NULLS LAST after the ORDER BY clause of the function, as in:</p> <pre><code>SELECT region_id, cust_nbr, SUM(tot_sales) cust_sales,\n\n  RANK( ) OVER (ORDER BY SUM(tot_sales) DESC NULLS LAST) sales_rank\n\nFROM orders\n\nWHERE year = 2001\n\nGROUP BY region_id, cust_nbr;\n</code></pre>"},{"location":"SQL/docs/sql-analytical-functions/#ntile","title":"NTILE","text":"<p>Another way rankings are commonly used is to generate buckets into which sets of rankings are grouped.  For example, you may want to find those customers whose total sales ranked in the top 25%.  The following query uses the NTILE function to group the customers into four buckets (or quartiles):</p> <pre><code>SELECT region_id, cust_nbr, SUM(tot_sales) cust_sales,\n\n  NTILE(4) OVER (ORDER BY SUM(tot_sales) DESC) sales_quartile\n\nFROM orders\n\nWHERE year = 2001\n\nGROUP BY region_id, cust_nbr\n\nORDER BY sales_quartile, cust_sales DESC;\n\nREGION_ID    CUST_NBR CUST_SALES SALES_QUARTILE\n\n---------- ---------- ---------- --------------\n\n         9         25    2232703              1\n\n         8         17    1944281              1\n\n         7         14    1929774              1\n</code></pre>"},{"location":"SQL/docs/sql-analytical-functions/#cume_dist-and-percent_rank","title":"CUME_DIST and PERCENT_RANK","text":"<p>CUME_DIST function (short for Cumulative Distribution) calculates the ratio of the number of rows that have a lesser or equal ranking to the total number of rows in the partition.  </p> <p>PERCENT_RANK function calculates the ratio of a row's ranking to the number of rows in the partition using the formula:</p> <pre><code>(RRP -- 1) / (NRP -- 1)\n</code></pre> <p>where RRP is the \"rank of row in partition,\" and NRP is the \"number of rows in partition.\"</p>"},{"location":"SQL/docs/sql-analytical-functions/#windowing-functions","title":"Windowing Functions","text":""},{"location":"SQL/docs/sql-analytical-functions/#rows-between-and","title":"ROWS BETWEEN &lt;&gt; AND &lt;&gt;","text":"<p>Some of the sample values can be -</p>"},{"location":"SQL/docs/sql-analytical-functions/#rows-between-unbounded-preceding-and-unbounded-following","title":"ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING","text":"<p>eg -</p> <pre><code>SELECT month, \n  SUM(tot_sales) monthly_sales,\n  SUM(SUM(tot_sales)) OVER ( ORDER BY month \n                             ROWS BETWEEN UNBOUNDED PRECEDING AND \n                                          UNBOUNDED FOLLOWING     ) total_sales\nFROM orders\nWHERE year = 2001 \n  AND region_id = 6\nGROUP BY month\nORDER BY month;\n\n     MONTH MONTHLY_SALES TOTAL_SALES\n         1        610697     6307766\n         2        428676     6307766\n         3        637031     6307766\n         4        541146     6307766\n</code></pre>"},{"location":"SQL/docs/sql-analytical-functions/#rows-between-unbounded-preceding-and-current-row","title":"ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW","text":"<p>eg 1 - max current value-</p> <pre><code>SELECT month, \n  SUM(tot_sales) monthly_sales,\n  MAX(SUM(tot_sales)) OVER (ORDER BY month \n                            ROWS BETWEEN UNBOUNDED PRECEDING \n                            AND CURRENT ROW) max_preceeding\nFROM orders\nWHERE year = 2001 \n  AND region_id = 6\nGROUP BY month\nORDER BY month;\n\n     MONTH MONTHLY_SALES MAX_PRECEEDING\n         1        610697         610697\n         2        428676         610697\n         3        637031         637031\n         4        541146         637031\n         5        592935         637031\n         6        501485         637031\n</code></pre> <p>eg 2 - cumulative SUM -</p> <pre><code>SELECT month, \n  SUM(tot_sales) monthly_sales,\n  SUM(SUM(tot_sales)) OVER (ORDER BY month ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) max_preceeding\nFROM orders\nWHERE year = 2001 \n  AND region_id = 6\nGROUP BY month\nORDER BY month;\n\n     MONTH MONTHLY_SALES RUNNING_TOTAL\n         1        610697         610697\n         2        428676        1039373\n         3        637031        1676404\n         4        541146        2217550\n         5        592935        2810485\n</code></pre>"},{"location":"SQL/docs/sql-analytical-functions/#rows-between-1-preceding-and-1-following-","title":"ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING -","text":"<p>eg -calculate avg of rolling 3 values (current row, rev row and next row);</p> <pre><code>SELECT month, \n  SUM(tot_sales) monthly_sales,\n  AVG(SUM(tot_sales)) OVER (ORDER BY month ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING) rolling_avg\nFROM orders\nWHERE year = 2001 \n  AND region_id = 6\nGROUP BY month\nORDER BY month;\n\n     MONTH MONTHLY_SALES ROLLING_AVG\n         1        610697    519686.5\n         2        428676  558801.333\n         3        637031  535617.667\n         4        541146  590370.667\n         5        592935  545188.667\n</code></pre>"},{"location":"SQL/docs/sql-analytical-functions/#range-between","title":"RANGE BETWEEN","text":"<ul> <li>ROWS BETWEEN - based on row ordered as per condition of ordering inside over() clause.</li> <li>RANGE BETWEEN - based on value ranges specified under over() clause.</li> </ul> <p>eg1 - to generate a three-month rolling average (similar to above ROWS BETWEEN question). In our table, month is numeric integer value, and so RANGE works perfectly fine here. This substitution works because the month column contains integer values, so adding and subtracting 1 from the current month yields a three-month range.</p> <p>But if its character, then below wont be suited.</p> <pre><code>SELECT \n  month,\n  SUM(tot_sales) monthly_sales,\n  AVG(SUM(tot_sales)) OVER (ORDER BY month \n                            RANGE BETWEEN \n                                1 PRECEDING \n                                AND \n                                1 FOLLOWING) rolling_avg\nFROM orders\nWHERE year = 2001  AND region_id = 6\nGROUP BY month\nORDER BY month;\n\n---------- ------------- -----------\nMONTH      MONTHLY_SALES ROLLING_AVG\n---------- ------------- -----------\n         1        610697    519686.5\n         2        428676  558801.333\n         3        637031  535617.667\n</code></pre> <p>eg2 - if we do a range of +/- 1.999, then also we get same values:</p> <pre><code>SELECT month,\n  SUM(tot_sales) monthly_sales,\n  AVG(SUM(tot_sales)) OVER (ORDER BY month \n                            RANGE BETWEEN \n                                1.99 PRECEDING \n                                AND \n                                1.99 FOLLOWING) rolling_avg\nFROM orders\nWHERE year = 2001  AND region_id = 6\nGROUP BY month\nORDER BY month;\n\n---------- ------------- -----------\n    MONTH  MONTHLY_SALES ROLLING_AVG\n---------- ------------- -----------\n         1        610697    519686.5\n         2        428676  558801.333\n         3        637031  535617.667\n</code></pre> <p>eg3 - working with DATE Range.  ROWS wont be of much use if we are working on Date Range. If you are generating a window based on a DATE column, you can specify a range in increments of days, months, or years.  Since the orders table has no DATE columns, the next example shows how a date range can be specified against the order_dt column of the cust_order table:</p> <pre><code>SELECT \n  TRUNC(order_dt) day,\n  SUM(sale_price) daily_sales,\n  AVG(SUM(sale_price)) OVER ( \n                              ORDER BY TRUNC(order_dt) \n                              RANGE BETWEEN \n                                INTERVAL '2' DAY PRECEDING \n                                AND \n                                INTERVAL '2' DAY FOLLOWING     \n                            ) five_day_avg\nFROM cust_order\nWHERE sale_price IS NOT NULL \n  AND order_dt BETWEEN TO_DATE('01-JUL-2001','DD-MON-YYYY')\n  AND TO_DATE('31-JUL-2001','DD-MON-YYYY')\nGROUP BY TRUNC(order_dt);\n\n\n--------- ----------- ------------\nDAY       DAILY_SALES FIVE_DAY_AVG\n--------- ----------- ------------\n16-JUL-01         112          146\n18-JUL-01         180          114\n20-JUL-01          50          169\n21-JUL-01          50   165.333333\n22-JUL-01         396   165.333333\n</code></pre>"},{"location":"SQL/docs/sql-analytical-functions/#first_value-and-last_value","title":"FIRST_VALUE and LAST_VALUE","text":"<p>They are used with windowing functions to identify the values of the first and last values in the window. sample que :\"How did each month's sales compare to the first month?\"</p> <p>eg - In the case of the three-month rolling average query shown previously, you could display the values of all three months along with the average of the three, as in:</p> <pre><code>SELECT month,\n  FIRST_VALUE(SUM(tot_sales)) OVER (ORDER BY month ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING) prev_month,\n  SUM(tot_sales) monthly_sales,\n  LAST_VALUE(SUM(tot_sales)) OVER (ORDER BY month ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING) next_month,\n  AVG(SUM(tot_sales)) OVER (ORDER BY month ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING) rolling_avg\nFROM orders\nWHERE year = 2001 \n  AND region_id = 6\nGROUP BY month\nORDER BY month;\n\n     MONTH PREV_MONTH MONTHLY_SALES NEXT_MONTH ROLLING_AVG\n         1     610697        610697     428676    519686.5\n         2     610697        428676     637031  558801.333\n         3     428676        637031     541146  535617.667\n         4     637031        541146     592935  590370.667\n         5     541146        592935     501485  545188.667\n</code></pre>"},{"location":"SQL/docs/sql-analytical-functions/#laglead-functions","title":"LAG/LEAD Functions","text":"<pre><code>\"LAG(VAL, N, &lt;default value&gt;) OVER () \" -- N=1 by default.\n\"LEAD(VAL, N, &lt;default value&gt;) OVER () \" -- N=1 by default.\n</code></pre> <p>Query - \"Compute the total sales per month for the Mid-Atlantic region, including the percent change from the previous month\" requires data from both the current and preceding rows to calculate the answer.</p> <p>Step 1 - get prev month's data using LAG function.</p> <pre><code>SELECT month, \n  SUM(tot_sales) monthly_sales,\n  LAG(SUM(tot_sales), 1) OVER (ORDER BY month) prev_month_sales\nFROM orders\nWHERE year = 2001\n  AND region_id = 6\nGROUP BY month\nORDER BY month;\n\n---------- ------------- ----------------\n     MONTH MONTHLY_SALES PREV_MONTH_SALES\n---------- ------------- ----------------\n         1        610697\n         2        428676           610697\n         3        637031           428676\n</code></pre> <p>Step 2 - handle NULL values.  If you see above, for month 1, PREV_MONTH_SALES is NULL. So, can calculate % change in sales. Here we keep current month sales as default value, and % sales in this case is 0%.</p> <pre><code>SELECT \n    months.month month, \n    months.monthly_sales monthly_sales,\n  ROUND((months.monthly_sales - months.prev_month_sales)/ months.prev_month_sales, 3) * 100 percent_change\nFROM\n (\n    SELECT month, \n           SUM(tot_sales) monthly_sales,\n           LAG(SUM(tot_sales), 1, SUM(tot_sales)) OVER (ORDER BY month) prev_month_sales\n\n    FROM orders\n    WHERE year = 2001\n    AND region_id = 6\n    GROUP BY month) months\nORDER BY month;\n\n---------- ------------- --------------\n     MONTH MONTHLY_SALES PERCENT_CHANGE\n---------- ------------- --------------\n         1        610697              0\n         2        428676          -29.8\n         3        637031           48.6\n</code></pre>"},{"location":"SQL/docs/sql-practise-questions/","title":"SQL Practise Questions","text":"<p>Leetcode SQL</p> <ol> <li>The Most Recent Three Orders Table: Customers</li> </ol> <pre><code>+---------------+---------+\n| Column Name   | Type    |\n+---------------+---------+\n| customer_id   | int     |\n| name          | varchar |\n+---------------+---------+\ncustomer_id is the primary key for this table.\nThis table contains information about customers.\n\n\nTable: Orders\n+---------------+---------+\n| Column Name   | Type    |\n+---------------+---------+\n| order_id      | int     |\n| order_date    | date    |\n| customer_id   | int     |\n| cost          | int     |\n+---------------+---------+\norder_id is the primary key for this table.\nThis table contains information about the orders made customer_id.\nEach customer has one order per day.\n</code></pre> <p>Write an SQL query to find the most recent 3 orders of each user.  If a user ordered less than 3 orders return all of their orders. Return the result table sorted by customer_name in ascending order and in case of a tie by the customer_id in ascending order.  If there still a tie, order them by the order_date in descending order. The query result format is in the following example:</p> <pre><code>Customers\n+-------------+-----------+\n| customer_id | name      |\n+-------------+-----------+\n| 1           | Winston   |\n| 2           | Jonathan  |\n| 3           | Annabelle |\n| 4           | Marwan    |\n| 5           | Khaled    |\n+-------------+-----------+\n\nOrders\n+----------+------------+-------------+------+\n| order_id | order_date | customer_id | cost |\n+----------+------------+-------------+------+\n| 1        | 2020-07-31 | 1           | 30   |\n| 2        | 2020-07-30 | 2           | 40   |\n| 3        | 2020-07-31 | 3           | 70   |\n| 4        | 2020-07-29 | 4           | 100  |\n| 5        | 2020-06-10 | 1           | 1010 |\n| 6        | 2020-08-01 | 2           | 102  |\n| 7        | 2020-08-01 | 3           | 111  |\n| 8        | 2020-08-03 | 1           | 99   |\n| 9        | 2020-08-07 | 2           | 32   |\n| 10       | 2020-07-15 | 1           | 2    |\n+----------+------------+-------------+------+\n\nResult table:\n+---------------+-------------+----------+------------+\n| customer_name | customer_id | order_id | order_date |\n+---------------+-------------+----------+------------+\n| Annabelle     | 3           | 7        | 2020-08-01 |\n| Annabelle     | 3           | 3        | 2020-07-31 |\n| Jonathan      | 2           | 9        | 2020-08-07 |\n| Jonathan      | 2           | 6        | 2020-08-01 |\n| Jonathan      | 2           | 2        | 2020-07-30 |\n| Marwan        | 4           | 4        | 2020-07-29 |\n| Winston       | 1           | 8        | 2020-08-03 |\n| Winston       | 1           | 1        | 2020-07-31 |\n| Winston       | 1           | 10       | 2020-07-15 |\n+---------------+-------------+----------+------------+\nWinston has 4 orders, we discard the order of \"2020-06-10\" because it is the oldest order.\nAnnabelle has only 2 orders, we return them.\nJonathan has exactly 3 orders.\nMarwan ordered only one time.\nWe sort the result table by customer_name in ascending order, by customer_id in ascending order and by order_date in descending order in case of a tie.\n</code></pre> <p>Ans.</p> <pre><code>select \ncustomer_name, \ncustomer_id, \norder_id, \norder_date  \n\nfrom \n(select\n    a.name as customer_name,\n    a.customer_id,\n    b.order_id,\n    to_char(b.order_date, 'YYYY-MM-DD') as order_date,\n    rank() over ( partition by a.customer_id order by b.order_date desc)   date_rank\n    from \n    customers a inner join orders b \n    on a.customer_id = b.customer_id \n    order by a.customer_id, b.order_date  \n)\nwhere date_rank&lt;=3\norder by customer_name, customer_id, order_date desc\n;\n</code></pre> <p>2A. Shortest Distance in a Plane https://leetcode.com/problems/shortest-distance-in-a-plane/  Table point_2d holds the coordinates (x,y) of some unique points (more than two) in a plane. Write a query to find the shortest distance between these points rounded to 2 decimals.</p> x y -1 -1 0 0 -1 -2 <p>The shortest distance is 1.00 from point (-1,-1) to (-1,2). So the output should be:</p> shortest 1.00 <p>Note: The longest distance among all the points are less than 10000.</p> <p>Ans.</p> <pre><code>select min(dist) as shortest from \n    (\n    select\n         a.x as x1,\n         a.y as y1,\n         b.x as x2,\n         b.y as y2,\n        round(sqrt((a.x - b.x)*(a.x - b.x) + (a.y - b.y)*(a.y - b.y) ),2) as dist\n    from point_2d a, point_2d b \n    where concat(a.x,a.y)!=concat(b.x,b.y) -- this bit of matching coordinates is IMP to ensure same coordinates are not being captured.\n    );\n</code></pre> <ol> <li>Investments in 2016  https://leetcode.com/problems/investments-in-2016/ </li> </ol> <p>Write a query to print the sum of all total investment values in 2016 (TIV_2016), to a scale of 2 decimal places, for all policy-holders who meet the following criteria: - Have the same TIV_2015 value as one or more other policyholders. - Are not located in the same city as any other policyholder (i.e.: the (latitude, longitude) attribute pairs must be unique).</p> <p>Input Format:</p> <p>The insurance table is described as follows:</p> Column Name Type PID INTEGER(11) TIV_2015 NUMERIC(15,2) TIV_2016 NUMERIC(15,2) LAT NUMERIC(5,2) LON NUMERIC(5,2) where PID is the policyholder's policy ID, TIV_2015 is the total investment value in 2015, TIV_2016 is the total investment value in 2016, LAT is the latitude of the policy holder's city, and LON is the longitude of the policy holder's city. <p>Sample Input</p> PID TIV_2015 TIV_2016 LAT LON 1 10 5 10 10 2 20 20 20 20 3 10 30 20 20 4 10 40 40 40 <p>Sample Output</p> <p>| TIV_2016 | |----------| | 45.00    | Explanation</p> <p>The first record in the table, like the last record, meets both of the two criteria. The TIV_2015 value '10' is as the same as the third and forth record, and its location unique.</p> <p>The second record does not meet any of the two criteria. Its TIV_2015 is not like any other policyholders.</p> <p>And its location is the same with the third record, which makes the third record fail, too.</p> <p>So, the result is the sum of TIV_2016 of the first and last record, which is 45.</p> <p>Ans.</p> <pre><code>select round(sum(tiv_2016),2) as \"TIV_2016\" from (\nselect\n    pid,\n    count(pid) over (partition by tiv_2015) as tiv_2015_count,\n    tiv_2016,\n    count(*) over (partition by concat(concat(lat,'_'),lon)) as locn_count\nfrom\n    insurance order by lat,lon) where locn_count=1 and tiv_2015_count&gt;1;\n</code></pre> <ol> <li>Calculate Salaries  https://leetcode.com/problems/calculate-salaries/ </li> </ol> <p>Table Salaries:</p> Column Name Type company_id int employee_id int employee_name varchar salary int <p>(company_id, employee_id) is the primary key for this table. This table contains the company id, the id, the name and the salary for an employee.</p> <p>Write an SQL query to find the salaries of the employees after applying taxes.</p> <p>The tax rate is calculated for each company based on the following criteria:</p> <p>0% If the max salary of any employee in the company is less than 1000$. 24% If the max salary of any employee in the company is in the range [1000, 10000] inclusive. 49% If the max salary of any employee in the company is greater than 10000$. Return the result table in any order. Round the salary to the nearest integer.</p> <p>The query result format is in the following example:</p> <pre><code>Salaries table:\n+------------+-------------+---------------+--------+\n| company_id | employee_id | employee_name | salary |\n+------------+-------------+---------------+--------+\n| 1          | 1           | Tony          | 2000   |\n| 1          | 2           | Pronub        | 21300  |\n| 1          | 3           | Tyrrox        | 10800  |\n| 2          | 1           | Pam           | 300    |\n| 2          | 7           | Bassem        | 450    |\n| 2          | 9           | Hermione      | 700    |\n| 3          | 7           | Bocaben       | 100    |\n| 3          | 2           | Ognjen        | 2200   |\n| 3          | 13          | Nyancat       | 3300   |\n| 3          | 15          | Morninngcat   | 1866   |\n+------------+-------------+---------------+--------+\n\nResult table:\n+------------+-------------+---------------+--------+\n| company_id | employee_id | employee_name | salary |\n+------------+-------------+---------------+--------+\n| 1          | 1           | Tony          | 1020   |\n| 1          | 2           | Pronub        | 10863  |\n| 1          | 3           | Tyrrox        | 5508   |\n| 2          | 1           | Pam           | 300    |\n| 2          | 7           | Bassem        | 450    |\n| 2          | 9           | Hermione      | 700    |\n| 3          | 7           | Bocaben       | 76     |\n| 3          | 2           | Ognjen        | 1672   |\n| 3          | 13          | Nyancat       | 2508   |\n| 3          | 15          | Morninngcat   | 5911   |\n+------------+-------------+---------------+--------+\nFor company 1, Max salary is 21300. Employees in company 1 have taxes = 49%\nFor company 2, Max salary is 700. Employees in company 2 have taxes = 0%\nFor company 3, Max salary is 7777. Employees in company 3 have taxes = 24%\nThe salary after taxes = salary - (taxes percentage / 100) * salary\nFor example, Salary for Morninngcat (3, 15) after taxes = 7777 - 7777 * (24 / 100) = 7777 - 1866.48 = 5910.52, which is rounded to 5911.\n</code></pre> <p>Ans.</p> <pre><code>select \ncompany_id,\nemployee_id,\nemployee_name,\nround(\n    salary*(case \n        when max_sal_per_company&lt;1000 then 1 \n        when max_sal_per_company&gt;=1000 and max_sal_per_company&lt;=10000 then 0.76\n        else 0.51 \n        end\n        ),0) as salary\nfrom\n(\nselect \n company_id , employee_id , employee_name , salary, \n max(salary) over (partition by company_id ) as max_sal_per_company\n from salaries);\n\n</code></pre> <ol> <li>Countries You Can Safely Invest In https://leetcode.com/problems/countries-you-can-safely-invest-in/ </li> </ol> <p>Table Person:</p> Column Name Type id int name varchar phone_number varchar <p>id is the primary key for this table. Each row of this table contains the name of a person and their phone number. Phone number will be in the form 'xxx-yyyyyyy' where xxx is the country code (3 characters) and yyyyyyy is the phone number (7 characters) where x and y are digits.  Both can contain leading zeros.</p> <p>Table Country:</p> Column Name Type name varchar country_code varchar <p>country_code is the primary key for this table. Each row of this table contains the country name and its code. country_code will be in the form 'xxx' where x is digits.</p> <p>Table Calls:</p> Column Name Type caller_id int callee_id int duration int <p>There is no primary key for this table, it may contain duplicates. Each row of this table contains the caller id, callee id and the duration of the call in minutes. caller_id != callee_id A telecommunications company wants to invest in new countries. The company intends to invest in the countries where the average call duration of the calls in this country is strictly greater than the global average call duration.</p> <p>Write an SQL query to find the countries where this company can invest.</p> <p>Return the result table in any order.</p> <p>The query result format is in the following example.</p> <pre><code>Person table:\n+----+----------+--------------+\n| id | name     | phone_number |\n+----+----------+--------------+\n| 3  | Jonathan | 051-1234567  |\n| 12 | Elvis    | 051-7654321  |\n| 1  | Moncef   | 212-1234567  |\n| 2  | Maroua   | 212-6523651  |\n| 7  | Meir     | 972-1234567  |\n| 9  | Rachel   | 972-0011100  |\n+----+----------+--------------+\n\nCountry table:\n+----------+--------------+\n| name     | country_code |\n+----------+--------------+\n| Peru     | 051          |\n| Israel   | 972          |\n| Morocco  | 212          |\n| Germany  | 049          |\n| Ethiopia | 251          |\n+----------+--------------+\n\nCalls table:\n+-----------+-----------+----------+\n| caller_id | callee_id | duration |\n+-----------+-----------+----------+\n| 1         | 9         | 33       |\n| 2         | 9         | 4        |\n| 1         | 2         | 59       |\n| 3         | 12        | 102      |\n| 3         | 12        | 330      |\n| 12        | 3         | 5        |\n| 7         | 9         | 13       |\n| 7         | 1         | 3        |\n| 9         | 7         | 1        |\n| 1         | 7         | 7        |\n+-----------+-----------+----------+\n\nResult table:\n+----------+\n| country  |\n+----------+\n| Peru     |\n+----------+\nThe average call duration for Peru is (102 + 102 + 330 + 330 + 5 + 5) / 6 = 145.666667\nThe average call duration for Israel is (33 + 4 + 13 + 13 + 3 + 1 + 1 + 7) / 8 = 9.37500\nThe average call duration for Morocco is (33 + 4 + 59 + 59 + 3 + 7) / 6 = 27.5000 \nGlobal call duration average = (2 * (33 + 3 + 59 + 102 + 330 + 5 + 13 + 3 + 1 + 7)) / 20 = 55.70000\nSince Peru is the only country where average call duration is greater than the global average, it's the only recommended country.\n</code></pre> <p>Ans.</p> <pre><code>with call_table as (select \n    distinct\n        caller_id, \n        callee_id,\n        duration\nfrom\ncalls\n)\nselect distinct country from (\n                                select \n                                t.name as country,\n                                c.country_code,\n                                avg(c.duration) over (partition by c.country_code) as country_avg,\n                                avg(c.duration) over () as overall_avg\n                                from (\n                                            select \n                                                substr(b.phone_number,1,3) as country_code,\n                                                a.duration \n                                                from \n                                                    call_table a \n                                                left join \n                                                    Person b \n                                                on a.caller_id = b.id \n                                            union all\n                                            select \n                                                substr(b.phone_number,1,3) as country_code,\n                                                a.duration \n                                                from \n                                                    call_table a \n                                                left join \n                                                    Person b \n                                                on a.callee_id = b.id\n                                    ) c\n                                      left join Country t\n                                      on trim(c.country_code) = trim(t.country_code)\n                                ) \n                                where country_avg&gt;overall_avg;\n</code></pre> <ol> <li>Rectangles Area https://leetcode.com/problems/rectangles-area/  Table: Points</li> </ol> <p>+---------------+---------+ | Column Name   | Type    | +---------------+---------+ | id            | int     | | x_value       | int     | | y_value       | int     | +---------------+---------+ id is the primary key for this table. Each point is represented as a 2D Dimensional (x_value, y_value). Write an SQL query to report of all possible rectangles which can be formed by any two points of the table. </p> <p>Each row in the result contains three columns (p1, p2, area) where:</p> <p>p1 and p2 are the id of two opposite corners of a rectangle and p1 &lt; p2. Area of this rectangle is represented by the column area. Report the query in descending order by area in case of tie in ascending order by p1 and p2.</p> <pre><code>Points table:\n+----------+-------------+-------------+\n| id       | x_value     | y_value     |\n+----------+-------------+-------------+\n| 1        | 2           | 8           |\n| 2        | 4           | 7           |\n| 3        | 2           | 10          |\n+----------+-------------+-------------+\n\nResult table:\n+----------+-------------+-------------+\n| p1       | p2          | area        |\n+----------+-------------+-------------+\n| 2        | 3           | 6           |\n| 1        | 2           | 2           |\n+----------+-------------+-------------+\n\np1 should be less than p2 and area greater than 0.\np1 = 1 and p2 = 2, has an area equal to |2-4| * |8-7| = 2.\np1 = 2 and p2 = 3, has an area equal to |4-2| * |7-10| = 6.\np1 = 1 and p2 = 3 It's not possible because the rectangle has an area equal to 0.\n</code></pre> <p>Ans.</p> <pre><code>-- Approach 1:\nselect \n    a.id as P1,\n    b.id as P2,\n    abs(a.x_value - b.x_value)*abs(a.y_value - b.y_value) as area\nfrom\n    points a, points b\nwhere   \n    a.id &lt; b.id   and \n    abs(a.x_value - b.x_value)*abs(a.y_value - b.y_value)&gt;0\norder by \n        abs(a.x_value - b.x_value)*abs(a.y_value - b.y_value) desc, \n        a.id, \n        b.id; \n-- Approach 2:\nselect     \n    a.p1, b.p2, \n    abs(a.x1-b.x2)*abs(a.y1-b.y2) as area\nfrom     \n(select \n    id as P1,\n    x_value as x1,\n    y_value as y1,\n    concat(x_value, y_value) as p1_coordinate\nfrom points) a, \n(select \n    id as P2,\n     x_value as x2,\n    y_value as y2,\n    concat(x_value, y_value) as p2_coordinate\nfrom points    ) b\nwhere \n    p1&lt;p2\n    and\n    p1_coordinate!=p2_coordinate --no duplicate/same point\n    and \n    abs(x1-x2)*abs(y1-y2)&gt;0\norder by abs(a.x1-b.x2)*abs(a.y1-b.y2) desc, a.p1, b.p2     \n</code></pre> <ol> <li>Active users : Imp -- date diff between consecutive days.  https://leetcode.com/problems/active-users/      </li> </ol> <p>Table Accounts: </p> Column Name Type id int name varchar <p>the id is the primary key for this table.  This table contains the account id and the user name of each account. </p> <p>Table Logins: </p> Column Name Type id int login_date date +---------------+---------+ There is no primary key for this table, it may contain duplicates.  This table contains the account id of the user who logged in and the login date. A user may log in multiple times in the day.  <p>Write an SQL query to find the id and the name of active users. </p> <p>Active users are those who logged in to their accounts for 5 or more consecutive days. </p> <p>Return the result table ordered by the id. </p> <p>The query result format is in the following example: </p> <p>Accounts table:</p> id name 1 Winston 7 Jonathan <p>Logins table: </p> id login_date 7 2020-05-30 1 2020-05-30 7 2020-05-31 7 2020-06-01 7 2020-06-02 7 2020-06-02 7 2020-06-03 1 2020-06-07 7 2020-06-10 <p>Result table: </p> id name 7 Jonathan <p>User Winston with id = 1 logged in 2 times only in 2 different days, so, Winston is not an active user.  User Jonathan with id = 7 logged in 7 times in 6 different days, five of them were consecutive days, so, Jonathan is an active user. </p> <p>Ans.</p> <pre><code>select distinct \n        d.id, \n        d.name \n    from \n        (\n            select \n                b.id,\n                c.name,\n                b.login_date,\n                b.found_recs_old_login_date,\n                to_date(b.login_date,'YYYY-MM-DD') - to_date(b.found_recs_old_login_date,'YYYY-MM-DD') as date_diff\n            from (\n                 select \n                    a.id,\n                    a.login_date,\n                    lag( a.login_date,4,'1990-01-01') over (partition by a.id order by  a.login_date) as found_recs_old_login_date \n                 from \n                 (select distinct id, to_char(login_date,'YYYY-MM-DD') as login_date from Logins ) a  \n                ) b\n            left join \n                  Accounts c\n            on b.id = c.id  \n        ) d \nwhere d.date_diff=4 \norder by d.id;\n</code></pre> <ol> <li>Apples &amp; Oranges  https://leetcode.com/problems/apples-oranges/ </li> </ol> <p>Table: Sales </p> Column Name Type sale_date date fruit enum sold_num int <p>(sale_date,fruit) is the primary key for this table.  This table contains the sales of \"apples\" and \"oranges\" sold each day. </p> <p>Write an SQL query to report the difference between number of apples and oranges sold each day. </p> <p>Return the result table ordered by sale_date in format ('YYYY-MM-DD'). </p> <p>The query result format is in the following example: </p> <pre><code>Sales table:\n+------------+------------+-------------+\n| sale_date  | fruit      | sold_num    |\n+------------+------------+-------------+\n| 2020-05-01 | apples     | 10          |\n| 2020-05-01 | oranges    | 8           |\n| 2020-05-02 | apples     | 15          |\n| 2020-05-02 | oranges    | 15          |\n| 2020-05-03 | apples     | 20          |\n| 2020-05-03 | oranges    | 0           |\n| 2020-05-04 | apples     | 15          |\n| 2020-05-04 | oranges    | 16          |\n+------------+------------+-------------+\n\nResult table:\n+------------+--------------+\n| sale_date  | diff         |\n+------------+--------------+\n| 2020-05-01 | 2            |\n| 2020-05-02 | 0            |\n| 2020-05-03 | 20           |\n| 2020-05-04 | -1           |\n+------------+--------------+\n\nDay 2020-05-01, 10 apples and 8 oranges were sold (Difference  10 - 8 = 2).\nDay 2020-05-02, 15 apples and 15 oranges were sold (Difference 15 - 15 = 0).\nDay 2020-05-03, 20 apples and 0 oranges were sold (Difference 20 - 0 = 20).\nDay 2020-05-04, 15 apples and 16 oranges were sold (Difference 15 - 16 = -1).\n</code></pre> <p>Ans.</p> <pre><code>KL -\nselect\n        coalesce(to_char(a.sale_date,'YYYY-MM-DD'),to_char(b.sale_date,'YYYY-MM-DD')) as sale_date,\n        coalesce(a.sold_num,0) - coalesce(b.sold_num,0) as diff\nfrom \n    (select sale_date , sold_num from sales where fruit='apples') a  \nfull join\n    (select sale_date , sold_num from sales where fruit='oranges') b\non \n    to_char(a.sale_date,'YYYY-MM-DD')=to_char(b.sale_date,'YYYY-MM-DD')\norder by \n    coalesce(to_char(a.sale_date,'YYYY-MM-DD'),to_char(b.sale_date,'YYYY-MM-DD'));\n\nNG -\n    select sale_date, \n           sum(case when fruit = 'apples' then sold_num else 0-sold_num end) as \"diff\"\n    from sales\n    group by sale_date\n</code></pre> <ol> <li>Evaluate Boolean Expression  https://leetcode.com/problems/evaluate-boolean-expression/ </li> </ol> <p>Table Variables: </p> Column Name Type name varchar value int <p>name is the primary key for this table.  This table contains the stored variables and their values. </p> <p>Table Expressions: </p> Column Name Type left_operand varchar operator enum right_operand varchar <p>(left_operand, operator, right_operand) is the primary key for this table.  This table contains a boolean expression that should be evaluated.  operator is an enum that takes one of the values ('&lt;', '&gt;', '=')  The values of left_operand and right_operand are guaranteed to be in the Variables table. </p> <p>Write an SQL query to evaluate the boolean expressions in Expressions table. </p> <p>Return the result table in any order. </p> <p>The query result format is in the following example. </p> <pre><code>Variables table:\n+------+-------+\n| name | value |\n+------+-------+\n| x    | 66    |\n| y    | 77    |\n+------+-------+\n\nExpressions table:\n+--------------+----------+---------------+\n| left_operand | operator | right_operand |\n+--------------+----------+---------------+\n| x            | &gt;        | y             |\n| x            | &lt;        | y             |\n| x            | =        | y             |\n| y            | &gt;        | x             |\n| y            | &lt;        | x             |\n| x            | =        | x             |\n+--------------+----------+---------------+\n\nResult table:\n+--------------+----------+---------------+-------+\n| left_operand | operator | right_operand | value |\n+--------------+----------+---------------+-------+\n| x            | &gt;        | y             | false |\n| x            | &lt;        | y             | true  |\n| x            | =        | y             | false |\n| y            | &gt;        | x             | true  |\n| y            | &lt;        | x             | false |\n| x            | =        | x             | true  |\n+--------------+----------+---------------+-------+\nAs shown, you need find the value of each boolean exprssion in the table using the variables table.\n</code></pre> <p>Ans.</p> <pre><code>select \n        left_operand as \"left_operand\",\n        operator as \"operator\",\n        right_operand as \"right_operand\",\n        case \n            when operator='&gt;' then case when left_value&gt;right_value then 'true' else 'false' end \n            when operator='&lt;' then case when left_value&lt;right_value then 'true' else 'false' end \n            when operator='=' then case when left_value=right_value then 'true' else 'false' end \n        end as \"value\" \n        from \n            (\n                select\n                    a.left_operand  ,\n                    b.value as left_value,\n                    a.operator ,\n                    a.right_operand ,\n                    c.value as right_value\n                from \n                    expressions a\n                left join \n                    variables b\n                on a.left_operand = b.name\n                left join \n                    variables c\n                on a.right_operand = c.name\n            );\n</code></pre> <ol> <li>Customers Who Bought Products A and B but Not C   https://leetcode.com/problems/customers-who-bought-products-a-and-b-but-not-c/submissions/  </li> </ol> <p>Table: Customers </p> Column Name Type customer_id int customer_name varchar <p>customer_id is the primary key for this table.  customer_name is the name of the customer. </p> <p>Table: Orders</p> Column Name Type order_id int customer_id int product_name varchar <p>order_id is the primary key for this table.  customer_id is the id of the customer who bought the product \"product_name\". </p> <p>Write an SQL query to report the customer_id and customer_name of customers who bought products \"A\", \"B\" but did not buy the product \"C\" since we want to recommend them buy this product. </p> <p>Return the result table ordered by customer_id. </p> <p>The query result format is in the following example. </p> <pre><code>Customers table:\n+-------------+---------------+\n| customer_id | customer_name |\n+-------------+---------------+\n| 1           | Daniel        |\n| 2           | Diana         |\n| 3           | Elizabeth     |\n| 4           | Jhon          |\n+-------------+---------------+\n\nOrders table:\n+------------+--------------+---------------+\n| order_id   | customer_id  | product_name  |\n+------------+--------------+---------------+\n| 10         |     1        |     A         |\n| 20         |     1        |     B         |\n| 30         |     1        |     D         |\n| 40         |     1        |     C         |\n| 50         |     2        |     A         |\n| 60         |     3        |     A         |\n| 70         |     3        |     B         |\n| 80         |     3        |     D         |\n| 90         |     4        |     C         |\n+------------+--------------+---------------+\n\nResult table:\n+-------------+---------------+\n| customer_id | customer_name |\n+-------------+---------------+\n| 3           | Elizabeth     |\n+-------------+---------------+\nOnly the customer_id with id 3 bought the product A and B but not the product C.\n</code></pre> <p>Ans. </p> <pre><code>select \n    customer_id,\n    customer_name\nfrom (\n        select \n            distinct  \n                    a.customer_id,  \n                    b.customer_name, \n                    a.product_name \n           from \n            orders a \n                left join      \n            customers b      \n            on a.customer_id = b.customer_id     \n        where a.product_name in ('A','B','C') \n    )\n    group by customer_id,customer_name\n    having sum(case when product_name='A' then 1 when product_name='B' then 2 else 100 end)=3\n    order by customer_id;\n</code></pre> <ol> <li>Capital Gain/Loss  https://leetcode.com/problems/capital-gainloss/ </li> </ol> <p>Table: Stocks </p> Column Name Type stock_name varchar operation enum operation_day int price int <p>(stock_name, day) is the primary key for this table.  The operation column is an ENUM of type ('Sell', 'Buy')  Each row of this table indicates that the stock which has stock_name had an operation on the day operation_day with the price.  It is guaranteed that each 'Sell' operation for a stock has a corresponding 'Buy' operation in a previous day. </p> <p>Write an SQL query to report the Capital gain/loss for each stock. </p> <p>The capital gain/loss of a stock is total gain or loss after buying and selling the stock one or many times. </p> <p>Return the result table in any order. </p> <p>The query result format is in the following example: </p> <pre><code>Stocks table:\n+---------------+-----------+---------------+--------+\n| stock_name    | operation | operation_day | price  |\n+---------------+-----------+---------------+--------+\n| Leetcode      | Buy       | 1             | 1000   |\n| Corona Masks  | Buy       | 2             | 10     |\n| Leetcode      | Sell      | 5             | 9000   |\n| Handbags      | Buy       | 17            | 30000  |\n| Corona Masks  | Sell      | 3             | 1010   |\n| Corona Masks  | Buy       | 4             | 1000   |\n| Corona Masks  | Sell      | 5             | 500    |\n| Corona Masks  | Buy       | 6             | 1000   |\n| Handbags      | Sell      | 29            | 7000   |\n| Corona Masks  | Sell      | 10            | 10000  |\n+---------------+-----------+---------------+--------+\n\nResult table:\n+---------------+-------------------+\n| stock_name    | capital_gain_loss |\n+---------------+-------------------+\n| Corona Masks  | 9500              |\n| Leetcode      | 8000              |\n| Handbags      | -23000            |\n+---------------+-------------------+\n</code></pre> <p>Leetcode stock was bought at day 1 for 1000$ and was sold at day 5 for 9000$. Capital gain = 9000 - 1000 = 8000$.  Handbags stock was bought at day 17 for 30000$ and was sold at day 29 for 7000$. Capital loss = 7000 - 30000 = -23000$.  Corona Masks stock was bought at day 1 for 10$ and was sold at day 3 for 1010$. It was bought again at day 4 for 1000$ and was sold at day 5 for 500$.   At last, it was bought at day 6 for 1000$ and was sold at day 10 for 10000$.   Capital gain/loss is the sum of capital gains/losses for each ('Buy' --&gt; 'Sell') operation   = (1010 - 10) + (500 - 1000) + (10000 - 1000) = 1000 - 500 + 9000 = 9500$.</p> <p>Ans.</p> <pre><code>select\n    stock_name,\n    sum(case when operation ='Sell' then price else 0-price end) as capital_gain_loss \nfrom \n    stocks\ngroup by stock_name\norder by stock_name;\n</code></pre> <ol> <li>Number of Trusted Contacts of a Customer  https://leetcode.com/problems/number-of-trusted-contacts-of-a-customer/ </li> </ol> <p>Table: Customers </p> Column Name Type customer_id int customer_name varchar email varchar <p>customer_id is the primary key for this table.  Each row of this table contains the name and the email of a customer of an online shop. </p> <p>Table: Contacts </p> Column Name Type user_id id contact_name varchar contact_email varchar <p>(user_id, contact_email) is the primary key for this table.  Each row of this table contains the name and email of one contact of customer with user_id.  This table contains information about people each customer trust. The contact may or may not exist in the Customers table. </p> <p>Table: Invoices </p> Column Name Type invoice_id int price int user_id int <p>invoice_id is the primary key for this table.  Each row of this table indicates that user_id has an invoice with invoice_id and a price. </p> <p>Write an SQL query to find the following for each invoice_id: </p> <p>customer_name: The name of the customer the invoice is related to.  price: The price of the invoice.  contacts_cnt: The number of contacts related to the customer.  trusted_contacts_cnt: The number of contacts related to the customer and at the same time they are customers to the shop.   (i.e His/Her email exists in the Customers table.)  Order the result table by invoice_id. </p> <p>The query result format is in the following example: </p> <p>Customers table: </p> customer_id customer_name email 1 Alice alice@leetcode.com 2 Bob bob@leetcode.com 13 John john@leetcode.com 6 Alex alex@leetcode.com <p>Contacts table:</p> user_id contact_name contact_email 1 Bob bob@leetcode.com 1 John john@leetcode.com 1 Jal jal@leetcode.com 2 Omar omar@leetcode.com 2 Meir meir@leetcode.com 6 Alice alice@leetcode.com <p>Invoices table:  </p> invoice_id price user_id 77 100 1 88 200 1 99 300 2 66 400 2 55 500 13 44 60 6 <p>Result table:</p> invoice_id customer_name price contacts_cnt trusted_contacts_cnt 44 Alex 60 1 1 55 John 500 0 0 66 Bob 400 2 0 77 Alice 100 3 2 88 Alice 200 3 2 99 Bob 300 2 0 <p>Alice has three contacts, two of them are trusted contacts (Bob and John).  Bob has two contacts, none of them is a trusted contact.  Alex has one contact and it is a trusted contact (Alice).  John doesn't have any contacts. </p> <p>Ans.</p> <pre><code>select\n    a.invoice_id ,\n    b.customer_name,\n    a.price,\n    (select count(*) from contacts where user_id =  a.user_id) as contacts_cnt ,\n    (select count(*) from contacts where user_id = a.user_id and contact_email in (select email from customers) ) as trusted_contacts_cnt \nfrom\n    invoices a\nleft join \n    customers b\non \n    a.user_id = b.customer_id\norder by invoice_id;\n\n-- Other Approach: Using Joins\nselect\n        a.invoice_id ,\n        b.customer_name,\n        a.price,\n        count(e.contact_email) as contacts_cnt  ,\n        sum(case when e.email is not NULL then 1 else 0 end) as trusted_contacts_cnt \nfrom\n        invoices a\nleft join \n        customers b\non \n        a.user_id = b.customer_id\nleft join \n(\n    select \n        c.user_id,\n        c.contact_email,\n        d.email\n        from contacts c left join \n            customers d \n        on c.contact_email = d.email\n) e\non a.user_id = e.user_id\ngroup by a.invoice_id , b.customer_name, a.price\norder by a.invoice_id;\n</code></pre> <ol> <li>Activity Participants  https://leetcode.com/problems/activity-participants/  </li> </ol> <p>Table: Friends </p> Column Name Type id int name varchar activity varchar <p>id is the id of the friend and primary key for this table. name is the name of the friend. activity is the name of the activity which the friend takes part in.</p> <p>Table: Activities </p> Column Name Type id int name varchar <p>id is the primary key for this table. name is the name of the activity.</p> <p>Write an SQL query to find the names of all the activities with neither maximum, nor minimum number of participants.</p> <p>Return the result table in any order. Each activity in table Activities is performed by any person in the table Friends.</p> <p>The query result format is in the following example:</p> <pre><code>Friends table:\n+------+--------------+---------------+\n| id   | name         | activity      |\n+------+--------------+---------------+\n| 1    | Jonathan D.  | Eating        |\n| 2    | Jade W.      | Singing       |\n| 3    | Victor J.    | Singing       |\n| 4    | Elvis Q.     | Eating        |\n| 5    | Daniel A.    | Eating        |\n| 6    | Bob B.       | Horse Riding  |\n+------+--------------+---------------+\n\nActivities table:\n+------+--------------+\n| id   | name         |\n+------+--------------+\n| 1    | Eating       |\n| 2    | Singing      |\n| 3    | Horse Riding |\n+------+--------------+\n\nResult table:\n+--------------+\n| activity     |\n+--------------+\n| Singing      |\n+--------------+\n\nEating activity is performed by 3 friends, maximum number of participants, (Jonathan D. , Elvis Q. and Daniel A.)\nHorse Riding activity is performed by 1 friend, minimum number of participants, (Bob B.)\nSinging is performed by 2 friends (Victor J. and Jade W.)\n</code></pre> <p>Ans.</p> <pre><code>select activity from (\n                select\n                    activity,\n                    count(*) as act_count,\n                    min(count(*)) over () as min_cnt,\n                    max(count(*)) over () as max_cnt\n                from friends\n                group by activity\n                order by activity\n                    ) \nwhere min_cnt&lt;act_count and act_count&lt;max_cnt;\n</code></pre> <ol> <li>Second Degree Follower  https://leetcode.com/problems/second-degree-follower/submissions/ </li> </ol> <p>In facebook, there is a follow table with two columns: followee, follower. </p> <p>Please write a sql query to get the amount of each follower\u2019s follower if he/she has one. </p> <p>For example: </p> followee follower A B B C B D D E <p>should output:</p> follower num B 2 D 1 <p>Explaination:  Both B and D exist in the follower list, when as a followee, B's follower is C and D, and D's follower is E.  A does not exist in follower list. </p> <p>Note:  Followee would not follow himself/herself in all cases. </p> <p>Ans.  There could be duplicates in table, so use count distinct for counting followers. </p> <pre><code>select \n    main as follower, \n    count(distinct follower) as num \nfrom (\n    select \n        a.follower as main,\n        b.follower    as follower\n    from follow a inner join follow b\n    on a.follower = b.followee\n    )\ngroup by main \norder by main;\n</code></pre>"},{"location":"SQL/docs/sql_performance_tuning/","title":"SQL Performance Tuning : Summary","text":""},{"location":"SQL/docs/sql_performance_tuning/#tip-1-never-use-star-to-fetch-all-records-from-table","title":"Tip 1: Never use *(Star) to fetch all records from table","text":"<p>Sql query become fast if you use actual columns instead of * to fetch all the records from the table.</p> <p>Not Recommended - </p> <pre><code>Select * from Employee;\n</code></pre> <p>Recommended  </p> <pre><code>Select Eno,Ename,Address from Employee;\n</code></pre> <p></p>"},{"location":"SQL/docs/sql_performance_tuning/#tip-2-try-to-avoid-distinct-keyword-from-the-query","title":"Tip 2: Try to avoid DISTINCT keyword from the query","text":"<p>Try to avoid DISTINCT keyword from select statements. DISTINCT keyword has high cost and low performance. When anyone uses DISTINCT keyword, it first SORTS the data from column and then fetches the distinct values. Use EXISTS operator instead of DISTINCT keyword.</p> <p>Not Recommended:</p> <pre><code>SELECT \n    DISTINCT \n             d.dept_no, \n             d.department_name\nFROM Department d,Employee e\nWHERE d.dept_no= e.dept_no;\n</code></pre> <p>Recommended:</p> <pre><code>SELECT \n    d.dept_no, \n    d.department_name\nFROM Department d\nWHERE EXISTS ( SELECT \u2018X\u2019 FROM Employee e WHERE d.dept_no= e.dept_no);\n</code></pre> <p></p>"},{"location":"SQL/docs/sql_performance_tuning/#tip-3-carefully-use-where-conditions-in-sql","title":"Tip 3: Carefully use WHERE conditions in sql","text":"<p>Try to use correct operator as per requirement given.</p> <p>Not Recommended:</p> <pre><code>Select * \nfrom Employee \nWHERE salary != 65000;\n</code></pre> <p>Recommended:</p> <pre><code>Select * \nfrom Employee \nWHERE salary &gt; 65000 and salary &lt; 65000;\n</code></pre> <p></p>"},{"location":"SQL/docs/sql_performance_tuning/#tip-4-use-like-operator-instead-of-equal-to","title":"Tip 4: Use Like operator instead of equal to (=)","text":"<p>Not Recommended:</p> <pre><code>Select * \nfrom Employee \nwhere name=\u2019Amit\u2019;\n</code></pre> <p>Recommended:</p> <pre><code>Select * \nfrom Employee \nwhere name like \u2018Amit%\u2019;\n</code></pre> <p></p>"},{"location":"SQL/docs/sql_performance_tuning/#tip-5-avoid-having-clausegroup-by-statements","title":"Tip 5: Avoid HAVING clause/GROUP BY statements","text":"<p>HAVING clause and GROUP BY statements have high cost. So try to avoid it in sql query.</p> <p>Not Recommended - </p> <pre><code>Select * \nfrom Employee \nWHERE name=\u2019Amit\u2019  \nGROUP BY department \nHAVING salary=45000;\n</code></pre> <p>Recommended - </p> <pre><code>Select * \nfrom Employee \nWHERE name=\u2019Amit\u2019 and salary=45000;\n</code></pre> <p>More added: -  Having clause-  We use Having clause to eliminate some of the group values.  Issue \u2013 Having clause restricts the rows AFTER they are read.   So if no restriction in \u201cwhere clause\u201d,  optimizer will use full table scan.  This is really important coz all predicates in the HAVING Clause will not be used as access predicates. So they will not make optimizer use indexes, partitions etc.  This is because to perform HAVING clause, it first reads all the rows and then eliminates unnecessary rows. </p> <p></p>"},{"location":"SQL/docs/sql_performance_tuning/#tip-6-use-of-exists-and-in-operators","title":"Tip 6: Use of EXISTS and IN Operators","text":"<p>Basically, Operator IN has lowest performance. IN operator is used when Filter criteria is in subquery, whereas EXISTS operator is used when filter criteria is in main query.</p> <p>Example: IN Operator </p> <pre><code>Select * \nfrom Employee \nwhere Department_name IN (\n                          Select Department_name \n                          from Department \n                          where Dno=10);\n</code></pre> <p>Exist operator </p> <pre><code>Select * \nfrom Employee \nwhere EXISTS (\n               Select Department_name \n               from Department \n               where Dno=10);\n</code></pre> <p>More added  When you run a query with IN clause, database will process it in below format \u2013 That is, in case of use of IN clause, each value of sub query is joined with outer query. Treats below Query -</p> <pre><code>select * from T1 where x in (select x from T2);\n</code></pre> <p>as -</p> <pre><code>select * from t1, (select x from t2) T2 where t1.x = t2.x;\n</code></pre> <p>But when you use EXIST clause, database will exit as soon as it gets the first match. So, in case of EXIST clause it runs executes query in below format \u2013 Treats below query -</p> <pre><code>select * from T1 where exists (select x from T2 where t1.x=t2.x);\n</code></pre> <p>as -</p> <pre><code>FOR x  IN (select * from t1) LOOP\n    IF (EXISTS ( SELECT X FROM T2) ) THEN\n        OUTPUT THE RECORD\n    END IF;\nEND;\n</code></pre> <p>That is, using EXIST clause will imply database will run it like a FOR loop and as soon as match is found, it moves to next record.</p> <p>So which one is faster \u2013 IN or EXIST? a. This totally depends on situation. Use IN when - outer table = Big and Subquery = Small Use EXISTS when \u2013 outer table = Small and Subquery = Big</p> <p>b. Even above rules are not fixed. For example, if subquery has bigger table, but it has an index, in this case use of IN is suggested.</p> <p>c. So- EXISTS doesn\u2019t work better than IN all the times. IN is better than EXISTS if \u2013 outer table = Big and Subquery = Small outer table = Small and Subquery = Big + Indexed</p> <p>NOT EXISTS vs NOT IN  \u2022   NOT EXISTS is not equivalent of NOT IN. \u2022   NOT EXISTS cannot be used instead of NOT IN all the times.  \u2022   More specifically, if there is any NULL value in your data, they will show different result. \u2022   If your subquery returns even one NULL value, NOT IN will not match any rows. \u2022   On other hand, if you have a chance to use NOT EXISTS instead of NOT IN, you should test it. \u2022   In most database versions of oracle, EXISTS and IN are treated similarly in terms of execution plan.</p> <p></p>"},{"location":"SQL/docs/sql_performance_tuning/#tip-7-try-to-use-union-all-instead-of-union","title":"Tip 7: Try to use UNION ALL instead of UNION","text":"<p>UNION scans all data first and then eliminate duplicate so it has slow performance.</p> <p>Not Recommended</p> <pre><code>Select * from Employee where dept_no=10\nUNION\nSelect * from Employee where dept_no=20;\n</code></pre> <p>Recommended </p> <pre><code>Select * from Employee where dept_no=10\nUNION  ALL\nSelect * from Employee where dept_no=20;\n</code></pre> <p></p>"},{"location":"SQL/docs/sql_performance_tuning/#tip-8-avoid-use-of-functions-in-where-condition","title":"Tip 8: Avoid use of Functions in Where condition.","text":"<p>Not Recommended </p> <pre><code>Select * from Employee where Substr(name,1,3)=\u2019Ami\u2019;\n</code></pre> <p>Recommended </p> <pre><code>Select * from Employee where name like \u2018Ami%\u2019;\n</code></pre> <p></p>"},{"location":"SQL/docs/sql_performance_tuning/#tip-9-convert-or-to-and","title":"Tip 9: convert OR to AND","text":"<p>If we use OR clause, it will PREVENT index usages. Instead, we should use AND where possible. Not Recommended </p> <pre><code>select * from sales where prod_id = 13 or promo_id=14;\n</code></pre> <p>Recommended </p> <pre><code>select * from sales where prod_id = 13\nUNION All\nselect * from sales where promo_id=14 AND prod_id &lt;&gt; 13;\n</code></pre> <p></p>"},{"location":"SQL/docs/sql_performance_tuning/#tip-10-subquery-unnesting","title":"Tip 10: Subquery Unnesting","text":"<p>Nested queries are very costly, and so transformer tries to convert them to equivalent join statements. Not Recommended </p> <pre><code>select * from sales \nwhere cust_id IN (select cust_id from customers);\n</code></pre> <p>Recommended </p> <pre><code>select sales.* from sales, customers \nwhere sales.cust_id=customers.cust_id;\n</code></pre> <p></p>"},{"location":"SQL/docs/sql_performance_tuning/#tip-11-in-and-between","title":"Tip 11: IN and BETWEEN","text":"<pre><code>select * from employees where emp_id in (2,3,4,5); \n</code></pre> <p>The above is equivalent to </p> <pre><code>select * from employees \nwhere emp_id = 2 OR emp_id=3 OR emp_id=4 OR emp_id=5\n</code></pre> <p>---this implies full table scan.</p> <p>Solution -</p> <pre><code>select * from employees where emp_id between 2 and 5;\n</code></pre> <p></p>"},{"location":"SQL/docs/sql_performance_tuning/#tip-12-fetching-first-n-records","title":"Tip 12: Fetching first N records","text":"<p>Suppose we want to see only 10 records in our select statement output.  There are 2 ways to do this \u2013 Using rownum  (Recommended)</p> <pre><code>SELECT * FROM EMPLOYEE where rownum&lt;11;\n</code></pre> <p>Using fetch first (not recommended)</p> <pre><code>SELECT * FROM EMPLOYEE FETCH FIRST 10 ROWS ONLY;\n</code></pre> <p>In case of rownum-  Here it reads first 10 rows use count STOPKEY operator, and so faster than fetch first method.</p> <p>In case of fetch first \u2013 Here we read whole table, and then applied a windowing function to select 1st 10 records.</p> <p></p>"},{"location":"SQL/docs/sql_performance_tuning/#tip-13-union-vs-union-all","title":"Tip 13: UNION vs UNION ALL:","text":"<p>UNION \u2013 combines data and drops duplicate rows. UNION ALL \u2013 combines data and retains duplicate rows. Suggest: Some key points- \u2022   by default, UNION ALL is less costly than UNION, as latter sorts data internally to remove duplicates. \u2022   But if your table is indexed, then sort operation in UNION wont be that costly, and so you can use UNION also. So \u2013 -   Use UNION if table is indexed and you don\u2019t want duplicates in output. -   Use UNION ALL if\u2013     -   There is no duplicate in your data, or     -   You are ok with having duplicate data in output. -   But overall, UNION ALL gives better performance than UNION.</p> <p></p>"},{"location":"SQL/docs/sql_performance_tuning/#tip-14-intersect-vs-exists-operator","title":"Tip 14: INTERSECT Vs EXISTS operator","text":"<p>Intersect gives common rows of 2 intersection in a Sorted order. As part of intersect, 2 rows sources are first sorted, and then common records are fetched.</p> <p>In place of INTERSECT operator, we should try and use EXISTS clause, which is more efficient.  One caveat is that, in case of EXISTS clause, output is not sorted, unlike in case of INTERSECT clause.</p> <p>Not Recommended </p> <pre><code>SELECT employee_id \nFROM employees \nwhere employee_id between 145 and 179\nINTERSECT\nSELECT employee_id \nFROM employees \nWHERE first_name LIKE 'A%';\n</code></pre> <p>Recommended </p> <pre><code>SELECT employee_id \nFROM employees e \nwhere employee_id between 145 and 179\nand exists\n         (SELECT employee_id \n          FROM employees t \n          WHERE first_name LIKE 'A%' and e.employee_id = t.employee_id);\n</code></pre> <p></p>"},{"location":"SQL/docs/sql_performance_tuning/#tip-15-minus-vs-not-exists","title":"Tip 15: MINUS Vs NOT EXISTS","text":"<p>MINUS operator eliminates matched rows of 1st (with 2nd) and returns rest of the rows of 1st.  NOT EXISTS clause can also do same work as MINUS, but has much better performance.  Not Recommended </p> <pre><code>SELECT employee_id FROM employees where employee_id between 145 and 179\nMINUS\nSELECT employee_id FROM employees WHERE first_name LIKE 'A%';\n</code></pre> <p>Recommended </p> <pre><code>SELECT employee_id FROM employees e where employee_id between 145 and 179\nand not exists\n(SELECT employee_id FROM employees t WHERE first_name LIKE 'A%' and e.employee_id = t.employee_id);\n</code></pre> <p></p>"},{"location":"SQL/docs/sql_performance_tuning/#tip-16-using-like-conditions","title":"Tip 16: Using Like conditions","text":"<p>To enable use of indexes, avoid use of wild card character at beginning of source text scan. If you are forced to use wild card character at beginning, you can create reverse index, and handle that problem using that index.  In this case, when you reverse index, then source text will eliminate use of wild card at beginning. Eg \u2013  Suppose you want to find records where last name ends is \u201chhar\u201d, then create reverse() index on last_name to reverse it and then use condition where reversed last name begins with \u201crahh\u201d.  Though reverse() index usage will have cost, but if your column is selective enough, it wont be much cost.</p> <p></p>"},{"location":"SQL/docs/sql_performance_tuning/#tip-17-using-functions-on-indexed-columns-will-suppress-index-usage","title":"Tip 17: Using Functions on Indexed Columns will suppress index usage.","text":"<p>Use of function on indexed column will suppress index usage. So, rewrite query to avoid use of function. BAD QUERY</p> <pre><code>    select employee_id, first_name, last_name\n    from employees \n    where trunc(hire_date,'YEAR') = '01-JAN-2002';\n</code></pre> <p>GOOD: rewritten query</p> <pre><code>    select employee_id, first_name, last_name\n    from employees \n    where hire_date between '01-JAN-2002' and '31-DEC-2002';\n</code></pre> <p>Eg2 - Bad</p> <pre><code>select * from mytable where substr(emp_name,1,2) = 'Po';\n</code></pre> <p>Good </p> <pre><code>select * from mytable where emp_name like 'Po%';\n</code></pre> <p>Note -  In Spark, we have HashAggregates and SortAggregates.  Hash Aggregates are more performant, and work only on mutable data types. That is, if all elements in your Select clause (except those in Group by clause) are mutable types like INT, FLOAT, etc, then spark will use Hash Aggregates.  This means, sometimes, for performance gain, we need to apply Function to transform values.</p> <p>See #14 at below link for details  https://github.com/kushal-luthra/spark-development/blob/master/notes/spark_opimization.md</p> <p></p>"},{"location":"SQL/docs/sql_performance_tuning/#tip-18-handling-null-values","title":"Tip 18: Handling NULL Values","text":"<p>Failing to deal with NULL values will lead to unintentional results or performance losses. Why - \u2022B-Tree indexes do not index NULL values. \u2022If there are any NULL values in your indexed columns and you need to get rows which have NULL values, optimizer will not use your index, and perform a full table scan instead. \u2022That is, having Null values in your index may sometimes suppress index usage.</p> <p>Ways to handle NULL value-based performance loss  a. Use IS NOT NULL condition in your WHERE clause.   Use IS NOT NULL condition in your WHERE clause if you don\u2019t need to have NULL values in result set.  That is, even if you now your result will not be having any NULL values, you should use \u201cis not null \u201c clause to make optimizer use indexes.   Eg \u2013 Query 1: </p> <pre><code>select emp_name, emp_id \nfrom employee \nwhere emp_id &lt;&gt; 1;\n</code></pre> <p>Query 2: </p> <pre><code>select emp_name, emp_id \nfrom employee \nwhere emp_id &lt;&gt; 1 and emp_id is not null;\n</code></pre> <p>In query 1, we will see FULL Table scan and in case of query 2, we see index-based scan, and lower query cost. </p> <p>b. Adding not null constraint to your columns and insert a specific value for NULL values like \u20180\u2019 if value in a column is null. </p> <p>c. If reasonable, create a BITMAP index instead of B-Tree index.  BITMAP indexes store NULL values.  So even if there are null values in our column, optimizer will be able to use our indexes.  However, you need to take into consideration index efficiency between B-Tree and BITMAP, as former as more efficient than latter. - We use BITMAP indexes when \u2013 cardinality is LOW and index not modified often.  - We use B-Tree index when \u2013 cardinality/selectivity  is high.</p>"},{"location":"SQL/docs/sql_performance_tuning/#tip-19-use-truncate-instead-of-delete","title":"Tip 19 : Use Truncate instead of Delete","text":"<p>Truncate is always faster than DELETE command.   This is because when you run delete command, oracle database generates lots of UNDO data for deleted rows and generating UNDO data is an expensive operation.  Truncate doesn\u2019t generate UNDO Data. </p> <p>But before using Truncate command, there are few things to note about it-  \u2022   No rollback \u2013   Truncate operation cannot be rollbacked, and Flashback is also not so easy after truncate operations.      You may need to use Flashback Data Archive or some other external tools in this case.   \u2022   Truncate is a DDL operation \u2013   So when you run Truncate, your transaction will be committed.  It performs commit before and after Truncate operation.  Since it does 2 commits, and even if truncate operation fails in between, the changes you did before will be permanent in any case.   \u2022   Truncate a partition -  We don\u2019t need to truncate whole table all the times. You can truncate partition as well.   \u2022   Truncate doesn\u2019t fire DML triggers -   So you wont be able to log your truncate operation because of that.  But it can fire the DDL triggers.   \u2022   Truncate makes unusable indexes usable again.  But delete does not.  </p>"},{"location":"SQL/docs/sql_performance_tuning/#tip-20-data-type-mismatches","title":"Tip 20: Data Type Mismatches","text":"<p>If data types of column and compared value dont match, this may suppress index usage.</p> <pre><code>select cust_id, cust_code from customers where cust_code = 101;\n\nVs\n\nselect cust_id, cust_code from customers where cust_code = '101';\n</code></pre> <p></p>"},{"location":"SQL/docs/sql_performance_tuning/#tip-21-tuning-ordered-queries-order-by-clause","title":"Tip 21: Tuning Ordered queries- Order By clause","text":"<p>Order by mostly requires SORT operations. This sort operation is done in PGA or in disk (if PGA doesn\u2019t have enough memory) This disk is shown as \u2018temporary table space\u2019 in execution plan. Issue \u2013 sorting in disk is a costly operation.</p> <p>Solution \u2013 \u2022 Create a B-Tree index on column used in Order by Clause, or \u2022 Modify a B-Tree index to include column used in Order by Clause. \u2022 Why\u2013&gt;  B-Tree indexes store columns in Order and using B-Tree index will eliminate sort operations.</p> <p></p>"},{"location":"SQL/docs/sql_performance_tuning/#tip-22-retrieving-min-and-max-values","title":"Tip 22 : Retrieving MIN and MAX Values","text":"<p>B-Tree indexes increase the performance a lot for min and max value searches. If no B-Tree index, optimizer will need to read whole table.</p> <p>If our query has another column or another aggregate function in your query, it will be reading whole index or whole table. For example- When you see below, if we are looking for min() and max() values individually, output is just 2 for each. But when we want to get min() and max() together, database will read full table, and hence cost is 8 times. This is coz we have 2 aggregate functions in our query. Bad - </p> <pre><code>    select min(), max() from mytable;\n</code></pre> <p>Good - </p> <pre><code>    select * FROM \n        (select min() from mytable) min_cust, \n        (select max() from mytable) max_cust;\n</code></pre> <p></p>"},{"location":"SQL/docs/sql_performance_tuning/#tip-23-views","title":"Tip 23 : Views","text":"<p>Simple view = view created from single table.  Complex view = view created by using multiple tables.  Some suggestions w.r.t. views-  1.  If you don\u2019t need to use all the tables in a view, then don\u2019t use the view.  Instead use the actual tables.  Otherwise, server will have to join all tables, do aggregation etc on them for a view. i.e. use view for the purpose for which it was created.   2.  Else create another view.    3.  Don\u2019t join complex views with a table or another view -  This is because most of the times view is first executed completely at first, and then result is used as row source to other table or view.  So, in this case you be reading lots of unnecessary data and performing unnecessary join and group by.  This will increase cost a lot.   4.  Avoid performing outer join to the views \u2013   because if you use equality predicate on view column, the optimizer gets wrong if the table of that column has an outer join in the view as well.   Because outer join may not know performance of view and may lead to bad execution as well.      E.g. \u2013 if we do outer join, optimizer may not be able to push predicate inside the view definition at times of execution plan.  </p> <p>Materialized Views-  Unlike basic and complex views, materialized views store both query and data.  Materialized view data can be refreshed manually or via auto-refresh.  But materialized view maintenance is a burden to database \u2013 it needs to be kept up to date for each modification on each change.  As compared to normal views, materialized view will improve performance as we will select data directly from materialized view, and there will be no sorts, joins etc.  We can create index, partitions etc on materialized view like in an ordinary table. </p> <p>Summary   \u2022 If you don\u2019t need to use all the tables in a view, then don\u2019t use the view. Instead use the actual tables.   \u2022 Don\u2019t join complex views with a table or another view.   \u2022 Avoid performing outer join to the views.   \u2022 Use Materialized View -   Unlike basic and complex views, materialized views store both query and data. Materialized view data can be refreshed manually or via auto-refresh. But materialized view maintenance is a burden to database .</p> <p></p>"},{"location":"SQL/docs/sql_performance_tuning/#tip-24-frequent-commit-is-not-desired","title":"Tip 24: Frequent commit is not desired","text":"<ul> <li>make REDO logs bulky as we may be committing prior to period. </li> <li>make lock on modified rows, making them unavailable to other applications.</li> </ul>"},{"location":"SQL/docs/sql_performance_tuning/#tip-25-multitable-dml-operations-skip-for-big-data","title":"Tip 25: Multitable DML operations (skip for big data)","text":"<p>Sometimes we have to read same table as input to different tables in our data warehouse.  So, if we have 5 different tables requiring input from 1 table, we should ideally be reading input table just once, and keep on feeding into different output tables as per requirements.  For this we have 2 options \u2013  \u2022   INSERT ALL  \u2022   MERGE INTO </p>"},{"location":"SQL/docs/sql_performance_tuning_summary/","title":"SQL Performance Tuning : Summary","text":""},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-1-never-use-star-to-fetch-all-records-from-table","title":"Tip 1: Never use *(Star) to fetch all records from table","text":""},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-2-try-to-avoid-distinct-keyword-from-the-query","title":"Tip 2: Try to avoid DISTINCT keyword from the query","text":"<p>Not Recommended:</p> <pre><code>SELECT DISTINCT d.dept_no, d.department_name\n    FROM Department d,Employee e\n    WHERE d.dept_no= e.dept_no;\n</code></pre> <p>Recommended:</p> <pre><code>SELECT \n    d.dept_no d.department_name\nFROM Department d\nWHERE EXISTS ( SELECT \u2018X\u2019 FROM Employee e WHERE d.dept_no= e.dept_no);\n</code></pre>"},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-3-carefully-use-where-conditions-in-sql","title":"Tip 3: Carefully use WHERE conditions in sql.","text":""},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-4-use-like-operator-instead-of-equal-to","title":"Tip 4: Use Like operator instead of equal to (=)","text":""},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-5-avoid-having-clausegroup-by-statements","title":"Tip 5: Avoid HAVING clause/GROUP BY statements","text":""},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-6-use-of-exists-and-in-operators","title":"Tip 6: Use of EXISTS and IN Operators","text":""},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-7-try-to-use-union-all-instead-of-union-as-union-scans-all-data-first-and-then-eliminate-duplicate-so-it-has-slow-performance","title":"Tip 7: Try to use UNION ALL instead of UNION as UNION scans all data first and then eliminate duplicate so it has slow performance.","text":""},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-9-convert-or-to-and","title":"Tip 9: convert OR to AND","text":""},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-10-subquery-unnesting","title":"Tip 10: Subquery Unnesting","text":""},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-11-in-and-between","title":"Tip 11: IN and BETWEEN","text":""},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-12-fetching-first-n-records-select-from-employee-where-rownum11","title":"Tip 12: Fetching first N records:  SELECT * FROM EMPLOYEE where rownum&lt;11","text":""},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-13-union-vs-union-all","title":"Tip 13: UNION vs UNION ALL:","text":"<p>\u2022 by default, UNION ALL is less costly than UNION, as latter sorts data internally to remove duplicates.  \u2022 But if your table is indexed, then sort operation in UNION wont be that costly, and so you can use UNION also.</p>"},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-14-intersect-vs-exists-operator","title":"Tip 14: INTERSECT Vs EXISTS operator","text":""},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-15-minus-vs-not-exists","title":"Tip 15: MINUS Vs NOT EXISTS","text":""},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-16-using-like-conditions","title":"Tip 16: Using Like conditions","text":"<p>To enable use of indexes, avoid use of wild card character at beginning of source text scan. If you are forced to use wild card character at beginning, you can create reverse index, and handle that problem using that index. In this case, when you reverse index, then source text will eliminate use of wild card at beginning.</p>"},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-17-using-functions-on-indexed-columns-will-suppress-index-usage","title":"Tip 17: Using Functions on Indexed Columns will suppress index usage.","text":"<p>So, rewrite query to avoid use of function. BAD QUERY</p> <pre><code>    select employee_id, first_name, last_name\n    from employees \n    where trunc(hire_date,'YEAR') = '01-JAN-2002';\n</code></pre> <p>GOOD: rewritten query</p> <pre><code>    select employee_id, first_name, last_name\n    from employees \n    where hire_date between '01-JAN-2002' and '31-DEC-2002';\n</code></pre> <p>Eg2 - Bad</p> <pre><code>select * from mytable where substr(emp_name,1,2) = 'Po';\n</code></pre> <p>Good </p> <pre><code>select * from mytable where emp_name like 'Po%';\n</code></pre>"},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-18-handling-null-values","title":"Tip 18: Handling NULL Values","text":"<p>\u2022B-Tree indexes do not index NULL values. \u2022If there are any NULL values in your indexed columns and you need to get rows which have NULL values, optimizer will not use your index, and perform a full table scan instead. \u2022That is, having Null values in your index may sometimes suppress index usage.</p> <p>Solution - - Use IS NOT NULL condition in your WHERE clause.   - Adding not null constraint to your columns and insert a specific value for NULL values like \u20180\u2019 if value in a column is null.  - If reasonable, create a BITMAP index instead of B-Tree index.  </p>"},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-19-use-truncate-instead-of-delete","title":"Tip 19 : Use Truncate instead of Delete","text":""},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-20-data-type-mismatches","title":"Tip 20: Data Type Mismatches","text":"<p>If data types of column and compared value dont match, this may suppress index usage.</p>"},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-21-tuning-ordered-queries-order-by-clause","title":"Tip 21: Tuning Ordered queries- Order By clause","text":"<p>Order by mostly requires sort operations. This sort operation is done in PGA or in disk (if PGA doesn\u2019t have enough memory) This disk is shown as \u2018temporary table space\u2019 in execution plan. Issue \u2013 sorting in disk is a costly operation.</p> <p>Solution \u2013 \u2022 Create a B-Tree index on column used in Order by Clause, or \u2022 Modify a B-Tree index to include column used in Order by Clause. \u2022 Why \u2013 B-Tree indexes store columns in Order and using B-Tree index will eliminate sort operations.</p>"},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-22-retrieving-min-and-max-values","title":"Tip 22 : Retrieving MIN and MAX Values","text":"<p>B-Tree indexes increase the performance a lot for min and max value searches. If no B-Tree index, optimizer will need to read whole table. Bad - </p> <pre><code>    select min(), max() from mytable;\n</code></pre> <p>Good - </p> <pre><code>    select * FROM \n        (select min() from mytable) min_cust, \n        (select max() from mytable) max_cust;\n</code></pre>"},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-23-views","title":"Tip 23 : Views","text":"<p>\u2022 If you don\u2019t need to use all the tables in a view, then don\u2019t use the view. Instead use the actual tables.   \u2022 Don\u2019t join complex views with a table or another view.   \u2022 Avoid performing outer join to the views.   \u2022 Unlike basic and complex views, materialized views store both query and data. Materialized view data can be refreshed manually or via auto-refresh. But materialized view maintenance is a burden to database .</p>"},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-24-frequent-commit-is-not-desired","title":"Tip 24: Frequent commit is not desired:","text":"<ul> <li>make REDO logs bulky as we may be committing prior to period.</li> <li>make lock on modified rows, making them unavailable to other applications.</li> </ul>"},{"location":"Storage%20Layer%20Choices/File%20Compression%20techniques%20in%20Big%20Data/","title":"File Compression Techniques in Big Data Systems","text":""},{"location":"Storage%20Layer%20Choices/File%20Compression%20techniques%20in%20Big%20Data/#file-compression-techniques","title":"File Compression Techniques","text":"<p>These techniques are common to Hadoop ecosystem, not just Hive. </p> <p>Why need compression?  \u2022   Helps reduce storage especially when it comes to data being replicated across various nodes.  \u2022   Helps us process data faster as size of data is less.  \u2022   Since data is compressed, so I/O costs is less \u2013 A major overhead in processing large amounts of data is disk and network I/O, reducing the amount of data that needs to be read and written to disk can significantly decrease overall processing time. This includes compression of source data, but also the intermediate data generated as part of data processing.  </p> <p>Compression and Decompression comes with some cost in terms of time taken to compress and decompress.  But when we compare I/O gains, we can actually ignore this additional time to compress-decompress. </p> <p>Important Compression Techniques \u2013  1.  Snappy  2.  Lzo  3.  Gzip  4.  Bzip2 </p> <p>Some of the compression codecs are optimized for storage \u2013 they bring down size drastically. But this takes time.  Some of compression codecs are optimized for speed \u2013 compression done quickly, but not efficiently. </p> <p>So trade-off is that \u2013   \u2022   if we want more compression ratio, we have to spend more time in compression.  \u2022   If we want faster compression, we spend less time in compression. </p>"},{"location":"Storage%20Layer%20Choices/File%20Compression%20techniques%20in%20Big%20Data/#snappy","title":"Snappy","text":"<p>Snappy is a very fast compression.   However, in terms of compression ratio, it is not that efficient.  But in most production scenarios, snappy is used as it provides a fine balance between speed and compression efficiency.  So, snappy is optimized for speed, not storage. </p>"},{"location":"Storage%20Layer%20Choices/File%20Compression%20techniques%20in%20Big%20Data/#splittablity-in-compression-techniques","title":"Splittablity in compression techniques","text":"<p>Although compression can greatly optimize processing performance, not all compression formats supported on Hadoop are splittable.   Because the MapReduce framework splits data for input to multiple tasks, having a non splittable compression format is an impediment to efficient processing.   If files cannot be split, that means the entire file needs to be passed to a single MapReduce task, eliminating the advantages of parallelism and data locality that Hadoop provides.   For this reason, splitability is a major consideration in choosing a compression format as well as file format. </p> <p>Snappy by default is not splittable \u2013 so if we use non splittable file formats like JSON and XML, snappy won\u2019t give splittable output.  Is this a big concern? No \u2013 because in production scenarios, we hardly use JSON and XML.  In production scenarios, we use container-based formats like Avro, Parquet, Orc \u2013 which are splittable by their structure and no need for compression technique to handle this aspect. </p> <p>So, Snappy is intended to be used with a container format like Avro, Orc, Parquet since it\u2019s not inherently splittable. </p>"},{"location":"Storage%20Layer%20Choices/File%20Compression%20techniques%20in%20Big%20Data/#lzo","title":"Lzo","text":"<p>LZO is similar to Snappy in that it\u2019s optimized for speed as opposed to size.   Unlike Snappy, LZO compressed files are splittable, but this requires an additional indexing step.   This makes LZO a good choice for things like plain-text files (like json, text and xml files) that are not being stored as part of a container format.   It should also be noted that LZO\u2019s license prevents it from being distributed with Hadoop and requires a separate install, unlike Snappy, which can be distributed with Hadoop. </p> <p>But snappy is fastest among all compression techniques. </p>"},{"location":"Storage%20Layer%20Choices/File%20Compression%20techniques%20in%20Big%20Data/#gzip","title":"Gzip","text":"<p>\u2022   Gzip provides very good compression performance (on average, about 2.5times the compression that\u2019d be offered by Snappy).   \u2022   But in terms of processing speed its slow.   \u2022   Gzip is also not splittable, so it should be used with a container format.   \u2022   Note that one reason Gzip is sometimes slower than Snappy for processing is that Gzip compressed files take up fewer blocks, so fewer tasks are required for processing the same data. For this reason, using smaller blocks with Gzip can lead to better performance.  \u2022   Eg \u2013  o   1 gb file \u2013 split into 8 blocks.  o   After gzip compression, we get 200 mb file \u2013 2 block \u2013 so number of blocks coming down, which reduces parallelism.  o   Solution \u2013 reduce block size to say, 50Mb, leading to 200mb file split into 4 blocks, and so parallelism doubles. </p>"},{"location":"Storage%20Layer%20Choices/File%20Compression%20techniques%20in%20Big%20Data/#bzip2","title":"Bzip2","text":"<p>\u2022   Bzip2 provides excellent compression performance, but can be significantly slower than other compression codecs such as Snappy in terms of processing performance.   \u2022   Unlike Snappy and Gzip, bzip2 is inherently splittable.   \u2022   In the examples we have seen, bzip2 will normally compress around 9% better than GZip, in terms of storage space.   \u2022   However, this extra compression comes with a significant read/write performance cost. This performance difference will vary with different machines, but in general bzip2 is about 10 times slower than GZip.   \u2022   For this reason, it\u2019s not an ideal codec for Hadoop storage, unless your primary need is reducing the storage footprint. One example of such a use case would be using Hadoop mainly for active archival purposes. </p> Codec Splittable compression speed snappy N low Highest lzo Y low high gzip N High slow bzip2 Y Highest Slowest"},{"location":"Storage%20Layer%20Choices/Text%20File%20Formats/","title":"Text-based File Formats","text":""},{"location":"Storage%20Layer%20Choices/Text%20File%20Formats/#text-file-format","title":"Text File format","text":"<p>Not used in production.  Types include \u2013  \u2022   Csv \u2013 raw files  \u2022   Xml, Json \u2013 structured text data - These are files with some structure attached to them </p> <p>These are human readable. </p> <p>CSV file  \u2022   Advantage \u2013  o   Data is stored in a human readable way  \u2022   Drawbacks \u2013  o   Everything is stored in a text form \u2013 so even an integer is stored as a text.  Issue \u2013 more bytes used to store an integer than needed as it is stored as a string.  Eg \u2013 val = 4561987.  If val = INT \u2013 then takes only 4 bytes to get stored  If val = STRING \u2013 then each position takes 2 bytes, so total 14 bytes used. </p> <p>To conclude \u2013 text file takes a lot of storage since everything is stored as string.  o   When data is stored as string, and you want to read it and use it as integer, this type of conversion is time consuming.   So, processing on text files can be very slow.  o   Further, I/O operations also take a lot of time \u2013 coz text file take lot of space as compared to other file formats, and transferring such data across the network will be I/O intensive process. </p> <p>XML, JSON files  Disadvantage \u2013  \u2022   Same as CSV \u2013  o   Storage space is large  o   Processing = slow  o   I/O = huge  \u2022   Not splittable \u2013 JSON and XML have schema attached to them, due to which they are not splittable. This defeats the whole purpose of using Hadoop as not being splittable means no parallelism possible. </p> <p>To summarize, in production we don\u2019t use text file formats. </p>"},{"location":"Storage%20Layer%20Choices/big%20data%20file%20formats/","title":"Big Data File Formats","text":""},{"location":"Storage%20Layer%20Choices/big%20data%20file%20formats/#avro-orc-and-parquet","title":"Avro, ORC and Parquet","text":"<p>There major ones that work well with Big Data Environment are \u2013  \u2022   Avro  \u2022   Orc  \u2022   Parquet </p> <p>All of above are \u2013  \u2022   Splittable \u2022   Agnostic compression -  Any compression can be used with them, without readers having to know the codec.  This is possible because codec is stored in the header metadata of the file format.  Reader needn\u2019t know in advance what kind of compression technique is used with these files.  Compression codec is kept in file metadata, and whenever reader wants to read the data, he gets to know compression codec from metadata and can easily read the data. </p> <p>Avro File Formats  1.  It is a row-based file format \u2013 data is stored row-by-row.  So, it supports faster writes, but slower reads (when you want to read a subset of columns).  2.  Self-describing schema - Schema is stored in JSON format and this metadata is embedded as part of data itself.  3.  Actual data is stored in Compressed Binary format, which is quite efficient in terms of storage.  4.  Language Neutral - Avro file format is general file format, and supports processing using lot of programming languages like C++, java, Python, Ruby, etc  5.  Schema Evolution \u2013 Avro is quite mature in terms of schema evolution as compared to other file formats. Schema evolution includes aspects like \u2013  a.  Adding new columns  b.  Removing old column  c.  Renaming columns, etc  6.  Splittable \u2013 file can be divided into parts which can be processed independently. </p> <p>Avro is a Serialization format.  Serialization \u2013 converting data into a form which can be easily transferred over a network and stored in a file system.  Deserialization \u2013 reading data and converting it into form which can be read by human.  </p> <p>In which scenario, Avro is best suited \u2013  \u2022   For storing data in landing zone of data lake \u2013 why \u2013  o   In lake, chances are different team requiring raw data, and since Avro is language neutral, so different teams can use it.  o   In lake, data is unprocessed, that is no ETL done. For ETL kind of operations, we tend to read whole row and not a subset of it, and here again Avro is suited.      KL \u2013 in dh, we read only subset of data in lake to build warehouse. So, this point is debatable.   o   Finally, in lake, data schema evolves over time, and so Avro is suited for handling Schema evolution.  \u2022   Avro is typically the format of choice for write-heavy workloads given it is easy to append new rows. </p> <p>Avro Vs Orc </p> Category Avro Parquet format row based column based reads slower reads faster reads - so useful for analytical querying writes faster writes -  so suited to ETL operations. slower writes schema evolution it is quite mature wrt Schema Evolution Limited schema evolution support - you can add/delete column from the end. complex data types No such support. Provides support for deeply nested data structure. <p>Orc Vs Parquet</p> Category Orc Parquet format column based column based predicate push down provides predicate push down to push the predicates at storage level - that allows us to query relevant data. No such support ACID properties Now supports ACID properties to an extent. No such support. Compression Better - has lot more encodings used as compared to parquet. good complex data types No such support. Provides support for deeply nested data structure. <p></p>"},{"location":"Storage%20Layer%20Choices/row%20based%20and%20column%20based%20file%20formats/","title":"Row-based Vs Column-based File Formats","text":""},{"location":"Storage%20Layer%20Choices/row%20based%20and%20column%20based%20file%20formats/#row-vs-column-file-format","title":"Row Vs Column File Format","text":"<p>When you are designing big data solution, one fundamental que is \u2013 \u201chow data will be stored\u201d  It involves taking into consideration 2 things \u2013  \u2022   File formats  \u2022   Compression techniques </p> <p>Why do we need different File Formats?  \u2022   To save storage  \u2022   To do fast processing   \u2022   To have less time in I/O operations \u2013 since we are dealing in big data, I/O operations are a big bottleneck, and so spending as less time in this area as possible. </p> <p>Our file formats help us in all 3 above if we go with right file format. </p> <p>There are a lot of choices available on file formats.   Below are key aspects for deciding a file format -   1.  Faster reads.  2.  Faster writes.  3.  Splittable - Some are designed in such a way that they are splittable \u2013 if a file is splittable, we can do parallel processing \u2013 in big data solutions, we consider only splittable file formats.  4.  Schema evolution support \u2013 some of the file formats support schema evolution -  i.e. to facilitate change in input data by allowing schema changes.  5.  Advanced compression techniques  6.  Most compatible platform - some work well with hive, some with spark, etc </p> <p>All the file formats have been divided into 2 broad categories \u2013  \u2022   Row based  \u2022   Column based </p> <p>Row based \u2013  Here data is stored row-by-row.  </p> <p>At a time, whole record is saved.  If a new record comes, it gets appended at the end.  So, writing a record is very easy coz you simply append at the end.</p> <p>Now let\u2019s talk about reading \u2013   While write is easy, but in order to get subset of columns, it has to read entire record. That is, performance is degraded when it comes to read.  In data warehousing, wherein we scan specific set of records, row-based formats is not suggested. </p> <p>Regarding Compression of row-based file\u2013  Since data is stored record by record, so different data types are present together, next to each other.  That means, compression is not as efficient as it could be. </p> <p>Column based file format \u2013  All column values are stored together.  </p> <p>For Reading data in column-based file format \u2013 it allows us to skip data and read only relevant columns.  Column based file format is suggested for data warehouse based query system. </p> <p>For write on column-based file, it is time consuming as you need to write on multiple places. </p> <p>Regarding Compression of column-based file\u2013  Since data is stored column by column, so same data types are present together, next to each other.  That means, compression can be applied efficiently for each data type.  </p> <p>To summarize \u2013  If you write once but read multiple times, go for column-based file format.  If you read once, but write multiple times, go for row-based file format. </p> Category row based column based read slower reads faster reads writes faster writes slower writes compression poor good"},{"location":"random/Amazon-Leadership-Principles-Jeff-Bezos-thought-process/","title":"Jeff Bezos on Amazon Leadership principles","text":""},{"location":"random/Amazon-Leadership-Principles-Jeff-Bezos-thought-process/#leaders-are-right-a-lot","title":"Leaders are right, a lot","text":"<p>Such people who are right a lot are hvaing following characteristics-&gt;  a. they listen a lot.   b. they change their mind a lot.  Why -&gt;  Life is complicated rules are complicated and so when you get more data, you change your mind.  Sometimes you don't get data but just re-analyze a situation and change your mind.  Anybody who doesn't change mind a lot underestimates the complexity of world we live in. </p> <p>c. they seek to disconfirm their most profoundly held convictions. </p>"},{"location":"random/Amazon-Leadership-Principles-Jeff-Bezos-thought-process/#frugality","title":"Frugality","text":"<p>Having small team size makes sense - so that essence of human organization - team work and cohesion comes up.  If team size is large, leader should make more sub-leaders so that communication and performance can be effective. </p>"},{"location":"random/Amazon-Leadership-Principles-Jeff-Bezos-thought-process/#hire-smart-people","title":"Hire smart people","text":"<p>No single individual person can keep in touch with plethora of new things that are happening.  So you need a recruitment process to hire and retain smart talented hard-working people who want to be part of your mission. </p> <p>What is Amazon's Mission : be the world's most customer-centric company  Its a goal larger than Amazon.  This involves 3 things -&gt; listen, invent, personalize.  - Companies that don't listen to customer cannot survive.  - Companies that only listen to customer fail. They need to invent and find novice solutions to customer's problems.      It is not customer job to invent, it is company's job and if they fail to do so, they cannot survive.  - See each and every customer as centre of their own universe ~ personalize. </p>"},{"location":"random/Amazon-Leadership-Principles-Jeff-Bezos-thought-process/#on-worklife-balance","title":"On work=life balance.","text":"<p>We should use then term work-life harmony.  If you are happy at work, you come home as a better husband, father and person.  And if you have harmony at home, you come to office as better employee.  Organizations must strive to ensure harmony at workspace. </p>"},{"location":"random/Amazon-Leadership-Principles-Jeff-Bezos-thought-process/#learn-be-curious-bias-for-actions","title":"Learn &amp; be curious, bias for actions","text":"<p>For most people harmony at work means finding work which is meaningful, interesting and impactful.  Because of kind of challenges we have chosen for ourselves, we get to work for future and its super fun to work for the future.  We work in environment where there is constant change. This is how high-tech domain is.  If one doesnt enjoy this environment, they wont be able to sustain in long term. </p> <p>We need to build people, and it is human nature to focus on things that aren't working, and its incredibly hard to get people to take bold bets.  That is something we need to  encourage because bets are prone to failure, but those that succeed, they can compensate for dozens of things that didn't work.  At amazon, it took some bold bets that worked out really well- Kindle, AWS, Prime, etc. </p> <p>Companies that don't embrace failure, they eventually get into position of desperation. </p>"},{"location":"random/Amazon-Leadership-Principles-Jeff-Bezos-thought-process/#have-backbone-disagree-and-commit-dive-deep-be-skeptical-when-metrics-and-anecdote-differ","title":"Have backbone , disagree and commit | dive deep ; be skeptical when metrics and anecdote differ","text":"<p>Generally assumed that companies need to eventually raise prices in order to increase profit margins.  Amazon wants to lower price further because it sees things in terms of scale.   Unlike physical stores wherein good service and low prices both aren't available, online stores can have both best service and lowest prices.  Need to increase profit not in terms of % margins but in terms of dollar margins, i.e. sell more product - when you lower price, you attract more people and more sales - so more profit. </p> <p>Credits : Link </p>"},{"location":"random/ELK%20Stack/","title":"ELK","text":""},{"location":"random/ELK%20Stack/#what-is-elk-stack-and-why-is-it-so-popular-for-log-management","title":"What is ELK Stack and why is it so popular for log management?","text":"<p>The ELK Stack is composed of three open-source products. </p> <p>ELK stands for Elasticsearch, Logstash, and Kibana. </p> <p>\ud83d\udd39 Elasticsearch is a full-text search and analysis engine, leveraging Apache Lucene search engine as its core component. </p> <p>\ud83d\udd39 Logstash collects data from all kinds of edge collectors, then transforms that data and sends it to various destinations for further processing or visualization. </p> <p>In order to scale the edge data ingestion, a new product Beats is later developed as lightweight agents installed on edge hosts to collect and ship logs to Logstash. </p> <p>\ud83d\udd39 Kibana is a visualization layer with which users analyze and visualize the data. </p> <p>The diagram below shows how ELK Stack works:  </p> <p>Step 1 - Beats collects data from various data sources. For example, Filebeat and Winlogbeat work with logs, and Packetbeat works with network traffic. </p> <p>Step 2 - Beats sends data to Logstash for aggregation and transformation. If we work with massive data, we can add a message queue (Kafka) to decouple the data producers and consumers. </p> <p>Step 3 - Logstash writes data into Elasticsearch for data indexing and storage. </p> <p>Step 4 - Kibana builds on top of Elasticsearch and provides users with various search tools and dashboards with which to visualize the data. </p> <p>ELK Stack is pretty convenient for troubleshooting and monitoring. It became popular by providing a simple and robust suite in the log analytics space, for a reasonable price. </p> <p>Credits : Alex Wu</p>"},{"location":"random/ETL%20VS%20ELT/","title":"Database Vs Datawarehouse Vs Datalake | ETL Vs ELT","text":""},{"location":"random/ETL%20VS%20ELT/#database","title":"Database","text":"<p>\u2022   Meant for Transactional data - OLTP (online transaction processing)  \u2022   Such data should be Structured data  \u2022   Meant for recent data - day to day data. Example - online banking transaction.  \u2022   Common databases are Oracle, Mysql  \u2022   It is Schema on Write \u2013 i.e. while writing any data into database, its data type and table structure gets validated, and if any mismatch occurs, it raises exception.  \u2022   the cost to store the data in database is high. </p>"},{"location":"random/ETL%20VS%20ELT/#datawarehouse-dwh","title":"Datawarehouse \u2013 DWH","text":"<p>\u2022   Its purpose is Analytical processing, wherein we require a lot of historical data to find the insights.   \u2022   Querying in data warehouse involves writing complex queries that scans across history.   \u2022   Why not use database for this purpose?   o   The moment we run complex queries on our database with an intent to do some analysis then your day to day transaction will become slow.  \u2022   We take the data from databases and migrate it to Datawarehouse to do analytical processing.  \u2022   we get the data from multiple sources.  \u2022   Meant for Structured Data.  \u2022   It is also Schema on write.      Eg \u2013 Teradata  \u2022   storage cost is high but lesser than your database.  \u2022   In case of data warehouse, its an ETL process \u2013 i.e. data is extracted, then transformed, and finally Loaded into database.  -   Issue \u2013 we lose flexibility in terms of managing data.  -   This is because even before writing, we are to decide how to store it. We cant always look into future and plan use case of our data.   Suppose your data is in database, wherein you -&gt;   - extract the data  - Transform it (is a complex process)  - Load it to Datawarehouse  This approach reduces our flexibility. </p>"},{"location":"random/ETL%20VS%20ELT/#data-lake","title":"Data Lake","text":"<p>\u2022   Its aim, like data warehouse, is to get insights from huge amount of data.  \u2022   Here the data is present in its raw form.  \u2022   It can be structured or unstructured.  \u2022   Eg - Log File - we can directly have this file in raw form in data lake.  \u2022   Its an ELT process - Extract Load &amp; Transform.  Eg - HDFS, Amazon S3  \u2022   Benefit \u2013   o   Cost effective \u2013 cheapest storage  o   Schema on Read.  o   create structure to visualize or see the data.  o   Since data stored in raw form, it gives you enough flexibility. </p>"},{"location":"random/LinkedIn%20growth/","title":"LinkedIn Growth","text":""},{"location":"random/LinkedIn%20growth/#tip-for-beginners","title":"Tip for Beginners","text":"<p>If you are just starting on LinkedIn:</p> <ul> <li>Decide what topic you want to be known for</li> <li>Post content 1x per day on that specific topic</li> <li>Comment under large accounts 3-5x per day</li> <li>At 6 months go 2x per day w/ repurposing</li> <li>Start a newsletter at 15,000 followers</li> </ul> <p>Follow this for 12 months, and you'll have an audience of raving fans. </p> <p>An incredible asset in 2022. </p> <p>Credits : Justin Welsh </p>"},{"location":"random/Top-10-hacks-to-be-a-Bad-Developer/","title":"Top 10 hacks to be a Bad developer","text":""},{"location":"random/Top-10-hacks-to-be-a-Bad-Developer/#top-10-hacks-to-be-a-bad-developer","title":"Top 10 hacks to be a BAD developer","text":"<ol> <li>Code first, design later.  </li> <li>If people can't understand your code, they are not smart enough  </li> <li>Never write unit tests and when you are forced to write them, write short and easy ones  </li> <li>The only job of test coverage is to please engineering managers  </li> <li>Take all suggestions in code reviews personally  </li> <li>Always avoid documentation. Only work on it long after your project is complete.  </li> <li>Logging, Metrics &amp; Alerts are just there to add more story points. Your code is always reliable.  </li> <li>Just get things to work once and push them to production quickly. Never actively monitor your changes in production.  </li> <li>Don't read software documentation, only rely on internet blogs  </li> <li>Never think about the end user, you are an engineer not a product freak  </li> </ol> <p>Of course, above is meant for what you should not do at you job. </p> <p>Credits : Shivam Gupta</p>"},{"location":"random/Work-Time-Motion-Study/","title":"Work Time Motion Study","text":"<p>Fredrick Taylor, often referred to as God who saved Capitalism from the clutches of Cummunism in 1920s, made a Scientific Study of doing work in industries. One of the key recommendation is with respect to Time-Work-Motion Study. He wanted answer to the question - How to increase Profits of organizations?</p> <p>Till that time, the mindset was that profits are directly proportional to number of hours spent by employee in factory, i.e. more he worked, more the organizations made items, and more the profts. This led to miserable working conditions in factories, wherein prolonged working hours drained the workers physically, emotionally and mentally.</p> <p>Taylor took a different view to address this problem. He wanted to increae profits, but not by making workers work for longer periods of time, but by working more efficiently. One fo the principles he gave, and is our discussion item today, is Work-Time-Motion Study.</p>"},{"location":"random/cold_mails/","title":"Cold Mails Tactics","text":"<p>\ud83d\udfe2 Run the triple: Start every sequence with an email, social touch, and cold call.</p> <p>\ud83d\udfe2 5x5x5: 5 minutes of research. Find 5 insights on the person/account. Write your message in 5 minutes.</p> <p>\ud83d\udfe2 Subject lines shouldn\u2019t read like an advertisement. They should be 4 words max, not a complete sentence, and without punctuation. </p> <p>\ud83d\udfe2 Don't equate formality with professionalism. You don't need to address me as \"Dear Mr. Farrokh\"). Use conjunctions and conversational language</p> <p>\ud83d\udfe2 Follow the 3x3 Rule for email: 3 paragraphs, no more than 3 lines each when read on your phone. </p> <p>\ud83d\udfe2 Don't try to sell everything under the sun in your email. One pain point per email, or you risk losing them. Mix in multiple pain points across the sequence.</p> <p>\ud83d\udfe2 Avoid large images, multiple hyperlinks, and big attachments. These add-ons scream \"marketing email\" and hurt deliverability.</p> <p>\ud83d\udfe2 Use Interest-based CTAs. \"Open to learning more?\" is better than \u201cHow's Thursday at 4:00 for a discovery call?\u201d</p> <p>\ud83d\udfe2 Your emails should read like text messages. If you read it out loud and sound like a robot, you\u2019re doing it wrong.</p> <p>\ud83d\udfe2 Tie VMs to emails: Leave voicemails that reference your emails and vice-versa. Even if they never get answered, voicemails should boost email replies. </p> <p>Credits : Armand Farokh</p>"},{"location":"random/pytest/","title":"PyTest overview","text":"<ul> <li>PyTest is a python unit testing framework. </li> <li>It provides the ability to create Tests, Test Modules, Test Classes, and Test Fixtures. </li> <li>It uses the built-in python assert statement which makes implementing unit tests much simpler than other frameworks. </li> <li>It also adds many useful command line arguments to help specify what tests should be run and in what order. </li> </ul>"},{"location":"random/pytest/#creating-a-test","title":"Creating a Test","text":"<ul> <li>Tests are python functions with \u201ctest\u201d at the beginning of the function name. </li> <li>Tests do verification of values using the standard python assert statement. </li> <li>Similar tests can be grouped together by including them in the same module or class </li> </ul> <pre><code># test_SomeFunction.py\ndef test_SomeFunction():\n assert 1 == 1\n\n</code></pre>"},{"location":"random/pytest/#test-discovery","title":"Test Discovery","text":"<ul> <li>Pytest will automatically find your tests when you run it from the command line using several naming rules for the test files, test classes, and test functions. </li> <li>Test function names should begin with \u201ctest\u201d. </li> <li>Classes with tests in them should have the word \u201cTest\u201d with a capital T at the beginning of the class name. These classes should also have no \u201cinit\u201d method. </li> <li>The filenames for test modules should start with \u201ctest_\u201d or end with \u201c_test\u201d. </li> </ul>"},{"location":"random/pytest/#xunit-style-setup-and-tear-down","title":"XUNIT Style setup and tear down","text":"<ul> <li>One key feature of all unit test frameworks is providing the ability to execute setup code before and after the tests. Pytest provides this capability with both XUnit style setup/teardown functions and with Pytest fixtures.</li> <li>The XUnit style setup and teardown functions allow you to execute code before and after: Test modules , Test Functions , Test Classes , and Test Methods in Test Classes. <li>Using these setup and teardown functions can help reduce code duplication by letting you specify setup and teardown code once at each of the different levels as necessary rather than repeating the code in each individual unit test. This can help keep your code clean and manageable.</li> <li>Lets look at some examples to see how this works.</li> <pre><code>def setup_module():\ndef teardown_module():\ndef setup_function():\ndef teardown_function():\ndef setup_class():\ndef teardown_class():\ndef setup_method():\ndef teardown_method():\n</code></pre> <pre><code>To run below code use command : pytest -v -s \nclass TestClass:\n\n    @classmethod\n    def setup_class(cls):\n        print('\\n setup up TestClass')\n\n    @classmethod\n    def teardown_class(cls):\n        print('\\n tear down TestClass')\n\n\n    def setup_method(self, method):\n        if method==self.test1:\n            print ('\\n setting up test1')\n        elif method==self.test2:\n            print ('\\n setting up test2')\n        else:\n            print('\\n setting up unkown test')\n\n    def teardown_method(self, method):\n        if method==self.test1:\n            print ('\\n tear down  test1')\n        elif method==self.test2:\n            print ('\\n tear down test2')\n        else:\n            print('\\n tear down unkown test')\n\n    def test1(self):\n        print ('\\n Executing Test1')\n        assert True\n\n    def test2(self):\n        print ('\\n Executing Test2')\n        assert True\n</code></pre>"},{"location":"random/pytest/#pytest-fixtures","title":"PyTest Fixtures","text":"<ul> <li>Like the XUnit style of setup and teardown functions, Test fixtures allow for re-use of code across tests by specifying functions that should be executed before the unit test runs.</li> <li>Specifying that a function is a test fixture is done by applying the \u201cpytest.fixture\u201d decorator to the function.</li> <li>Individual unit tests can specify they want to use that function by specifying it in their parameter list or by using the \u201cpytest.mark.usefixture\u201d decorator.</li> <li>The fixture can also set its autouse parameter to true which will cause all tests in the fixture\u2019s scope to automatically execute the fixture before the test executes.</li> <li>Lets look at an example.</li> </ul> <pre><code>@pytest.fixture():\ndef math():\n    return Math()\ndef test_Add(math):\n    assert math.add(1,1) == 2 \n</code></pre> <pre><code>import pytest\n\n@pytest.fixture()\ndef setup():\n    print ('\\n Setup')\n\n# below setup fixture gets called before test1 executes\n# This is because we passed setup as parameter to out test1() function\ndef test1(setup):\n    print ('Executing test1')\n    assert True\n\n# below setup fixture doesn't gets called before test2 executes\ndef test2():\n    print ('Executing test2')\n    assert True\n\n# below setup fixture gets called before test1 executes\n# This is because we passed setup via decorator using below syntax.\n@pytest.mark.usefixtures(\"setup\")\ndef test3():\n    print ('Executing test3')\n    assert True\n\n</code></pre> <p>Output </p> <pre><code>test_fixture_example1.py::test1 \n Setup\nExecuting test1\nPASSED\ntest_fixture_example1.py::test2 Executing test2\nPASSED\ntest_fixture_example1.py::test3 \n Setup\nExecuting test3\nPASSED\n</code></pre> <pre><code>Note - It would be cumbersome to pass fixture to each test manually.\nInstead, we use autouse functionality so that fixture gets called before each test.\nEg - \n    @pytest.fixture(autouse=True)\n    def setup():\n        print ('\\n Setup')\n</code></pre>"},{"location":"random/pytest/#test-fixture-teardown","title":"Test Fixture Teardown","text":"<ul> <li>Often there is some type of teardown or cleanup that a test, class, or module need to perform after testing has been completed.</li> <li>Each test fixture can specify their own teardown code that should be executed.</li> <li>There are two methods of specifying a teardown code for a test fixture: </li> <li>The \u201cyield\u201d keyword and </li> <li>The request-context object\u2019s \u201caddfinalizer\u201d method. </li> </ul> <p>Test Fixture Teardown - Yield - The yield keyword is the simpler of the two options for teardown code. - The code after the yield is executed after the fixture goes out of scope. - The yield keyword is a replacement for return and any return values should be passed to it.</p> <pre><code>@pytest.fixture():\ndef setup():\n     print(\u201cSetup!\u201d)\n     yield\n     print(\u201cTeardown!\u201d) \n</code></pre> <p> Test Fixture Teardown - addfinalizer  - The addfinalizer method of adding teardown code is a little more complicated but also a little more capable than the yield statement. - With the addfinalizer method one or more finalizer functions are added via the request-context\u2019s addfinalizer method. - One of the big differences between this method and the yield keyword method is that this method allows for multiple finalization functions to be specified. - Now lets take a look at some examples.</p> <pre><code>@pytest.fixture():\ndef setup(request):\n     print(\u201cSetup!\u201d)\n     def teardown:\n     print(\u201cTeardown!\u201d)\n     request.addfinalizer(teardown) \n</code></pre> <p>Example of 2 teardown methods </p> <pre><code>import pytest\n\n@pytest.fixture()\ndef setup1():\n    print ('\\n Setup 1')\n    yield\n    print ('\\n Teardown 1')\n\n\n@pytest.fixture()\ndef setup2(request):\n    print ('\\n Setup 2')\n\n    def teardown_2_a():\n        print ('\\n Teardown 2A called')\n\n    def teardown_2_b():\n        print ('\\n Teardown 2B called')\n\n    request.addfinalizer(teardown_2_a)\n    request.addfinalizer(teardown_2_b)\n\n@pytest.mark.usefixtures(\"setup1\")\ndef test1():\n    print ('Executing test1')\n    assert True\n\n@pytest.mark.usefixtures(\"setup2\")\ndef test2():\n    print ('Executing test2')\n    assert True\n\n</code></pre> <p>Output </p> <pre><code>test_fixture_example2.py::test1 \n Setup 1\nExecuting test1\nPASSED\n Teardown 1\n\ntest_fixture_example2.py::test2 \n Setup 2\nExecuting test2\nPASSED\n Teardown 2B called\n\n Teardown 2A called\n</code></pre>"},{"location":"random/pytest/#test-fixtures-scope","title":"Test Fixtures Scope","text":"<ul> <li>Which tests a test fixture applies to and how often it is run depends on the fixture\u2019s scope.</li> <li>Test fixtures have four different scopes:</li> <li>Function - Run the fixture once for each test</li> <li>Class - Run the fixture once for each class of tests</li> <li>Module - Run once when the module goes in scope</li> <li>Session - The fixture is run when pytest starts.</li> <li>By default, the scope is set to function and this specifies that the fixture should be called for all tests in the module.</li> <li>Class scope specifies the test fixture should be executed once per test class.</li> <li>Module scope specifies that the fixture should be executed once per module.</li> <li>Session scope specifies that the fixture should be executed once when PyTest starts.</li> </ul> <p>Eg - code 1 : test_fixture_example3_scope_of_fixtures.py </p> <pre><code>import pytest\n\n@pytest.fixture(scope='session', autouse=True)\ndef setupSession():\n    print (\"\\n Setup Session\")\n\n    yield\n    print(\"\\n Teardown Session\")\n\n\n@pytest.fixture(scope='module', autouse=True)\ndef setupModule():\n    print (\"\\n Setup Module\")\n\n    yield\n    print(\"\\n Teardown Module\")\n\n\n@pytest.fixture(scope='function', autouse=True)\ndef setupFunction():\n    print (\"\\n Setup Function\")\n\n    yield\n    print(\"\\n Teardown Function\")\n\ndef test1():\n    print ('Executing Test 1')\n    assert True\n\ndef test2():\n    print ('Executing Test 2')\n    assert True\n</code></pre> <p>Eg - code 2 : test_fixture_example4_scope_of_fixtures.py </p> <pre><code>import pytest\n\n@pytest.fixture(scope='module', autouse=True)\ndef setupModule():\n    print (\"\\n Setup Module2\")\n\n    yield\n    print(\"\\n Teardown Module2\")\n\n\n@pytest.fixture(scope='class', autouse=True)\ndef setupClass():\n    print (\"\\n Setup Class2\")\n\n    yield\n    print(\"\\n Teardown Class2\")\n\n@pytest.fixture(scope='function', autouse=True)\ndef setupFunction():\n    print (\"\\n Setup Function2\")\n\n    yield\n    print(\"\\n Teardown Function2\")\n\nclass TestClass:\n    def test1(self):\n        print ('Executing Test 1')\n        assert True\n\n    def test2(self):\n        print ('Executing Test 2')\n        assert True\n</code></pre> <p>When both are together run for pytest, the output is -&gt;  </p> <pre><code>test_fixture_example3_scope_of_fixtures.py::test1 \n Setup Session\n\n Setup Module\n\n Setup Function\nExecuting Test 1\nPASSED\n Teardown Function\n\ntest_fixture_example3_scope_of_fixtures.py::test2 \n Setup Function\nExecuting Test 2\nPASSED\n Teardown Function\n\n Teardown Module\n\ntest_fixture_example4_scope_of_fixtures.py::TestClass::test1 \n Setup Module2\n\n Setup Class2\n\n Setup Function2\nExecuting Test 1\nPASSED\n Teardown Function2\n\ntest_fixture_example4_scope_of_fixtures.py::TestClass::test2 \n Setup Function2\nExecuting Test 2\nPASSED\n Teardown Function2\n\n Teardown Class2\n\n Teardown Module2\n\n Teardown Session\n</code></pre>"},{"location":"random/pytest/#test-fixture-return-objects-and-params","title":"Test Fixture Return Objects and Params","text":"<ul> <li>PyTest Test Fixtures allow you to optionally return data from the fixture that can be used in the test.</li> <li>The optional params array argument in the fixture decorator can be used to specify one or more values that should be passed to the test.</li> <li>When a params argument has multiple values then the test will be called once with each value.</li> <li>Let's look at a working example. </li> </ul> <pre><code>import pytest\n\n@pytest.fixture(params=[1,2,3])\ndef setup(request):\n    return_val = request.param\n    print ('\\n Setup return_val = {}'.format(return_val))\n\n    return return_val\n\n# Note: we didn't use  @pytest.mark.usefixtures(\"setup\") here as we needed to access value of setup fixture.\ndef test1(setup):\n    print ('Executing test1')\n    print ('Setup return value inside test function = ',format(setup))\n    assert True\n</code></pre> <p>Output </p> <pre><code>test_fixture_example5_return_objects_and_params.py::test1[1] \n Setup return_val = 1\nExecuting test1\nSetup return value inside test function =  1\nPASSED\ntest_fixture_example5_return_objects_and_params.py::test1[2] \n Setup return_val = 2\nExecuting test1\nSetup return value inside test function =  2\nPASSED\ntest_fixture_example5_return_objects_and_params.py::test1[3] \n Setup return_val = 3\nExecuting test1\nSetup return value inside test function =  3\nPASSED\n</code></pre>"},{"location":"random/pytest/#using-assert-and-testing-exceptions","title":"Using Assert and Testing Exceptions","text":""},{"location":"random/pytest/#using-the-assert-statement","title":"Using the assert Statement","text":"<ul> <li>Pytest allows the use of the built in python assert statement for performing verifications in a unit test.</li> <li>The normal comparison operators can be used on all python data types: &lt;, &gt;,&lt;=, &gt;=, ==, and !=</li> <li>Pytest expands on the messages that are reported for assert failures to provide more context in the test results.</li> </ul> <pre><code>import pytest\ndef test_IntAssert():\n  assert 1 == 1\n\ndef test_StrAssert():\n  assert \"str\" == \"str\"\n\ndef test_floatAssert():\n  assert 1.0 == 1.0\n\ndef test_arrayAssert():\n  assert [1,2,3] == [1,2,3]\n\ndef test_dictAssert():\n  assert {\"1\":1} == {\"1\":1}\n</code></pre> <p>Output </p> <pre><code>collected 6 items                                                                                                                                                                                                      \n\nsrc/test/test_assert_example.py::test_IntAssert PASSED\nsrc/test/test_assert_example.py::test_StrAssert PASSED\nsrc/test/test_assert_example.py::test_floatAssert PASSED\nsrc/test/test_assert_example.py::test_arrayAssert PASSED\nsrc/test/test_assert_example.py::test_dictAssert PASSED\nsrc/test/test_exception_example.py::test_exception PASSED\n</code></pre> <p>Comparing Floating Point Values  - Validating floating point values can sometimes be difficult as internally the value is stored as a series of binary fractions. - Because of this some comparisons that we\u2019d expect to pass will fail. - Pytest provides the \u201capprox\u201d function which will validate that two floating point values are \u201capproximately\u201d the same value as each other to within a default tolerance of 1 time E to the -6 value.</p> <pre><code>from pytest import approx\n\n\n# Failing Test\ndef test_BadFloatCompare():\n   assert (0.1 + 0.2) == 0.3\n\n\n# Passing Test\ndef test_GoodFloatCompare():\n   val = 0.1 + 0.2\n   assert val == approx(0.3) \n</code></pre>"},{"location":"random/pytest/#verifying-exceptions","title":"Verifying Exceptions","text":"<ul> <li>In some test cases we need to verify that a function raises an exception under certain conditions.</li> <li>Pytest provides the raises helper to perform this verification using the \u201cwith\u201d keyword.</li> <li>When the \u201craises\u201d helper is used the unit test will fail if the specified exception is not thrown in the code block after the \u201craises line.</li> </ul> <pre><code>from pytest import raises\n\ndef raisesValueException():\n    raise ValueError\n\ndef test_exception():\n    with raises(ValueError):\n        raisesValueException()\n</code></pre>"},{"location":"random/pytest/#pytest-command-line-arguments","title":"PyTest Command Line Arguments","text":""},{"location":"random/pytest/#specifying-what-tests-should-run","title":"Specifying What Tests Should Run","text":"<ul> <li>By default, Pytest runs all tests that it finds in the current working directory and sub-directory using the naming conventions for automatic test discovery.</li> <li>There are several pytest command line arguments that can be specified to try and be more selective about which tests will be executed.</li> <li>moduleName - You can simply pass in the module name to execute only the unit tests in one particular module.</li> <li>DirectoryName - You can also simply pass in a directory path to have pytest run only the tests in that directory.</li> <li>You can use the \u201c-k\u201d option to specify an evaluation string based on keywords such as: module name, class name, and function name.</li> <li>You can use the \u201c-m\u201d option to specify that any tests that have a \u201cpytest.mark\u201d decorator that matches the specified expression string will be executed.</li> </ul> <p>Examples </p> <pre><code># 1 : DirectoryName\npytest -v -s path_to_my_directory/\n\n#2 : -k option : runs all test having keyword test1 inside them.\npytest -v -s -k \"test1\"\n\n#3 : -k option : runs all test having keyword test1 or test2 inside them.\npytest -v -s -k \"test1 or test2\"\n\n\n#4 : -m option\ndefine your tests by adding pytest.mark as shown below-&gt;\n\n@pytest.mark.test1\ndef test1():\n    print ('executing test1')\n    assert True\n\n@pytest.mark.test2\ndef test2():\n    print ('executing test2')\n    assert True\n\npytest -v -s -m \"test1 or test2\"         \n</code></pre> <p>Some additional command line arguments that can be very useful.  - -v option specifies that verbose output from pytest should be enabled. - -q option specifies the opposite. It specifies that the tests should be run quietly (or minimal output). This can be helpful from a performance perspective when you\u2019re running 100\u2019s or 1000\u2019s of tests. - -s option specifies that PyTest should NOT capture the console output. - \u2014ignore option allows you to specify a path that should be ignore during test discovery. - \u2014maxfail option specifies that PyTest should stop after N number of test failures.</p>"}]}