{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Data Engineering - 101 Index SQL Performance Tuning: SQL Performance Tuning Page SQL Performance Tuning Summary Concepts Data Warehousing Concepts Career Development Cold Mail Tactics About Me Author : Kushal Luthra","title":"Home"},{"location":"#welcome-to-data-engineering-101","text":"","title":"Welcome to Data Engineering - 101"},{"location":"#index","text":"SQL Performance Tuning: SQL Performance Tuning Page SQL Performance Tuning Summary Concepts Data Warehousing Concepts Career Development Cold Mail Tactics About Me Author : Kushal Luthra","title":"Index"},{"location":"aboutme/","text":"About me Kushal is currently Lead Engineer at Airtel Africa Digital Labs (BigData & Analytics Team). He is the Lead for Business Intelligence product, which is being built from scratch, and is aimed to be scalable (handles data that can be in Gigabytes and Terabytes) and replicable across 14 OpCos. He has extensive experience that spans across various technologies, including Python, Spark(PySpark), SQL, Hive, Hadoop, Apache Hudi, Airflow, Sqoop, Gitlab, BItBucket, CICD. He also has experience of migrating Data Solutions on legacy system to Big Data Stack. He has built data pipelines from scratch, with focus on data frameworks. Domain : Retail and Telecom Distributed Computing: Hadoop, HDFS, Yarn, Spark Programming Languages: Python, Scala Operating System: Linux, Unix Development Tools: JIRA Databases: Postgres, MongoDB, Oracle Exadata Methodologies: Agile/Scrum Open to hearing about exciting information/opportunities in meaningful industries, more tech connections and both mentor/mentee relationships.","title":"About Me"},{"location":"SQL/docs/Data-Warehousing-basics/","text":"Data Warehouse What is Data Warehouse? Put simply, it\u2019s a central place where data is stored for the purpose of analysis and reporting. The data may be collected from a variety of sources. It\u2019s organised to provide complete data for users that can be easily understood in a business context. Different from databases in that it\u2019s purpose is for analysis -In companies, we use dimensional modelling to design our data warehouses. Dimensional modelling always uses two types of tables \u2013 facts and dimensions. a. Fact Tables- Contain the measurements, metrics or facts of a business process i.e. Transactions (items & baskets) Each fact will include measures (e.g. spend) and context data. Describe the measurements/facts in a transaction table \u2013 what was bought/how much it cost, etc. b. Dimension Tables- Stores attributes that describe the objects in a fact table i.e. Stores, Products, Customers They are linked together \u2013 this is a relational model - a. Primary key - Is the attribute or a set of attributes in an entity whose value(s) guarantee only one tuple (row) exists for each value. The fact table will also include foreign keys that relate each fact to our dimension tables. b. Foreign key The primary key of another table referenced here. Each entity in a dimension table will contain an attributes that describe that entity. There will also be a key that is used to join the dimension table to the fact table. Characteristics of Data warehouse Data warehouse is a database which is separate from operational database which stores historical information also. Data warehouse database contains transaction(OLTP) as well as analytical data(OLAP). Data warehouse helps higher management to take strategic as well as tactical decisions using historical or current data. Data warehouse helps consolidated historical data analysis. Data warehouse helps business user to see the current trends to run the business. Data warehouse is used for reporting and data analysis purpose. Types of Data warehouse systems a. Data Mart Data Mart is a simplest set of Data warehouse which is used to focus on single functional area of the business. b. Online Analytical Processing (OLAP) - Refer OLAP is used at strategic level and contains aggregated data, covering number of years. The key purpose to use OLAP system is to reduce the query response time and increase the effectiveness of reporting. So, data is denormalized. OLAP uses Star-Schema,Snowflakes schema or Fact-Dimensions. OLAP database stores aggregated historical data in multidimensional schema. eg - summaries data. c. Online Transactional Processing (OLTP) - Refer It is operational database, maintaining large number of small daily transactions like insert,update and delete. Data is normailzed. OLTP uses Entity Relations. OLTP system maintains concurrency and it avoids the centralization so as to avoid the single point of failures. Data concurrency and integrity = focus. d. Predictive Analysis Difference between Data Warehouse and Data Mart a. Definition - The Data Warehouse is a large repository of data collected from different organizations or departments within a corporation. The data mart is an only sub-type of a Data Warehouse. It is designed to meet the need of a certain user group. b. Focus - Data warehouse focuses on multiple business areas. Data mart focuses only on single subject area. c. Usage - DW - It helps to take a strategic decision. DM - The data mart is used to take tactical decisions for growth of business. d. Type of system - DW - This is centralized system where one fact is at center surrounded by dimension tables. DM - Data mart system is de-centralized system e. data model - DW = top down DM - bottom up f. source - Data warehouse data comes from multiple heterogeneous data sources. Data mart data is data of only one business area.Many times it will come from only one data source. g. Implementation Time - Data warehouse contains all data which will come from multiple data sources. It will take time to build data warehouse. The Time to build data warehouse is months to years. Data mart is small data warehouse which will contain the data of only a single business area. The implementation time to build data mart is in months. Data Warehousing -It\u2019s the process of TRANSFORMING data into information and making it available to users in a TIMELY enough manner to make a difference. Different use cases of ETL a. Data Warehousing - User needs to fetch the historical data as well as current data for developing data warehouse. The Data warehouse data is nothing but combination of historical data as well as transactional data. Its data sources might be different.User needs to fetch the data from multiple heterogeneous systems and load it in to single target system which is also called as data warehouse. b. Data Migration ETL tools are widely used in data migration projects. If the organization is managing the data in oracle 10 g previously and now organization wants to go for SQL server cloud database then there is need to migrate the data from Source to Target. To do this kind of migration the ETL tools are very useful. If user wants to write the code of ETL it is very time consuming process. To make this simple the ETL tools are very useful in which the coding is simple as compare to PL SQL or T-SQL code. So ETL process i very useful in Data migration projects. c. Data Integration Now a days big organizations are acquiring small firms. Obviously the data source for the different organizations may be different.We need to integrate the data from one organization to other organization. These kind of integration projects need the ETL process to extract the data,transform the data and load the data. d. Third Party data management- many a times company outsources process to different vendors. eg - telecom - billing managed by one and CRM by other vendor. If CRM company needs some data from the company who is managing the Billing. That company will receive a data feed from the other company. To load the data from the feed ETL process is used. ETL Extract Extract source data from our client- Once the data we will receive has been agreed, it is transferred from the client to us via our secure FTP system called Axway. This data is known as source data. Once the data has been received, it is validated according to the retailer-specific rules which are outlined in the DIS. If the data does not reflect what is outlined in the DIS changes may need to be made \u2013 either by updating the DIS or requesting a resupply. Once we agree the data is in the correct format it can be read in. Data Extraction can be - Full or Partial (Delta). DIS - The data we receive is mapped in a document known as Data Interface Specification (DIS). QA check in Source / RAW layer- A key section of RAW is quality assurance (QA). We carry out standard checks to ensure data is \u201chealthy\u201d and without errors Main focus of the checks is the fact tables, and relation on the fact data with the key dimension tables. Checks include: * Number of baskets in the basket and item tables * Levels of spend * Missing foreign/primary keys Any issues found are recorded and can either be resolved with the solution DSG or may require input from the client Once these checks are successfully completed the build moves forward into the PREP stage. Transform Transform this client data to meet the operational and business needs of our internal database. Within prep we transform the client data into a standardized format within the guidelines of Marketplace What types of transformations do we perform? i. Reject bad data - Bad data i.e. record where key info is missing. What is rejected depends on the business rules. We keep a record of rejected data by extracting them to a separate table, mark the missing field. ii. Remove duplicates Duplicate data can have negative impact on results. Important to understand if it is really a duplicate before removing. iii. Convert fields Convert from character to date and numeric fields where relevant e.g. spends/quantities. iv. Text manipulations Changes format e.g. change lookup value to descriptive form. v. Merges with other tables Merge lookups/useful fields that should be on specific table. vi. Aggregate data Sometimes need to roll up products in same basket or even create basket table - involves summations. vii. Rename fields Rename to make more meaningful \u2013 esp if in a foreign language. viii. Create standard fields Essential to marketplace, same naming conventions e.g. dib_bask_code. Load Load into our analytical data mart within Marketplace The load is automated, so you will not be expected to know exactly what occurs. Here is an overview: Inbound Outbound -extracts and updates required data into standard structure Staging - manage slowly changing dimensions, generate surrogate keys and *create skeleton records SCD - a dimension is considered a SCD when its attributes remain almost constant over time, requiring relatively minor alterations to represent the evolved state. Surrogate Keys - system-generated and non-persistent integer keys which replace foreign keys. Skeleton records - Generated when a foreign key in a fact table does not have a match in the dimension table. A dummy or \u2018skeleton\u2019 record is created in the dimension table. There are following 3 Types of Data Loading Strategies : i. Initial load : Populating all the data tables from source system and loads it in to data warehouse table. ii. Incremental Load : Applying the ongoing changes as necessary in periodic manner. iii. Full Refresh : Completely erases the data from one or more tables and reload the fresh data. Star and Snowflake schema Star Schema In the star schema design, a single object (the fact table) sits in the middle and is radically connected to other surrounding objects (dimension lookup tables) like a star. Each dimension is represented as a single table. The primary key in each dimension table is related to a foreign key in the fact table. All measures in the fact table are related to all the dimensions that fact table is related to. In other words, they all have the same level of granularity. A star schema can be simple or complex. A simple star consists of one fact table; a complex star can have more than one fact table. Snowflake Schema It is an extension of star schema. In a star schema, each dimension is represented by a single dimensional table, whereas in a snowflake schema, that dimensional table is normalized into multiple lookup tables, each representing a level in the dimensional hierarchy. Adv - improvement in query performance due to minimized disk storage requirements and joining smaller lookup tables. Disadvantage- additional maintenance efforts needed due to the increase number of lookup tables. Fact Table Granularity The first step in designing a fact table is to determine the granularity of the fact table. By granularity, we mean the lowest level of information that will be stored in the fact table. This constitutes two steps: i. Determine which dimensions will be included - this depends on business process being targetted. ii. Determine where along the hierarchy of each dimension the information will be kept - This depends on requirements. Eg - if client wants hourly reports, then fact table will keep hour as lowest level of granularity. If daily reports are fine, then date_id is lowest level of granularity. The determining factors usually goes back to the requirements. Fact And Fact Table Types There are three types of facts: i. Additive: Additive facts are facts that can be summed up through all of the dimensions in the fact table. ii. Semi-Additive: Semi-additive facts are facts that can be summed up for some of the dimensions in the fact table, but not the others. iii. Non-Additive: Non-additive facts are facts that cannot be summed up for any of the dimensions present in the fact table. eg1 - Additive Fact - Consider a retailer fact table with following columns - - Date - Store - Product - Sales_Amount The purpose of this table is to record the sales amount for each product in each store on a daily basis. Sales_Amount is an additive fact, because you can sum up this fact along any of the three dimensions present in the fact table -- date, store, and product. eg2A - Semi-Additive Fact and Non-Additive Fact - Say we are a bank with the following fact table: - Date - Account - Current_Balance - Profit_Margin The purpose of this table is to record the current balance for each account at the end of each day, as well as the profit margin for each account for each day. Current_Balance and Profit_Margin are the facts. Current_Balance is a semi-additive fact, as it makes sense to add them up for all accounts (what's the total current balance for all accounts in the bank?), but it does not make sense to add them up through time (adding up all current balances for a given account for each day of the month does not give us any useful information). Profit_Margin is a non-additive fact, for it does not make sense to add them up for the account level or the day level. eg 2B - semi -additive - distinct customers who shopped in a day = semi additive. Across all stores, this number can be aggregated. For example, store A has 300 customers and store B has 200 customers. So total 500 customers. But cant add across date dimension. So no summation possible across days in a week. non-additive = %age loyalty transaction in a day. For example, store A has 30% sales as loyalty count, and store B has 40%. But we cant add these two figures to find overall loyalty sales. Based on the above classifications, there are two types of Fact TABLES: * Cumulative: This type of fact table describes what has happened over a period of time. For example, this fact table may describe the total sales by product by store by day. The facts for this type of fact tables are mostly additive facts. The first example presented here is a cumulative fact table. * Snapshot: This type of fact table describes the state of things in a particular instance of time, and usually includes more semi-additive and non-additive facts. The second example presented here is a snapshot fact table. Slowly Changing Dimensions The \"Slowly Changing Dimension\" problem is a common one particular to data warehousing. In a nutshell, this applies to cases where the attribute for a record varies over time. There are in general three ways to solve this type of problem, and they are categorized as follows: Type 1 The new record replaces the original record. No trace of the old record exists. In other words, no history is kept. Advantage - * easiest to handle as no need to maintain history. Disadvantage- *History is lost. Cant track past behavior. So, Type 1 slowly changing dimension should be used when it is not necessary for the data warehouse to keep track of historical changes. Type 2 A new record is added into the customer dimension table. Therefore, the customer is treated essentially as two people. Both the original and the new record will be present. The new record gets its own primary key. Advantages: - This allows us to accurately keep all historical information. Disadvantages: - This will cause the size of the table to grow fast. In cases where the number of rows for the table is very high to start with, storage and performance can become a concern. - This necessarily complicates the ETL process. Type 3 The original record is modified to reflect the change. We add more column to track change. But this is feasible only if changes to be tracked are finite. For example, phone or address changes more than once will complicate things. Data Integrity Data integrity refers to the validity of data, meaning data is consistent and correct. In the data warehousing field, we frequently hear the term, \"Garbage In, Garbage Out.\" If there is no data integrity in the data warehouse, any resulting report and analysis will not be useful. In a data warehouse or a data mart, there are 3 areas of where data integrity needs to be enforced: a. Database level We can enforce data integrity at the database level. Common ways of enforcing data integrity include: i. Referential integrity The relationship between the primary key of one table and the foreign key of another table must always be maintained. For example, a primary key cannot be deleted if there is still a foreign key that refers to this primary key. ii. Primary key / Unique constraint Primary keys and the UNIQUE constraint are used to make sure every row in a table can be uniquely identified. iii. Not NULL vs. NULL-able For columns identified as NOT NULL, they may not have a NULL value. iv. Valid Values Only allowed values are permitted in the database. For example, if a column can only have positive integers, a value of '-1' cannot be allowed. b. ETL process For each step of the ETL process, data integrity checks should be put in place to ensure that source data is the same as the data in the destination. Most common checks include record counts or record sums. c. Access level We need to ensure that data is not altered by any unauthorized means either during the ETL process or in the data warehouse. To do this, there needs to be safeguards against unauthorized access to data (including physical access to the servers), as well as logging of all data access history. Data integrity can only ensured if there is no unauthorized access to the data. 4F. Factless Fact Table A factless fact table is a fact table that does not have any measures. It is essentially an intersection of dimensions. On the surface, a factless fact table does not make sense, since a fact table is, after all, about facts. However, there are situations where having this kind of relationship makes sense in data warehousing. eg1 - student class attendance record. In this case, the fact table would consist of 3 dimensions: the student dimension, the time dimension, and the class dimension. This factless fact table would look like the following: Fact Table \"school_attendance\" date_id classId student_id 02-02-2020 1 101 02-02-2020 1 102 02-02-2020 1 103 ---------------------------------- The only measure that you can possibly attach to each combination is \"1\" to show the presence of that particular combination. However, adding a fact that always shows 1 is redundant because we can simply use the COUNT function in SQL to answer the same questions. eg 2 - online sales in CRV. columns - date_id, store_id, till_id, pos_id In essence it contains only 1 column = basket_key. If a basket is in this table, it means its online sale, else offline sale. eg3 - Promotion data. Table structure could be - date_id | store_id| promo_type| promo_id| basket_key promo_type = Promotion can be online, in-store, flat discount, coupon, voucher, etc. Above table contains info of promotion applied on a basket. No measurable fact exists here. But why needed ? Transaction data contains info of what item was sold on promotion. But promotion data contains information of all the promotion during the purchase period. That is, all products having promotion applied on them, including those which were not sold in spite of promotion. And so, this promotion table becomes pivotal even though it contains no measurable fact. Why need factless facts? Factless fact tables offer the most flexibility in data warehouse design. For example, one can easily answer the following questions with this factless fact table: * How many students attended a particular class on a particular day? * How many classes on average does a student attend on a given day? Without using a factless fact table, we will need two separate fact tables to answer the above two questions. With the above factless fact table, it becomes the only fact table that's needed. Junk Dimension There are columns in Fact table which can have only a few or 2 kind of values - true or false, 1 or 0, etc. eg = bulk Vs non-bulk online vs offline promo vs non-promo vs hybrid sale etc. From business point of view, capturing above info in Fact table is very important. Issue -having these info will only make our fact table bulky and eventually unmanageable. Soln - junk dimension. eg - CRV basket channel seg - shop_channel_code in 0,1,2 or 3 - covers both bulk/non-bulk and online/offline. this would reduce 2 columns in fact table to 1. we can expand scope of above column to include promo info, and in that way we replace 3 fact columns by 1. This will result in a data warehousing environment that offer better performance as well as being easier to manage. [reference] (https://www.1keydata.com/datawarehousing/junk-dimension.html)","title":"Data Warehousing Concepts"},{"location":"SQL/docs/Data-Warehousing-basics/#data-warehouse","text":"","title":"Data Warehouse"},{"location":"SQL/docs/Data-Warehousing-basics/#what-is-data-warehouse","text":"Put simply, it\u2019s a central place where data is stored for the purpose of analysis and reporting. The data may be collected from a variety of sources. It\u2019s organised to provide complete data for users that can be easily understood in a business context. Different from databases in that it\u2019s purpose is for analysis -In companies, we use dimensional modelling to design our data warehouses. Dimensional modelling always uses two types of tables \u2013 facts and dimensions. a. Fact Tables- Contain the measurements, metrics or facts of a business process i.e. Transactions (items & baskets) Each fact will include measures (e.g. spend) and context data. Describe the measurements/facts in a transaction table \u2013 what was bought/how much it cost, etc. b. Dimension Tables- Stores attributes that describe the objects in a fact table i.e. Stores, Products, Customers They are linked together \u2013 this is a relational model - a. Primary key - Is the attribute or a set of attributes in an entity whose value(s) guarantee only one tuple (row) exists for each value. The fact table will also include foreign keys that relate each fact to our dimension tables. b. Foreign key The primary key of another table referenced here. Each entity in a dimension table will contain an attributes that describe that entity. There will also be a key that is used to join the dimension table to the fact table.","title":"What is Data Warehouse?"},{"location":"SQL/docs/Data-Warehousing-basics/#characteristics-of-data-warehouse","text":"Data warehouse is a database which is separate from operational database which stores historical information also. Data warehouse database contains transaction(OLTP) as well as analytical data(OLAP). Data warehouse helps higher management to take strategic as well as tactical decisions using historical or current data. Data warehouse helps consolidated historical data analysis. Data warehouse helps business user to see the current trends to run the business. Data warehouse is used for reporting and data analysis purpose.","title":"Characteristics of Data warehouse"},{"location":"SQL/docs/Data-Warehousing-basics/#types-of-data-warehouse-systems","text":"a. Data Mart Data Mart is a simplest set of Data warehouse which is used to focus on single functional area of the business. b. Online Analytical Processing (OLAP) - Refer OLAP is used at strategic level and contains aggregated data, covering number of years. The key purpose to use OLAP system is to reduce the query response time and increase the effectiveness of reporting. So, data is denormalized. OLAP uses Star-Schema,Snowflakes schema or Fact-Dimensions. OLAP database stores aggregated historical data in multidimensional schema. eg - summaries data. c. Online Transactional Processing (OLTP) - Refer It is operational database, maintaining large number of small daily transactions like insert,update and delete. Data is normailzed. OLTP uses Entity Relations. OLTP system maintains concurrency and it avoids the centralization so as to avoid the single point of failures. Data concurrency and integrity = focus. d. Predictive Analysis","title":"Types of Data warehouse systems"},{"location":"SQL/docs/Data-Warehousing-basics/#difference-between-data-warehouse-and-data-mart","text":"a. Definition - The Data Warehouse is a large repository of data collected from different organizations or departments within a corporation. The data mart is an only sub-type of a Data Warehouse. It is designed to meet the need of a certain user group. b. Focus - Data warehouse focuses on multiple business areas. Data mart focuses only on single subject area. c. Usage - DW - It helps to take a strategic decision. DM - The data mart is used to take tactical decisions for growth of business. d. Type of system - DW - This is centralized system where one fact is at center surrounded by dimension tables. DM - Data mart system is de-centralized system e. data model - DW = top down DM - bottom up f. source - Data warehouse data comes from multiple heterogeneous data sources. Data mart data is data of only one business area.Many times it will come from only one data source. g. Implementation Time - Data warehouse contains all data which will come from multiple data sources. It will take time to build data warehouse. The Time to build data warehouse is months to years. Data mart is small data warehouse which will contain the data of only a single business area. The implementation time to build data mart is in months.","title":"Difference between Data Warehouse and Data Mart"},{"location":"SQL/docs/Data-Warehousing-basics/#data-warehousing","text":"-It\u2019s the process of TRANSFORMING data into information and making it available to users in a TIMELY enough manner to make a difference.","title":"Data Warehousing"},{"location":"SQL/docs/Data-Warehousing-basics/#different-use-cases-of-etl","text":"a. Data Warehousing - User needs to fetch the historical data as well as current data for developing data warehouse. The Data warehouse data is nothing but combination of historical data as well as transactional data. Its data sources might be different.User needs to fetch the data from multiple heterogeneous systems and load it in to single target system which is also called as data warehouse. b. Data Migration ETL tools are widely used in data migration projects. If the organization is managing the data in oracle 10 g previously and now organization wants to go for SQL server cloud database then there is need to migrate the data from Source to Target. To do this kind of migration the ETL tools are very useful. If user wants to write the code of ETL it is very time consuming process. To make this simple the ETL tools are very useful in which the coding is simple as compare to PL SQL or T-SQL code. So ETL process i very useful in Data migration projects. c. Data Integration Now a days big organizations are acquiring small firms. Obviously the data source for the different organizations may be different.We need to integrate the data from one organization to other organization. These kind of integration projects need the ETL process to extract the data,transform the data and load the data. d. Third Party data management- many a times company outsources process to different vendors. eg - telecom - billing managed by one and CRM by other vendor. If CRM company needs some data from the company who is managing the Billing. That company will receive a data feed from the other company. To load the data from the feed ETL process is used.","title":"Different use cases of ETL"},{"location":"SQL/docs/Data-Warehousing-basics/#etl","text":"","title":"ETL"},{"location":"SQL/docs/Data-Warehousing-basics/#extract","text":"Extract source data from our client- Once the data we will receive has been agreed, it is transferred from the client to us via our secure FTP system called Axway. This data is known as source data. Once the data has been received, it is validated according to the retailer-specific rules which are outlined in the DIS. If the data does not reflect what is outlined in the DIS changes may need to be made \u2013 either by updating the DIS or requesting a resupply. Once we agree the data is in the correct format it can be read in. Data Extraction can be - Full or Partial (Delta). DIS - The data we receive is mapped in a document known as Data Interface Specification (DIS). QA check in Source / RAW layer- A key section of RAW is quality assurance (QA). We carry out standard checks to ensure data is \u201chealthy\u201d and without errors Main focus of the checks is the fact tables, and relation on the fact data with the key dimension tables. Checks include: * Number of baskets in the basket and item tables * Levels of spend * Missing foreign/primary keys Any issues found are recorded and can either be resolved with the solution DSG or may require input from the client Once these checks are successfully completed the build moves forward into the PREP stage.","title":"Extract"},{"location":"SQL/docs/Data-Warehousing-basics/#transform","text":"Transform this client data to meet the operational and business needs of our internal database. Within prep we transform the client data into a standardized format within the guidelines of Marketplace What types of transformations do we perform? i. Reject bad data - Bad data i.e. record where key info is missing. What is rejected depends on the business rules. We keep a record of rejected data by extracting them to a separate table, mark the missing field. ii. Remove duplicates Duplicate data can have negative impact on results. Important to understand if it is really a duplicate before removing. iii. Convert fields Convert from character to date and numeric fields where relevant e.g. spends/quantities. iv. Text manipulations Changes format e.g. change lookup value to descriptive form. v. Merges with other tables Merge lookups/useful fields that should be on specific table. vi. Aggregate data Sometimes need to roll up products in same basket or even create basket table - involves summations. vii. Rename fields Rename to make more meaningful \u2013 esp if in a foreign language. viii. Create standard fields Essential to marketplace, same naming conventions e.g. dib_bask_code.","title":"Transform"},{"location":"SQL/docs/Data-Warehousing-basics/#load","text":"Load into our analytical data mart within Marketplace The load is automated, so you will not be expected to know exactly what occurs. Here is an overview: Inbound Outbound -extracts and updates required data into standard structure Staging - manage slowly changing dimensions, generate surrogate keys and *create skeleton records SCD - a dimension is considered a SCD when its attributes remain almost constant over time, requiring relatively minor alterations to represent the evolved state. Surrogate Keys - system-generated and non-persistent integer keys which replace foreign keys. Skeleton records - Generated when a foreign key in a fact table does not have a match in the dimension table. A dummy or \u2018skeleton\u2019 record is created in the dimension table. There are following 3 Types of Data Loading Strategies : i. Initial load : Populating all the data tables from source system and loads it in to data warehouse table. ii. Incremental Load : Applying the ongoing changes as necessary in periodic manner. iii. Full Refresh : Completely erases the data from one or more tables and reload the fresh data.","title":"Load"},{"location":"SQL/docs/Data-Warehousing-basics/#star-and-snowflake-schema","text":"","title":"Star and Snowflake schema"},{"location":"SQL/docs/Data-Warehousing-basics/#star-schema","text":"In the star schema design, a single object (the fact table) sits in the middle and is radically connected to other surrounding objects (dimension lookup tables) like a star. Each dimension is represented as a single table. The primary key in each dimension table is related to a foreign key in the fact table. All measures in the fact table are related to all the dimensions that fact table is related to. In other words, they all have the same level of granularity. A star schema can be simple or complex. A simple star consists of one fact table; a complex star can have more than one fact table.","title":"Star Schema"},{"location":"SQL/docs/Data-Warehousing-basics/#snowflake-schema","text":"It is an extension of star schema. In a star schema, each dimension is represented by a single dimensional table, whereas in a snowflake schema, that dimensional table is normalized into multiple lookup tables, each representing a level in the dimensional hierarchy. Adv - improvement in query performance due to minimized disk storage requirements and joining smaller lookup tables. Disadvantage- additional maintenance efforts needed due to the increase number of lookup tables.","title":"Snowflake Schema"},{"location":"SQL/docs/Data-Warehousing-basics/#fact-table-granularity","text":"The first step in designing a fact table is to determine the granularity of the fact table. By granularity, we mean the lowest level of information that will be stored in the fact table. This constitutes two steps: i. Determine which dimensions will be included - this depends on business process being targetted. ii. Determine where along the hierarchy of each dimension the information will be kept - This depends on requirements. Eg - if client wants hourly reports, then fact table will keep hour as lowest level of granularity. If daily reports are fine, then date_id is lowest level of granularity. The determining factors usually goes back to the requirements.","title":"Fact Table Granularity"},{"location":"SQL/docs/Data-Warehousing-basics/#fact-and-fact-table-types","text":"There are three types of facts: i. Additive: Additive facts are facts that can be summed up through all of the dimensions in the fact table. ii. Semi-Additive: Semi-additive facts are facts that can be summed up for some of the dimensions in the fact table, but not the others. iii. Non-Additive: Non-additive facts are facts that cannot be summed up for any of the dimensions present in the fact table. eg1 - Additive Fact - Consider a retailer fact table with following columns - - Date - Store - Product - Sales_Amount The purpose of this table is to record the sales amount for each product in each store on a daily basis. Sales_Amount is an additive fact, because you can sum up this fact along any of the three dimensions present in the fact table -- date, store, and product. eg2A - Semi-Additive Fact and Non-Additive Fact - Say we are a bank with the following fact table: - Date - Account - Current_Balance - Profit_Margin The purpose of this table is to record the current balance for each account at the end of each day, as well as the profit margin for each account for each day. Current_Balance and Profit_Margin are the facts. Current_Balance is a semi-additive fact, as it makes sense to add them up for all accounts (what's the total current balance for all accounts in the bank?), but it does not make sense to add them up through time (adding up all current balances for a given account for each day of the month does not give us any useful information). Profit_Margin is a non-additive fact, for it does not make sense to add them up for the account level or the day level. eg 2B - semi -additive - distinct customers who shopped in a day = semi additive. Across all stores, this number can be aggregated. For example, store A has 300 customers and store B has 200 customers. So total 500 customers. But cant add across date dimension. So no summation possible across days in a week. non-additive = %age loyalty transaction in a day. For example, store A has 30% sales as loyalty count, and store B has 40%. But we cant add these two figures to find overall loyalty sales. Based on the above classifications, there are two types of Fact TABLES: * Cumulative: This type of fact table describes what has happened over a period of time. For example, this fact table may describe the total sales by product by store by day. The facts for this type of fact tables are mostly additive facts. The first example presented here is a cumulative fact table. * Snapshot: This type of fact table describes the state of things in a particular instance of time, and usually includes more semi-additive and non-additive facts. The second example presented here is a snapshot fact table.","title":"Fact And Fact Table Types "},{"location":"SQL/docs/Data-Warehousing-basics/#slowly-changing-dimensions","text":"The \"Slowly Changing Dimension\" problem is a common one particular to data warehousing. In a nutshell, this applies to cases where the attribute for a record varies over time. There are in general three ways to solve this type of problem, and they are categorized as follows:","title":"Slowly Changing Dimensions"},{"location":"SQL/docs/Data-Warehousing-basics/#type-1","text":"The new record replaces the original record. No trace of the old record exists. In other words, no history is kept. Advantage - * easiest to handle as no need to maintain history. Disadvantage- *History is lost. Cant track past behavior. So, Type 1 slowly changing dimension should be used when it is not necessary for the data warehouse to keep track of historical changes.","title":"Type 1"},{"location":"SQL/docs/Data-Warehousing-basics/#type-2","text":"A new record is added into the customer dimension table. Therefore, the customer is treated essentially as two people. Both the original and the new record will be present. The new record gets its own primary key. Advantages: - This allows us to accurately keep all historical information. Disadvantages: - This will cause the size of the table to grow fast. In cases where the number of rows for the table is very high to start with, storage and performance can become a concern. - This necessarily complicates the ETL process.","title":"Type 2"},{"location":"SQL/docs/Data-Warehousing-basics/#type-3","text":"The original record is modified to reflect the change. We add more column to track change. But this is feasible only if changes to be tracked are finite. For example, phone or address changes more than once will complicate things.","title":"Type 3"},{"location":"SQL/docs/Data-Warehousing-basics/#data-integrity","text":"Data integrity refers to the validity of data, meaning data is consistent and correct. In the data warehousing field, we frequently hear the term, \"Garbage In, Garbage Out.\" If there is no data integrity in the data warehouse, any resulting report and analysis will not be useful. In a data warehouse or a data mart, there are 3 areas of where data integrity needs to be enforced: a. Database level We can enforce data integrity at the database level. Common ways of enforcing data integrity include: i. Referential integrity The relationship between the primary key of one table and the foreign key of another table must always be maintained. For example, a primary key cannot be deleted if there is still a foreign key that refers to this primary key. ii. Primary key / Unique constraint Primary keys and the UNIQUE constraint are used to make sure every row in a table can be uniquely identified. iii. Not NULL vs. NULL-able For columns identified as NOT NULL, they may not have a NULL value. iv. Valid Values Only allowed values are permitted in the database. For example, if a column can only have positive integers, a value of '-1' cannot be allowed. b. ETL process For each step of the ETL process, data integrity checks should be put in place to ensure that source data is the same as the data in the destination. Most common checks include record counts or record sums. c. Access level We need to ensure that data is not altered by any unauthorized means either during the ETL process or in the data warehouse. To do this, there needs to be safeguards against unauthorized access to data (including physical access to the servers), as well as logging of all data access history. Data integrity can only ensured if there is no unauthorized access to the data. 4F. Factless Fact Table A factless fact table is a fact table that does not have any measures. It is essentially an intersection of dimensions. On the surface, a factless fact table does not make sense, since a fact table is, after all, about facts. However, there are situations where having this kind of relationship makes sense in data warehousing. eg1 - student class attendance record. In this case, the fact table would consist of 3 dimensions: the student dimension, the time dimension, and the class dimension. This factless fact table would look like the following: Fact Table \"school_attendance\" date_id classId student_id 02-02-2020 1 101 02-02-2020 1 102 02-02-2020 1 103 ---------------------------------- The only measure that you can possibly attach to each combination is \"1\" to show the presence of that particular combination. However, adding a fact that always shows 1 is redundant because we can simply use the COUNT function in SQL to answer the same questions. eg 2 - online sales in CRV. columns - date_id, store_id, till_id, pos_id In essence it contains only 1 column = basket_key. If a basket is in this table, it means its online sale, else offline sale. eg3 - Promotion data. Table structure could be - date_id | store_id| promo_type| promo_id| basket_key promo_type = Promotion can be online, in-store, flat discount, coupon, voucher, etc. Above table contains info of promotion applied on a basket. No measurable fact exists here. But why needed ? Transaction data contains info of what item was sold on promotion. But promotion data contains information of all the promotion during the purchase period. That is, all products having promotion applied on them, including those which were not sold in spite of promotion. And so, this promotion table becomes pivotal even though it contains no measurable fact. Why need factless facts? Factless fact tables offer the most flexibility in data warehouse design. For example, one can easily answer the following questions with this factless fact table: * How many students attended a particular class on a particular day? * How many classes on average does a student attend on a given day? Without using a factless fact table, we will need two separate fact tables to answer the above two questions. With the above factless fact table, it becomes the only fact table that's needed.","title":"Data Integrity"},{"location":"SQL/docs/Data-Warehousing-basics/#junk-dimension","text":"There are columns in Fact table which can have only a few or 2 kind of values - true or false, 1 or 0, etc. eg = bulk Vs non-bulk online vs offline promo vs non-promo vs hybrid sale etc. From business point of view, capturing above info in Fact table is very important. Issue -having these info will only make our fact table bulky and eventually unmanageable. Soln - junk dimension. eg - CRV basket channel seg - shop_channel_code in 0,1,2 or 3 - covers both bulk/non-bulk and online/offline. this would reduce 2 columns in fact table to 1. we can expand scope of above column to include promo info, and in that way we replace 3 fact columns by 1. This will result in a data warehousing environment that offer better performance as well as being easier to manage. [reference] (https://www.1keydata.com/datawarehousing/junk-dimension.html)","title":"Junk Dimension"},{"location":"SQL/docs/sql_performance_tuning/","text":"SQL Performance Tuning : Summary Tip 1: Never use *(Star) to fetch all records from table Sql query become fast if you use actual columns instead of * to fetch all the records from the table. Not Recommended - Select * from Employee; Recommended Select Eno,Ename,Address from Employee; Tip 2: Try to avoid DISTINCT keyword from the query Try to avoid DISTINCT keyword from select statements. DISTINCT keyword has high cost and low performance. When anyone uses DISTINCT keyword, it first SORTS the data from column and then fetches the distinct values. Use EXISTS operator instead of DISTINCT keyword. Not Recommended: SELECT DISTINCT d.dept_no, d.department_name FROM Department d,Employee e WHERE d.dept_no= e.dept_no; Recommended: SELECT d.dept_no, d.department_name FROM Department d WHERE EXISTS ( SELECT \u2018X\u2019 FROM Employee e WHERE d.dept_no= e.dept_no); Tip 3: Carefully use WHERE conditions in sql Try to use correct operator as per requirement given. Not Recommended: Select * from Employee WHERE salary != 65000; Recommended: Select * from Employee WHERE salary > 65000 and salary < 65000; Tip 4: Use Like operator instead of equal to (=) Not Recommended: Select * from Employee where name=\u2019Amit\u2019; Recommended: Select * from Employee where name like \u2018Amit%\u2019; Tip 5: Avoid HAVING clause/GROUP BY statements HAVING clause and GROUP BY statements have high cost. So try to avoid it in sql query. Not Recommended - Select * from Employee WHERE name=\u2019Amit\u2019 GROUP BY department HAVING salary=45000; Recommended - Select * from Employee WHERE name=\u2019Amit\u2019 and salary=45000; More added: - Having clause- We use Having clause to eliminate some of the group values. Issue \u2013 Having clause restricts the rows AFTER they are read. So if no restriction in \u201cwhere clause\u201d, optimizer will use full table scan. This is really important coz all predicates in the HAVING Clause will not be used as access predicates. So they will not make optimizer use indexes, partitions etc. This is because to perform HAVING clause, it first reads all the rows and then eliminates unnecessary rows. Tip 6: Use of EXISTS and IN Operators Basically, Operator IN has lowest performance. IN operator is used when Filter criteria is in subquery, whereas EXISTS operator is used when filter criteria is in main query. Example: IN Operator Select * from Employee where Department_name IN ( Select Department_name from Department where Dno=10); Exist operator Select * from Employee where EXISTS ( Select Department_name from Department where Dno=10); More added When you run a query with IN clause, database will process it in below format \u2013 That is, in case of use of IN clause, each value of sub query is joined with outer query. Treats below Query - select * from T1 where x in (select x from T2); as - select * from t1, (select x from t2) T2 where t1.x = t2.x; But when you use EXIST clause, database will exit as soon as it gets the first match . So, in case of EXIST clause it runs executes query in below format \u2013 Treats below query - select * from T1 where exists (select x from T2 where t1.x=t2.x); as - FOR x IN (select * from t1) LOOP IF (EXISTS ( SELECT X FROM T2) ) THEN OUTPUT THE RECORD END IF; END; That is, using EXIST clause will imply database will run it like a FOR loop and as soon as match is found, it moves to next record. So which one is faster \u2013 IN or EXIST? a. This totally depends on situation. Use IN when - outer table = Big and Subquery = Small Use EXISTS when \u2013 outer table = Small and Subquery = Big b. Even above rules are not fixed. For example, if subquery has bigger table, but it has an index, in this case use of IN is suggested. c. So- EXISTS doesn\u2019t work better than IN all the times. IN is better than EXISTS if \u2013 outer table = Big and Subquery = Small outer table = Small and Subquery = Big + Indexed NOT EXISTS vs NOT IN \u2022 NOT EXISTS is not equivalent of NOT IN. \u2022 NOT EXISTS cannot be used instead of NOT IN all the times. \u2022 More specifically, if there is any NULL value in your data, they will show different result. \u2022 If your subquery returns even one NULL value, NOT IN will not match any rows. \u2022 On other hand, if you have a chance to use NOT EXISTS instead of NOT IN, you should test it. \u2022 In most database versions of oracle, EXISTS and IN are treated similarly in terms of execution plan. Tip 7: Try to use UNION ALL instead of UNION UNION scans all data first and then eliminate duplicate so it has slow performance. Not Recommended Select * from Employee where dept_no=10 UNION Select * from Employee where dept_no=20; Recommended Select * from Employee where dept_no=10 UNION ALL Select * from Employee where dept_no=20; Tip 8: Avoid use of Functions in Where condition. Not Recommended Select * from Employee where Substr(name,1,3)=\u2019Ami\u2019; Recommended Select * from Employee where name like \u2018Ami%\u2019; Tip 9: convert OR to AND If we use OR clause, it will PREVENT index usages. Instead, we should use AND where possible. Not Recommended select * from sales where prod_id = 13 or promo_id=14; Recommended select * from sales where prod_id = 13 UNION All select * from sales where promo_id=14 AND prod_id <> 13; Tip 10: Subquery Unnesting Nested queries are very costly, and so transformer tries to convert them to equivalent join statements. Not Recommended select * from sales where cust_id IN (select cust_id from customers); Recommended select sales.* from sales, customers where sales.cust_id=customers.cust_id; Tip 11: IN and BETWEEN select * from employees where emp_id in (2,3,4,5); The above is equivalent to select * from employees where emp_id = 2 OR emp_id=3 OR emp_id=4 OR emp_id=5 ---this implies full table scan. Solution - select * from employees where emp_id between 2 and 5; Tip 12: Fetching first N records Suppose we want to see only 10 records in our select statement output. There are 2 ways to do this \u2013 Using rownum (Recommended) SELECT * FROM EMPLOYEE where rownum<11; Using fetch first (not recommended) SELECT * FROM EMPLOYEE FETCH FIRST 10 ROWS ONLY; In case of rownum- Here it reads first 10 rows use count STOPKEY operator, and so faster than fetch first method. In case of fetch first \u2013 Here we read whole table, and then applied a windowing function to select 1st 10 records. Tip 13: UNION vs UNION ALL: UNION \u2013 combines data and drops duplicate rows. UNION ALL \u2013 combines data and retains duplicate rows. Suggest: Some key points- \u2022 by default, UNION ALL is less costly than UNION, as latter sorts data internally to remove duplicates. \u2022 But if your table is indexed, then sort operation in UNION wont be that costly, and so you can use UNION also. So \u2013 - Use UNION if table is indexed and you don\u2019t want duplicates in output. - Use UNION ALL if\u2013 - There is no duplicate in your data, or - You are ok with having duplicate data in output. - But overall, UNION ALL gives better performance than UNION. Tip 14: INTERSECT Vs EXISTS operator Intersect gives common rows of 2 intersection in a Sorted order. As part of intersect, 2 rows sources are first sorted, and then common records are fetched. In place of INTERSECT operator, we should try and use EXISTS clause, which is more efficient. One caveat is that, in case of EXISTS clause, output is not sorted, unlike in case of INTERSECT clause. Not Recommended SELECT employee_id FROM employees where employee_id between 145 and 179 INTERSECT SELECT employee_id FROM employees WHERE first_name LIKE 'A%'; Recommended SELECT employee_id FROM employees e where employee_id between 145 and 179 and exists (SELECT employee_id FROM employees t WHERE first_name LIKE 'A%' and e.employee_id = t.employee_id); Tip 15: MINUS Vs NOT EXISTS MINUS operator eliminates matched rows of 1st (with 2nd) and returns rest of the rows of 1st. NOT EXISTS clause can also do same work as MINUS, but has much better performance. Not Recommended SELECT employee_id FROM employees where employee_id between 145 and 179 MINUS SELECT employee_id FROM employees WHERE first_name LIKE 'A%'; Recommended SELECT employee_id FROM employees e where employee_id between 145 and 179 and not exists (SELECT employee_id FROM employees t WHERE first_name LIKE 'A%' and e.employee_id = t.employee_id); Tip 16: Using Like conditions To enable use of indexes, avoid use of wild card character at beginning of source text scan. If you are forced to use wild card character at beginning, you can create reverse index, and handle that problem using that index. In this case, when you reverse index, then source text will eliminate use of wild card at beginning. Eg \u2013 Suppose you want to find records where last name ends is \u201chhar\u201d, then create reverse() index on last_name to reverse it and then use condition where reversed last name begins with \u201crahh\u201d. Though reverse() index usage will have cost, but if your column is selective enough, it wont be much cost. Tip 17: Using Functions on Indexed Columns will suppress index usage. Use of function on indexed column will suppress index usage. So, rewrite query to avoid use of function. BAD QUERY select employee_id, first_name, last_name from employees where trunc(hire_date,'YEAR') = '01-JAN-2002'; GOOD: rewritten query select employee_id, first_name, last_name from employees where hire_date between '01-JAN-2002' and '31-DEC-2002'; Eg2 - Bad select * from mytable where substr(emp_name,1,2) = 'Po'; Good select * from mytable where emp_name like 'Po%'; Note - In Spark, we have HashAggregates and SortAggregates. Hash Aggregates are more performant, and work only on mutable data types. That is, if all elements in your Select clause (except those in Group by clause) are mutable types like INT, FLOAT, etc, then spark will use Hash Aggregates. This means, sometimes, for performance gain, we need to apply Function to transform values. See #14 at below link for details https://github.com/kushal-luthra/spark-development/blob/master/notes/spark_opimization.md Tip 18: Handling NULL Values Failing to deal with NULL values will lead to unintentional results or performance losses. Why - \u2022B-Tree indexes do not index NULL values. \u2022If there are any NULL values in your indexed columns and you need to get rows which have NULL values, optimizer will not use your index, and perform a full table scan instead. \u2022That is, having Null values in your index may sometimes suppress index usage. Ways to handle NULL value-based performance loss a. Use IS NOT NULL condition in your WHERE clause. Use IS NOT NULL condition in your WHERE clause if you don\u2019t need to have NULL values in result set. That is, even if you now your result will not be having any NULL values, you should use \u201cis not null \u201c clause to make optimizer use indexes. Eg \u2013 Query 1: select emp_name, emp_id from employee where emp_id <> 1; Query 2: select emp_name, emp_id from employee where emp_id <> 1 and emp_id is not null; In query 1, we will see FULL Table scan and in case of query 2, we see index-based scan, and lower query cost. b. Adding not null constraint to your columns and insert a specific value for NULL values like \u20180\u2019 if value in a column is null. c. If reasonable, create a BITMAP index instead of B-Tree index. BITMAP indexes store NULL values. So even if there are null values in our column, optimizer will be able to use our indexes. However, you need to take into consideration index efficiency between B-Tree and BITMAP, as former as more efficient than latter. - We use BITMAP indexes when \u2013 cardinality is LOW and index not modified often. - We use B-Tree index when \u2013 cardinality/selectivity is high. Tip 19 : Use Truncate instead of Delete Truncate is always faster than DELETE command. This is because when you run delete command, oracle database generates lots of UNDO data for deleted rows and generating UNDO data is an expensive operation. Truncate doesn\u2019t generate UNDO Data. But before using Truncate command, there are few things to note about it- \u2022 No rollback \u2013 Truncate operation cannot be rollbacked, and Flashback is also not so easy after truncate operations. You may need to use Flashback Data Archive or some other external tools in this case. \u2022 Truncate is a DDL operation \u2013 So when you run Truncate, your transaction will be committed. It performs commit before and after Truncate operation. Since it does 2 commits, and even if truncate operation fails in between, the changes you did before will be permanent in any case. \u2022 Truncate a partition - We don\u2019t need to truncate whole table all the times. You can truncate partition as well. \u2022 Truncate doesn\u2019t fire DML triggers - So you wont be able to log your truncate operation because of that. But it can fire the DDL triggers. \u2022 Truncate makes unusable indexes usable again. But delete does not. Tip 20: Data Type Mismatches If data types of column and compared value dont match, this may suppress index usage. select cust_id, cust_code from customers where cust_code = 101; Vs select cust_id, cust_code from customers where cust_code = '101'; Tip 21: Tuning Ordered queries- Order By clause Order by mostly requires SORT operations . This sort operation is done in PGA or in disk (if PGA doesn\u2019t have enough memory) This disk is shown as \u2018temporary table space\u2019 in execution plan. Issue \u2013 sorting in disk is a costly operation. Solution \u2013 \u2022 Create a B-Tree index on column used in Order by Clause, or \u2022 Modify a B-Tree index to include column used in Order by Clause. \u2022 Why\u2013> B-Tree indexes store columns in Order and using B-Tree index will eliminate sort operations. Tip 22 : Retrieving MIN and MAX Values B-Tree indexes increase the performance a lot for min and max value searches. If no B-Tree index, optimizer will need to read whole table. If our query has another column or another aggregate function in your query, it will be reading whole index or whole table. For example- When you see below, if we are looking for min() and max() values individually, output is just 2 for each. But when we want to get min() and max() together, database will read full table, and hence cost is 8 times. This is coz we have 2 aggregate functions in our query. Bad - select min(), max() from mytable; Good - select * FROM (select min() from mytable) min_cust, (select max() from mytable) max_cust; Tip 23 : Views Simple view = view created from single table. Complex view = view created by using multiple tables. Some suggestions w.r.t. views- 1. If you don\u2019t need to use all the tables in a view, then don\u2019t use the view. Instead use the actual tables. Otherwise, server will have to join all tables, do aggregation etc on them for a view. i.e. use view for the purpose for which it was created. 2. Else create another view. 3. Don\u2019t join complex views with a table or another view - This is because most of the times view is first executed completely at first, and then result is used as row source to other table or view. So, in this case you be reading lots of unnecessary data and performing unnecessary join and group by. This will increase cost a lot. 4. Avoid performing outer join to the views \u2013 because if you use equality predicate on view column, the optimizer gets wrong if the table of that column has an outer join in the view as well. Because outer join may not know performance of view and may lead to bad execution as well. E.g. \u2013 if we do outer join, optimizer may not be able to push predicate inside the view definition at times of execution plan. Materialized Views- Unlike basic and complex views, materialized views store both query and data. Materialized view data can be refreshed manually or via auto-refresh. But materialized view maintenance is a burden to database \u2013 it needs to be kept up to date for each modification on each change. As compared to normal views, materialized view will improve performance as we will select data directly from materialized view, and there will be no sorts, joins etc. We can create index, partitions etc on materialized view like in an ordinary table. Summary \u2022 If you don\u2019t need to use all the tables in a view, then don\u2019t use the view. Instead use the actual tables. \u2022 Don\u2019t join complex views with a table or another view. \u2022 Avoid performing outer join to the views. \u2022 Use Materialized View - Unlike basic and complex views, materialized views store both query and data. Materialized view data can be refreshed manually or via auto-refresh. But materialized view maintenance is a burden to database . Tip 24: Frequent commit is not desired make REDO logs bulky as we may be committing prior to period. make lock on modified rows, making them unavailable to other applications. Tip 25: Multitable DML operations (skip for big data) Sometimes we have to read same table as input to different tables in our data warehouse. So, if we have 5 different tables requiring input from 1 table, we should ideally be reading input table just once, and keep on feeding into different output tables as per requirements. For this we have 2 options \u2013 \u2022 INSERT ALL \u2022 MERGE INTO","title":"SQL Performance Tuning"},{"location":"SQL/docs/sql_performance_tuning/#sql-performance-tuning-summary","text":"","title":"SQL Performance Tuning : Summary"},{"location":"SQL/docs/sql_performance_tuning/#tip-1-never-use-star-to-fetch-all-records-from-table","text":"Sql query become fast if you use actual columns instead of * to fetch all the records from the table. Not Recommended - Select * from Employee; Recommended Select Eno,Ename,Address from Employee;","title":"Tip 1: Never use *(Star) to fetch all records from table"},{"location":"SQL/docs/sql_performance_tuning/#tip-2-try-to-avoid-distinct-keyword-from-the-query","text":"Try to avoid DISTINCT keyword from select statements. DISTINCT keyword has high cost and low performance. When anyone uses DISTINCT keyword, it first SORTS the data from column and then fetches the distinct values. Use EXISTS operator instead of DISTINCT keyword. Not Recommended: SELECT DISTINCT d.dept_no, d.department_name FROM Department d,Employee e WHERE d.dept_no= e.dept_no; Recommended: SELECT d.dept_no, d.department_name FROM Department d WHERE EXISTS ( SELECT \u2018X\u2019 FROM Employee e WHERE d.dept_no= e.dept_no);","title":"Tip 2: Try to avoid DISTINCT keyword from the query"},{"location":"SQL/docs/sql_performance_tuning/#tip-3-carefully-use-where-conditions-in-sql","text":"Try to use correct operator as per requirement given. Not Recommended: Select * from Employee WHERE salary != 65000; Recommended: Select * from Employee WHERE salary > 65000 and salary < 65000;","title":"Tip 3: Carefully use WHERE conditions in sql"},{"location":"SQL/docs/sql_performance_tuning/#tip-4-use-like-operator-instead-of-equal-to","text":"Not Recommended: Select * from Employee where name=\u2019Amit\u2019; Recommended: Select * from Employee where name like \u2018Amit%\u2019;","title":"Tip 4: Use Like operator instead of equal to (=)"},{"location":"SQL/docs/sql_performance_tuning/#tip-5-avoid-having-clausegroup-by-statements","text":"HAVING clause and GROUP BY statements have high cost. So try to avoid it in sql query. Not Recommended - Select * from Employee WHERE name=\u2019Amit\u2019 GROUP BY department HAVING salary=45000; Recommended - Select * from Employee WHERE name=\u2019Amit\u2019 and salary=45000; More added: - Having clause- We use Having clause to eliminate some of the group values. Issue \u2013 Having clause restricts the rows AFTER they are read. So if no restriction in \u201cwhere clause\u201d, optimizer will use full table scan. This is really important coz all predicates in the HAVING Clause will not be used as access predicates. So they will not make optimizer use indexes, partitions etc. This is because to perform HAVING clause, it first reads all the rows and then eliminates unnecessary rows.","title":"Tip 5: Avoid HAVING clause/GROUP BY statements"},{"location":"SQL/docs/sql_performance_tuning/#tip-6-use-of-exists-and-in-operators","text":"Basically, Operator IN has lowest performance. IN operator is used when Filter criteria is in subquery, whereas EXISTS operator is used when filter criteria is in main query. Example: IN Operator Select * from Employee where Department_name IN ( Select Department_name from Department where Dno=10); Exist operator Select * from Employee where EXISTS ( Select Department_name from Department where Dno=10); More added When you run a query with IN clause, database will process it in below format \u2013 That is, in case of use of IN clause, each value of sub query is joined with outer query. Treats below Query - select * from T1 where x in (select x from T2); as - select * from t1, (select x from t2) T2 where t1.x = t2.x; But when you use EXIST clause, database will exit as soon as it gets the first match . So, in case of EXIST clause it runs executes query in below format \u2013 Treats below query - select * from T1 where exists (select x from T2 where t1.x=t2.x); as - FOR x IN (select * from t1) LOOP IF (EXISTS ( SELECT X FROM T2) ) THEN OUTPUT THE RECORD END IF; END; That is, using EXIST clause will imply database will run it like a FOR loop and as soon as match is found, it moves to next record. So which one is faster \u2013 IN or EXIST? a. This totally depends on situation. Use IN when - outer table = Big and Subquery = Small Use EXISTS when \u2013 outer table = Small and Subquery = Big b. Even above rules are not fixed. For example, if subquery has bigger table, but it has an index, in this case use of IN is suggested. c. So- EXISTS doesn\u2019t work better than IN all the times. IN is better than EXISTS if \u2013 outer table = Big and Subquery = Small outer table = Small and Subquery = Big + Indexed NOT EXISTS vs NOT IN \u2022 NOT EXISTS is not equivalent of NOT IN. \u2022 NOT EXISTS cannot be used instead of NOT IN all the times. \u2022 More specifically, if there is any NULL value in your data, they will show different result. \u2022 If your subquery returns even one NULL value, NOT IN will not match any rows. \u2022 On other hand, if you have a chance to use NOT EXISTS instead of NOT IN, you should test it. \u2022 In most database versions of oracle, EXISTS and IN are treated similarly in terms of execution plan.","title":"Tip 6: Use of EXISTS and IN Operators"},{"location":"SQL/docs/sql_performance_tuning/#tip-7-try-to-use-union-all-instead-of-union","text":"UNION scans all data first and then eliminate duplicate so it has slow performance. Not Recommended Select * from Employee where dept_no=10 UNION Select * from Employee where dept_no=20; Recommended Select * from Employee where dept_no=10 UNION ALL Select * from Employee where dept_no=20;","title":"Tip 7: Try to use UNION ALL instead of UNION"},{"location":"SQL/docs/sql_performance_tuning/#tip-8-avoid-use-of-functions-in-where-condition","text":"Not Recommended Select * from Employee where Substr(name,1,3)=\u2019Ami\u2019; Recommended Select * from Employee where name like \u2018Ami%\u2019;","title":"Tip 8: Avoid use of Functions in Where condition."},{"location":"SQL/docs/sql_performance_tuning/#tip-9-convert-or-to-and","text":"If we use OR clause, it will PREVENT index usages. Instead, we should use AND where possible. Not Recommended select * from sales where prod_id = 13 or promo_id=14; Recommended select * from sales where prod_id = 13 UNION All select * from sales where promo_id=14 AND prod_id <> 13;","title":"Tip 9: convert OR to AND"},{"location":"SQL/docs/sql_performance_tuning/#tip-10-subquery-unnesting","text":"Nested queries are very costly, and so transformer tries to convert them to equivalent join statements. Not Recommended select * from sales where cust_id IN (select cust_id from customers); Recommended select sales.* from sales, customers where sales.cust_id=customers.cust_id;","title":"Tip 10: Subquery Unnesting"},{"location":"SQL/docs/sql_performance_tuning/#tip-11-in-and-between","text":"select * from employees where emp_id in (2,3,4,5); The above is equivalent to select * from employees where emp_id = 2 OR emp_id=3 OR emp_id=4 OR emp_id=5 ---this implies full table scan. Solution - select * from employees where emp_id between 2 and 5;","title":"Tip 11: IN and BETWEEN"},{"location":"SQL/docs/sql_performance_tuning/#tip-12-fetching-first-n-records","text":"Suppose we want to see only 10 records in our select statement output. There are 2 ways to do this \u2013 Using rownum (Recommended) SELECT * FROM EMPLOYEE where rownum<11; Using fetch first (not recommended) SELECT * FROM EMPLOYEE FETCH FIRST 10 ROWS ONLY; In case of rownum- Here it reads first 10 rows use count STOPKEY operator, and so faster than fetch first method. In case of fetch first \u2013 Here we read whole table, and then applied a windowing function to select 1st 10 records.","title":"Tip 12: Fetching first N records"},{"location":"SQL/docs/sql_performance_tuning/#tip-13-union-vs-union-all","text":"UNION \u2013 combines data and drops duplicate rows. UNION ALL \u2013 combines data and retains duplicate rows. Suggest: Some key points- \u2022 by default, UNION ALL is less costly than UNION, as latter sorts data internally to remove duplicates. \u2022 But if your table is indexed, then sort operation in UNION wont be that costly, and so you can use UNION also. So \u2013 - Use UNION if table is indexed and you don\u2019t want duplicates in output. - Use UNION ALL if\u2013 - There is no duplicate in your data, or - You are ok with having duplicate data in output. - But overall, UNION ALL gives better performance than UNION.","title":"Tip 13: UNION vs UNION ALL:"},{"location":"SQL/docs/sql_performance_tuning/#tip-14-intersect-vs-exists-operator","text":"Intersect gives common rows of 2 intersection in a Sorted order. As part of intersect, 2 rows sources are first sorted, and then common records are fetched. In place of INTERSECT operator, we should try and use EXISTS clause, which is more efficient. One caveat is that, in case of EXISTS clause, output is not sorted, unlike in case of INTERSECT clause. Not Recommended SELECT employee_id FROM employees where employee_id between 145 and 179 INTERSECT SELECT employee_id FROM employees WHERE first_name LIKE 'A%'; Recommended SELECT employee_id FROM employees e where employee_id between 145 and 179 and exists (SELECT employee_id FROM employees t WHERE first_name LIKE 'A%' and e.employee_id = t.employee_id);","title":"Tip 14: INTERSECT Vs EXISTS operator"},{"location":"SQL/docs/sql_performance_tuning/#tip-15-minus-vs-not-exists","text":"MINUS operator eliminates matched rows of 1st (with 2nd) and returns rest of the rows of 1st. NOT EXISTS clause can also do same work as MINUS, but has much better performance. Not Recommended SELECT employee_id FROM employees where employee_id between 145 and 179 MINUS SELECT employee_id FROM employees WHERE first_name LIKE 'A%'; Recommended SELECT employee_id FROM employees e where employee_id between 145 and 179 and not exists (SELECT employee_id FROM employees t WHERE first_name LIKE 'A%' and e.employee_id = t.employee_id);","title":"Tip 15: MINUS Vs NOT EXISTS"},{"location":"SQL/docs/sql_performance_tuning/#tip-16-using-like-conditions","text":"To enable use of indexes, avoid use of wild card character at beginning of source text scan. If you are forced to use wild card character at beginning, you can create reverse index, and handle that problem using that index. In this case, when you reverse index, then source text will eliminate use of wild card at beginning. Eg \u2013 Suppose you want to find records where last name ends is \u201chhar\u201d, then create reverse() index on last_name to reverse it and then use condition where reversed last name begins with \u201crahh\u201d. Though reverse() index usage will have cost, but if your column is selective enough, it wont be much cost.","title":"Tip 16: Using Like conditions"},{"location":"SQL/docs/sql_performance_tuning/#tip-17-using-functions-on-indexed-columns-will-suppress-index-usage","text":"Use of function on indexed column will suppress index usage. So, rewrite query to avoid use of function. BAD QUERY select employee_id, first_name, last_name from employees where trunc(hire_date,'YEAR') = '01-JAN-2002'; GOOD: rewritten query select employee_id, first_name, last_name from employees where hire_date between '01-JAN-2002' and '31-DEC-2002'; Eg2 - Bad select * from mytable where substr(emp_name,1,2) = 'Po'; Good select * from mytable where emp_name like 'Po%'; Note - In Spark, we have HashAggregates and SortAggregates. Hash Aggregates are more performant, and work only on mutable data types. That is, if all elements in your Select clause (except those in Group by clause) are mutable types like INT, FLOAT, etc, then spark will use Hash Aggregates. This means, sometimes, for performance gain, we need to apply Function to transform values. See #14 at below link for details https://github.com/kushal-luthra/spark-development/blob/master/notes/spark_opimization.md","title":"Tip 17: Using Functions on Indexed Columns will suppress index usage."},{"location":"SQL/docs/sql_performance_tuning/#tip-18-handling-null-values","text":"Failing to deal with NULL values will lead to unintentional results or performance losses. Why - \u2022B-Tree indexes do not index NULL values. \u2022If there are any NULL values in your indexed columns and you need to get rows which have NULL values, optimizer will not use your index, and perform a full table scan instead. \u2022That is, having Null values in your index may sometimes suppress index usage. Ways to handle NULL value-based performance loss a. Use IS NOT NULL condition in your WHERE clause. Use IS NOT NULL condition in your WHERE clause if you don\u2019t need to have NULL values in result set. That is, even if you now your result will not be having any NULL values, you should use \u201cis not null \u201c clause to make optimizer use indexes. Eg \u2013 Query 1: select emp_name, emp_id from employee where emp_id <> 1; Query 2: select emp_name, emp_id from employee where emp_id <> 1 and emp_id is not null; In query 1, we will see FULL Table scan and in case of query 2, we see index-based scan, and lower query cost. b. Adding not null constraint to your columns and insert a specific value for NULL values like \u20180\u2019 if value in a column is null. c. If reasonable, create a BITMAP index instead of B-Tree index. BITMAP indexes store NULL values. So even if there are null values in our column, optimizer will be able to use our indexes. However, you need to take into consideration index efficiency between B-Tree and BITMAP, as former as more efficient than latter. - We use BITMAP indexes when \u2013 cardinality is LOW and index not modified often. - We use B-Tree index when \u2013 cardinality/selectivity is high.","title":"Tip 18: Handling NULL Values"},{"location":"SQL/docs/sql_performance_tuning/#tip-19-use-truncate-instead-of-delete","text":"Truncate is always faster than DELETE command. This is because when you run delete command, oracle database generates lots of UNDO data for deleted rows and generating UNDO data is an expensive operation. Truncate doesn\u2019t generate UNDO Data. But before using Truncate command, there are few things to note about it- \u2022 No rollback \u2013 Truncate operation cannot be rollbacked, and Flashback is also not so easy after truncate operations. You may need to use Flashback Data Archive or some other external tools in this case. \u2022 Truncate is a DDL operation \u2013 So when you run Truncate, your transaction will be committed. It performs commit before and after Truncate operation. Since it does 2 commits, and even if truncate operation fails in between, the changes you did before will be permanent in any case. \u2022 Truncate a partition - We don\u2019t need to truncate whole table all the times. You can truncate partition as well. \u2022 Truncate doesn\u2019t fire DML triggers - So you wont be able to log your truncate operation because of that. But it can fire the DDL triggers. \u2022 Truncate makes unusable indexes usable again. But delete does not.","title":"Tip 19 : Use Truncate instead of Delete"},{"location":"SQL/docs/sql_performance_tuning/#tip-20-data-type-mismatches","text":"If data types of column and compared value dont match, this may suppress index usage. select cust_id, cust_code from customers where cust_code = 101; Vs select cust_id, cust_code from customers where cust_code = '101';","title":"Tip 20: Data Type Mismatches"},{"location":"SQL/docs/sql_performance_tuning/#tip-21-tuning-ordered-queries-order-by-clause","text":"Order by mostly requires SORT operations . This sort operation is done in PGA or in disk (if PGA doesn\u2019t have enough memory) This disk is shown as \u2018temporary table space\u2019 in execution plan. Issue \u2013 sorting in disk is a costly operation. Solution \u2013 \u2022 Create a B-Tree index on column used in Order by Clause, or \u2022 Modify a B-Tree index to include column used in Order by Clause. \u2022 Why\u2013> B-Tree indexes store columns in Order and using B-Tree index will eliminate sort operations.","title":"Tip 21: Tuning Ordered queries- Order By clause"},{"location":"SQL/docs/sql_performance_tuning/#tip-22-retrieving-min-and-max-values","text":"B-Tree indexes increase the performance a lot for min and max value searches. If no B-Tree index, optimizer will need to read whole table. If our query has another column or another aggregate function in your query, it will be reading whole index or whole table. For example- When you see below, if we are looking for min() and max() values individually, output is just 2 for each. But when we want to get min() and max() together, database will read full table, and hence cost is 8 times. This is coz we have 2 aggregate functions in our query. Bad - select min(), max() from mytable; Good - select * FROM (select min() from mytable) min_cust, (select max() from mytable) max_cust;","title":"Tip 22 : Retrieving MIN and MAX Values"},{"location":"SQL/docs/sql_performance_tuning/#tip-23-views","text":"Simple view = view created from single table. Complex view = view created by using multiple tables. Some suggestions w.r.t. views- 1. If you don\u2019t need to use all the tables in a view, then don\u2019t use the view. Instead use the actual tables. Otherwise, server will have to join all tables, do aggregation etc on them for a view. i.e. use view for the purpose for which it was created. 2. Else create another view. 3. Don\u2019t join complex views with a table or another view - This is because most of the times view is first executed completely at first, and then result is used as row source to other table or view. So, in this case you be reading lots of unnecessary data and performing unnecessary join and group by. This will increase cost a lot. 4. Avoid performing outer join to the views \u2013 because if you use equality predicate on view column, the optimizer gets wrong if the table of that column has an outer join in the view as well. Because outer join may not know performance of view and may lead to bad execution as well. E.g. \u2013 if we do outer join, optimizer may not be able to push predicate inside the view definition at times of execution plan. Materialized Views- Unlike basic and complex views, materialized views store both query and data. Materialized view data can be refreshed manually or via auto-refresh. But materialized view maintenance is a burden to database \u2013 it needs to be kept up to date for each modification on each change. As compared to normal views, materialized view will improve performance as we will select data directly from materialized view, and there will be no sorts, joins etc. We can create index, partitions etc on materialized view like in an ordinary table. Summary \u2022 If you don\u2019t need to use all the tables in a view, then don\u2019t use the view. Instead use the actual tables. \u2022 Don\u2019t join complex views with a table or another view. \u2022 Avoid performing outer join to the views. \u2022 Use Materialized View - Unlike basic and complex views, materialized views store both query and data. Materialized view data can be refreshed manually or via auto-refresh. But materialized view maintenance is a burden to database .","title":"Tip 23 : Views"},{"location":"SQL/docs/sql_performance_tuning/#tip-24-frequent-commit-is-not-desired","text":"make REDO logs bulky as we may be committing prior to period. make lock on modified rows, making them unavailable to other applications.","title":"Tip 24: Frequent commit is not desired"},{"location":"SQL/docs/sql_performance_tuning/#tip-25-multitable-dml-operations-skip-for-big-data","text":"Sometimes we have to read same table as input to different tables in our data warehouse. So, if we have 5 different tables requiring input from 1 table, we should ideally be reading input table just once, and keep on feeding into different output tables as per requirements. For this we have 2 options \u2013 \u2022 INSERT ALL \u2022 MERGE INTO","title":"Tip 25: Multitable DML operations (skip for big data)"},{"location":"SQL/docs/sql_performance_tuning_summary/","text":"SQL Performance Tuning : Summary Tip 1: Never use *(Star) to fetch all records from table Tip 2: Try to avoid DISTINCT keyword from the query Not Recommended: SELECT DISTINCT d.dept_no, d.department_name FROM Department d,Employee e WHERE d.dept_no= e.dept_no; Recommended: SELECT d.dept_no d.department_name FROM Department d WHERE EXISTS ( SELECT \u2018X\u2019 FROM Employee e WHERE d.dept_no= e.dept_no); Tip 3: Carefully use WHERE conditions in sql. Tip 4: Use Like operator instead of equal to (=) Tip 5: Avoid HAVING clause/GROUP BY statements Tip 6: Use of EXISTS and IN Operators Tip 7: Try to use UNION ALL instead of UNION as UNION scans all data first and then eliminate duplicate so it has slow performance. Tip 9: convert OR to AND Tip 10: Subquery Unnesting Tip 11: IN and BETWEEN Tip 12: Fetching first N records: SELECT * FROM EMPLOYEE where rownum<11 Tip 13: UNION vs UNION ALL: \u2022 by default, UNION ALL is less costly than UNION, as latter sorts data internally to remove duplicates. \u2022 But if your table is indexed, then sort operation in UNION wont be that costly, and so you can use UNION also. Tip 14: INTERSECT Vs EXISTS operator Tip 15: MINUS Vs NOT EXISTS Tip 16: Using Like conditions To enable use of indexes, avoid use of wild card character at beginning of source text scan. If you are forced to use wild card character at beginning, you can create reverse index, and handle that problem using that index. In this case, when you reverse index, then source text will eliminate use of wild card at beginning. Tip 17: Using Functions on Indexed Columns will suppress index usage. So, rewrite query to avoid use of function. BAD QUERY select employee_id, first_name, last_name from employees where trunc(hire_date,'YEAR') = '01-JAN-2002'; GOOD: rewritten query select employee_id, first_name, last_name from employees where hire_date between '01-JAN-2002' and '31-DEC-2002'; Eg2 - Bad select * from mytable where substr(emp_name,1,2) = 'Po'; Good select * from mytable where emp_name like 'Po%'; Tip 18: Handling NULL Values \u2022B-Tree indexes do not index NULL values. \u2022If there are any NULL values in your indexed columns and you need to get rows which have NULL values, optimizer will not use your index, and perform a full table scan instead. \u2022That is, having Null values in your index may sometimes suppress index usage. Solution - - Use IS NOT NULL condition in your WHERE clause. - Adding not null constraint to your columns and insert a specific value for NULL values like \u20180\u2019 if value in a column is null. - If reasonable, create a BITMAP index instead of B-Tree index. Tip 19 : Use Truncate instead of Delete Tip 20: Data Type Mismatches If data types of column and compared value dont match, this may suppress index usage. Tip 21: Tuning Ordered queries- Order By clause Order by mostly requires sort operations. This sort operation is done in PGA or in disk (if PGA doesn\u2019t have enough memory) This disk is shown as \u2018temporary table space\u2019 in execution plan. Issue \u2013 sorting in disk is a costly operation. Solution \u2013 \u2022 Create a B-Tree index on column used in Order by Clause, or \u2022 Modify a B-Tree index to include column used in Order by Clause. \u2022 Why \u2013 B-Tree indexes store columns in Order and using B-Tree index will eliminate sort operations. Tip 22 : Retrieving MIN and MAX Values B-Tree indexes increase the performance a lot for min and max value searches. If no B-Tree index, optimizer will need to read whole table. Bad - select min(), max() from mytable; Good - select * FROM (select min() from mytable) min_cust, (select max() from mytable) max_cust; Tip 23 : Views \u2022 If you don\u2019t need to use all the tables in a view, then don\u2019t use the view. Instead use the actual tables. \u2022 Don\u2019t join complex views with a table or another view. \u2022 Avoid performing outer join to the views. \u2022 Unlike basic and complex views, materialized views store both query and data. Materialized view data can be refreshed manually or via auto-refresh. But materialized view maintenance is a burden to database . Tip 24: Frequent commit is not desired: make REDO logs bulky as we may be committing prior to period. make lock on modified rows, making them unavailable to other applications.","title":"SQL Performance Tuning Summary"},{"location":"SQL/docs/sql_performance_tuning_summary/#sql-performance-tuning-summary","text":"","title":"SQL Performance Tuning : Summary"},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-1-never-use-star-to-fetch-all-records-from-table","text":"","title":"Tip 1: Never use *(Star) to fetch all records from table"},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-2-try-to-avoid-distinct-keyword-from-the-query","text":"Not Recommended: SELECT DISTINCT d.dept_no, d.department_name FROM Department d,Employee e WHERE d.dept_no= e.dept_no; Recommended: SELECT d.dept_no d.department_name FROM Department d WHERE EXISTS ( SELECT \u2018X\u2019 FROM Employee e WHERE d.dept_no= e.dept_no);","title":"Tip 2: Try to avoid DISTINCT keyword from the query "},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-3-carefully-use-where-conditions-in-sql","text":"","title":"Tip 3: Carefully use WHERE conditions in sql. "},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-4-use-like-operator-instead-of-equal-to","text":"","title":"Tip 4: Use Like operator instead of equal to (=)"},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-5-avoid-having-clausegroup-by-statements","text":"","title":"Tip 5: Avoid HAVING clause/GROUP BY statements"},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-6-use-of-exists-and-in-operators","text":"","title":"Tip 6: Use of EXISTS and IN Operators"},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-7-try-to-use-union-all-instead-of-union-as-union-scans-all-data-first-and-then-eliminate-duplicate-so-it-has-slow-performance","text":"","title":"Tip 7: Try to use UNION ALL instead of UNION as UNION scans all data first and then eliminate duplicate so it has slow performance."},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-9-convert-or-to-and","text":"","title":"Tip 9: convert OR to AND"},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-10-subquery-unnesting","text":"","title":"Tip 10: Subquery Unnesting"},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-11-in-and-between","text":"","title":"Tip 11: IN and BETWEEN"},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-12-fetching-first-n-records-select-from-employee-where-rownum11","text":"","title":"Tip 12: Fetching first N records:  SELECT * FROM EMPLOYEE where rownum&lt;11"},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-13-union-vs-union-all","text":"\u2022 by default, UNION ALL is less costly than UNION, as latter sorts data internally to remove duplicates. \u2022 But if your table is indexed, then sort operation in UNION wont be that costly, and so you can use UNION also.","title":"Tip 13: UNION vs UNION ALL:"},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-14-intersect-vs-exists-operator","text":"","title":"Tip 14: INTERSECT Vs EXISTS operator"},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-15-minus-vs-not-exists","text":"","title":"Tip 15: MINUS Vs NOT EXISTS"},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-16-using-like-conditions","text":"To enable use of indexes, avoid use of wild card character at beginning of source text scan. If you are forced to use wild card character at beginning, you can create reverse index, and handle that problem using that index. In this case, when you reverse index, then source text will eliminate use of wild card at beginning.","title":"Tip 16: Using Like conditions"},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-17-using-functions-on-indexed-columns-will-suppress-index-usage","text":"So, rewrite query to avoid use of function. BAD QUERY select employee_id, first_name, last_name from employees where trunc(hire_date,'YEAR') = '01-JAN-2002'; GOOD: rewritten query select employee_id, first_name, last_name from employees where hire_date between '01-JAN-2002' and '31-DEC-2002'; Eg2 - Bad select * from mytable where substr(emp_name,1,2) = 'Po'; Good select * from mytable where emp_name like 'Po%';","title":"Tip 17: Using Functions on Indexed Columns will suppress index usage."},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-18-handling-null-values","text":"\u2022B-Tree indexes do not index NULL values. \u2022If there are any NULL values in your indexed columns and you need to get rows which have NULL values, optimizer will not use your index, and perform a full table scan instead. \u2022That is, having Null values in your index may sometimes suppress index usage. Solution - - Use IS NOT NULL condition in your WHERE clause. - Adding not null constraint to your columns and insert a specific value for NULL values like \u20180\u2019 if value in a column is null. - If reasonable, create a BITMAP index instead of B-Tree index.","title":"Tip 18: Handling NULL Values"},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-19-use-truncate-instead-of-delete","text":"","title":"Tip 19 : Use Truncate instead of Delete"},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-20-data-type-mismatches","text":"If data types of column and compared value dont match, this may suppress index usage.","title":"Tip 20: Data Type Mismatches"},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-21-tuning-ordered-queries-order-by-clause","text":"Order by mostly requires sort operations. This sort operation is done in PGA or in disk (if PGA doesn\u2019t have enough memory) This disk is shown as \u2018temporary table space\u2019 in execution plan. Issue \u2013 sorting in disk is a costly operation. Solution \u2013 \u2022 Create a B-Tree index on column used in Order by Clause, or \u2022 Modify a B-Tree index to include column used in Order by Clause. \u2022 Why \u2013 B-Tree indexes store columns in Order and using B-Tree index will eliminate sort operations.","title":"Tip 21: Tuning Ordered queries- Order By clause"},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-22-retrieving-min-and-max-values","text":"B-Tree indexes increase the performance a lot for min and max value searches. If no B-Tree index, optimizer will need to read whole table. Bad - select min(), max() from mytable; Good - select * FROM (select min() from mytable) min_cust, (select max() from mytable) max_cust;","title":"Tip 22 : Retrieving MIN and MAX Values"},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-23-views","text":"\u2022 If you don\u2019t need to use all the tables in a view, then don\u2019t use the view. Instead use the actual tables. \u2022 Don\u2019t join complex views with a table or another view. \u2022 Avoid performing outer join to the views. \u2022 Unlike basic and complex views, materialized views store both query and data. Materialized view data can be refreshed manually or via auto-refresh. But materialized view maintenance is a burden to database .","title":"Tip 23 : Views"},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-24-frequent-commit-is-not-desired","text":"make REDO logs bulky as we may be committing prior to period. make lock on modified rows, making them unavailable to other applications.","title":"Tip 24: Frequent commit is not desired:"},{"location":"random/cold_mails/","text":"Cold Mails Tactics \ud83d\udfe2 Run the triple: Start every sequence with an email, social touch, and cold call. \ud83d\udfe2 5x5x5: 5 minutes of research. Find 5 insights on the person/account. Write your message in 5 minutes. \ud83d\udfe2 Subject lines shouldn\u2019t read like an advertisement. They should be 4 words max, not a complete sentence, and without punctuation. \ud83d\udfe2 Don't equate formality with professionalism. You don't need to address me as \"Dear Mr. Farrokh\"). Use conjunctions and conversational language \ud83d\udfe2 Follow the 3x3 Rule for email: 3 paragraphs, no more than 3 lines each when read on your phone. \ud83d\udfe2 Don't try to sell everything under the sun in your email. One pain point per email, or you risk losing them. Mix in multiple pain points across the sequence. \ud83d\udfe2 Avoid large images, multiple hyperlinks, and big attachments. These add-ons scream \"marketing email\" and hurt deliverability. \ud83d\udfe2 Use Interest-based CTAs. \"Open to learning more?\" is better than \u201cHow's Thursday at 4:00 for a discovery call?\u201d \ud83d\udfe2 Your emails should read like text messages. If you read it out loud and sound like a robot, you\u2019re doing it wrong. \ud83d\udfe2 Tie VMs to emails: Leave voicemails that reference your emails and vice-versa. Even if they never get answered, voicemails should boost email replies. Credits : Armand Farokh","title":"Cold Mail Tactics"},{"location":"random/cold_mails/#cold-mails-tactics","text":"\ud83d\udfe2 Run the triple: Start every sequence with an email, social touch, and cold call. \ud83d\udfe2 5x5x5: 5 minutes of research. Find 5 insights on the person/account. Write your message in 5 minutes. \ud83d\udfe2 Subject lines shouldn\u2019t read like an advertisement. They should be 4 words max, not a complete sentence, and without punctuation. \ud83d\udfe2 Don't equate formality with professionalism. You don't need to address me as \"Dear Mr. Farrokh\"). Use conjunctions and conversational language \ud83d\udfe2 Follow the 3x3 Rule for email: 3 paragraphs, no more than 3 lines each when read on your phone. \ud83d\udfe2 Don't try to sell everything under the sun in your email. One pain point per email, or you risk losing them. Mix in multiple pain points across the sequence. \ud83d\udfe2 Avoid large images, multiple hyperlinks, and big attachments. These add-ons scream \"marketing email\" and hurt deliverability. \ud83d\udfe2 Use Interest-based CTAs. \"Open to learning more?\" is better than \u201cHow's Thursday at 4:00 for a discovery call?\u201d \ud83d\udfe2 Your emails should read like text messages. If you read it out loud and sound like a robot, you\u2019re doing it wrong. \ud83d\udfe2 Tie VMs to emails: Leave voicemails that reference your emails and vice-versa. Even if they never get answered, voicemails should boost email replies. Credits : Armand Farokh","title":"Cold Mails Tactics"}]}