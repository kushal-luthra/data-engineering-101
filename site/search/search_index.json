{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Data Engineering - 101 Index SQL SQL Performance Tuning Page SQL Performance Tuning Summary SQL Analytical Functions SQL Practise Questions Concepts Data Warehousing Concepts Career Development Cold Mail Tactics About Me Author : Kushal Luthra","title":"Home"},{"location":"#welcome-to-data-engineering-101","text":"","title":"Welcome to Data Engineering - 101"},{"location":"#index","text":"SQL SQL Performance Tuning Page SQL Performance Tuning Summary SQL Analytical Functions SQL Practise Questions Concepts Data Warehousing Concepts Career Development Cold Mail Tactics About Me Author : Kushal Luthra","title":"Index"},{"location":"aboutme/","text":"About me Kushal is currently Lead Engineer at Airtel Africa Digital Labs (BigData & Analytics Team). He is the Lead for Business Intelligence product, which is being built from scratch, and is aimed to be scalable (handles data that can be in Gigabytes and Terabytes) and replicable across 14 OpCos. He has extensive experience that spans across various technologies, including Python, Spark(PySpark), SQL, Hive, Hadoop, Apache Hudi, Airflow, Sqoop, Gitlab, BItBucket, CICD. He also has experience of migrating Data Solutions on legacy system to Big Data Stack. He has built data pipelines from scratch, with focus on data frameworks. Domain : Retail and Telecom Distributed Computing: Hadoop, HDFS, Yarn, Spark Programming Languages: Python, Scala Operating System: Linux, Unix Development Tools: JIRA Databases: Postgres, MongoDB, Oracle Exadata Methodologies: Agile/Scrum Open to hearing about exciting information/opportunities in meaningful industries, more tech connections and both mentor/mentee relationships.","title":"About Me"},{"location":"SQL/docs/Data-Warehousing-basics/","text":"Data Warehouse What is Data Warehouse? Put simply, it\u2019s a central place where data is stored for the purpose of analysis and reporting. The data may be collected from a variety of sources. It\u2019s organised to provide complete data for users that can be easily understood in a business context. Different from databases in that it\u2019s purpose is for analysis -In companies, we use dimensional modelling to design our data warehouses. Dimensional modelling always uses two types of tables \u2013 facts and dimensions. a. Fact Tables- Contain the measurements, metrics or facts of a business process i.e. Transactions (items & baskets) Each fact will include measures (e.g. spend) and context data. Describe the measurements/facts in a transaction table \u2013 what was bought/how much it cost, etc. b. Dimension Tables- Stores attributes that describe the objects in a fact table i.e. Stores, Products, Customers They are linked together \u2013 this is a relational model - a. Primary key - Is the attribute or a set of attributes in an entity whose value(s) guarantee only one tuple (row) exists for each value. The fact table will also include foreign keys that relate each fact to our dimension tables. b. Foreign key The primary key of another table referenced here. Each entity in a dimension table will contain an attributes that describe that entity. There will also be a key that is used to join the dimension table to the fact table. Characteristics of Data warehouse Data warehouse is a database which is separate from operational database which stores historical information also. Data warehouse database contains transaction(OLTP) as well as analytical data(OLAP). Data warehouse helps higher management to take strategic as well as tactical decisions using historical or current data. Data warehouse helps consolidated historical data analysis. Data warehouse helps business user to see the current trends to run the business. Data warehouse is used for reporting and data analysis purpose. Types of Data warehouse systems a. Data Mart Data Mart is a simplest set of Data warehouse which is used to focus on single functional area of the business. b. Online Analytical Processing (OLAP) - Refer OLAP is used at strategic level and contains aggregated data, covering number of years. The key purpose to use OLAP system is to reduce the query response time and increase the effectiveness of reporting. So, data is denormalized. OLAP uses Star-Schema,Snowflakes schema or Fact-Dimensions. OLAP database stores aggregated historical data in multidimensional schema. eg - summaries data. c. Online Transactional Processing (OLTP) - Refer It is operational database, maintaining large number of small daily transactions like insert,update and delete. Data is normailzed. OLTP uses Entity Relations. OLTP system maintains concurrency and it avoids the centralization so as to avoid the single point of failures. Data concurrency and integrity = focus. d. Predictive Analysis Difference between Data Warehouse and Data Mart a. Definition - The Data Warehouse is a large repository of data collected from different organizations or departments within a corporation. The data mart is an only sub-type of a Data Warehouse. It is designed to meet the need of a certain user group. b. Focus - Data warehouse focuses on multiple business areas. Data mart focuses only on single subject area. c. Usage - DW - It helps to take a strategic decision. DM - The data mart is used to take tactical decisions for growth of business. d. Type of system - DW - This is centralized system where one fact is at center surrounded by dimension tables. DM - Data mart system is de-centralized system e. data model - DW = top down DM - bottom up f. source - Data warehouse data comes from multiple heterogeneous data sources. Data mart data is data of only one business area.Many times it will come from only one data source. g. Implementation Time - Data warehouse contains all data which will come from multiple data sources. It will take time to build data warehouse. The Time to build data warehouse is months to years. Data mart is small data warehouse which will contain the data of only a single business area. The implementation time to build data mart is in months. Data Warehousing -It\u2019s the process of TRANSFORMING data into information and making it available to users in a TIMELY enough manner to make a difference. Different use cases of ETL a. Data Warehousing - User needs to fetch the historical data as well as current data for developing data warehouse. The Data warehouse data is nothing but combination of historical data as well as transactional data. Its data sources might be different.User needs to fetch the data from multiple heterogeneous systems and load it in to single target system which is also called as data warehouse. b. Data Migration ETL tools are widely used in data migration projects. If the organization is managing the data in oracle 10 g previously and now organization wants to go for SQL server cloud database then there is need to migrate the data from Source to Target. To do this kind of migration the ETL tools are very useful. If user wants to write the code of ETL it is very time consuming process. To make this simple the ETL tools are very useful in which the coding is simple as compare to PL SQL or T-SQL code. So ETL process i very useful in Data migration projects. c. Data Integration Now a days big organizations are acquiring small firms. Obviously the data source for the different organizations may be different.We need to integrate the data from one organization to other organization. These kind of integration projects need the ETL process to extract the data,transform the data and load the data. d. Third Party data management- many a times company outsources process to different vendors. eg - telecom - billing managed by one and CRM by other vendor. If CRM company needs some data from the company who is managing the Billing. That company will receive a data feed from the other company. To load the data from the feed ETL process is used. ETL Extract Extract source data from our client- Once the data we will receive has been agreed, it is transferred from the client to us via our secure FTP system called Axway. This data is known as source data. Once the data has been received, it is validated according to the retailer-specific rules which are outlined in the DIS. If the data does not reflect what is outlined in the DIS changes may need to be made \u2013 either by updating the DIS or requesting a resupply. Once we agree the data is in the correct format it can be read in. Data Extraction can be - Full or Partial (Delta). DIS - The data we receive is mapped in a document known as Data Interface Specification (DIS). QA check in Source / RAW layer- A key section of RAW is quality assurance (QA). We carry out standard checks to ensure data is \u201chealthy\u201d and without errors Main focus of the checks is the fact tables, and relation on the fact data with the key dimension tables. Checks include: * Number of baskets in the basket and item tables * Levels of spend * Missing foreign/primary keys Any issues found are recorded and can either be resolved with the solution DSG or may require input from the client Once these checks are successfully completed the build moves forward into the PREP stage. Transform Transform this client data to meet the operational and business needs of our internal database. Within prep we transform the client data into a standardized format within the guidelines of Marketplace What types of transformations do we perform? i. Reject bad data - Bad data i.e. record where key info is missing. What is rejected depends on the business rules. We keep a record of rejected data by extracting them to a separate table, mark the missing field. ii. Remove duplicates Duplicate data can have negative impact on results. Important to understand if it is really a duplicate before removing. iii. Convert fields Convert from character to date and numeric fields where relevant e.g. spends/quantities. iv. Text manipulations Changes format e.g. change lookup value to descriptive form. v. Merges with other tables Merge lookups/useful fields that should be on specific table. vi. Aggregate data Sometimes need to roll up products in same basket or even create basket table - involves summations. vii. Rename fields Rename to make more meaningful \u2013 esp if in a foreign language. viii. Create standard fields Essential to marketplace, same naming conventions e.g. dib_bask_code. Load Load into our analytical data mart within Marketplace The load is automated, so you will not be expected to know exactly what occurs. Here is an overview: Inbound Outbound -extracts and updates required data into standard structure Staging - manage slowly changing dimensions, generate surrogate keys and *create skeleton records SCD - a dimension is considered a SCD when its attributes remain almost constant over time, requiring relatively minor alterations to represent the evolved state. Surrogate Keys - system-generated and non-persistent integer keys which replace foreign keys. Skeleton records - Generated when a foreign key in a fact table does not have a match in the dimension table. A dummy or \u2018skeleton\u2019 record is created in the dimension table. There are following 3 Types of Data Loading Strategies : i. Initial load : Populating all the data tables from source system and loads it in to data warehouse table. ii. Incremental Load : Applying the ongoing changes as necessary in periodic manner. iii. Full Refresh : Completely erases the data from one or more tables and reload the fresh data. Star and Snowflake schema Star Schema In the star schema design, a single object (the fact table) sits in the middle and is radically connected to other surrounding objects (dimension lookup tables) like a star. Each dimension is represented as a single table. The primary key in each dimension table is related to a foreign key in the fact table. All measures in the fact table are related to all the dimensions that fact table is related to. In other words, they all have the same level of granularity. A star schema can be simple or complex. A simple star consists of one fact table; a complex star can have more than one fact table. Snowflake Schema It is an extension of star schema. In a star schema, each dimension is represented by a single dimensional table, whereas in a snowflake schema, that dimensional table is normalized into multiple lookup tables, each representing a level in the dimensional hierarchy. Adv - improvement in query performance due to minimized disk storage requirements and joining smaller lookup tables. Disadvantage- additional maintenance efforts needed due to the increase number of lookup tables. Fact Table Granularity The first step in designing a fact table is to determine the granularity of the fact table. By granularity, we mean the lowest level of information that will be stored in the fact table. This constitutes two steps: i. Determine which dimensions will be included - this depends on business process being targetted. ii. Determine where along the hierarchy of each dimension the information will be kept - This depends on requirements. Eg - if client wants hourly reports, then fact table will keep hour as lowest level of granularity. If daily reports are fine, then date_id is lowest level of granularity. The determining factors usually goes back to the requirements. Fact And Fact Table Types There are three types of facts: i. Additive: Additive facts are facts that can be summed up through all of the dimensions in the fact table. ii. Semi-Additive: Semi-additive facts are facts that can be summed up for some of the dimensions in the fact table, but not the others. iii. Non-Additive: Non-additive facts are facts that cannot be summed up for any of the dimensions present in the fact table. eg1 - Additive Fact - Consider a retailer fact table with following columns - - Date - Store - Product - Sales_Amount The purpose of this table is to record the sales amount for each product in each store on a daily basis. Sales_Amount is an additive fact, because you can sum up this fact along any of the three dimensions present in the fact table -- date, store, and product. eg2A - Semi-Additive Fact and Non-Additive Fact - Say we are a bank with the following fact table: - Date - Account - Current_Balance - Profit_Margin The purpose of this table is to record the current balance for each account at the end of each day, as well as the profit margin for each account for each day. Current_Balance and Profit_Margin are the facts. Current_Balance is a semi-additive fact, as it makes sense to add them up for all accounts (what's the total current balance for all accounts in the bank?), but it does not make sense to add them up through time (adding up all current balances for a given account for each day of the month does not give us any useful information). Profit_Margin is a non-additive fact, for it does not make sense to add them up for the account level or the day level. eg 2B - semi -additive - distinct customers who shopped in a day = semi additive. Across all stores, this number can be aggregated. For example, store A has 300 customers and store B has 200 customers. So total 500 customers. But cant add across date dimension. So no summation possible across days in a week. non-additive = %age loyalty transaction in a day. For example, store A has 30% sales as loyalty count, and store B has 40%. But we cant add these two figures to find overall loyalty sales. Based on the above classifications, there are two types of Fact TABLES: * Cumulative: This type of fact table describes what has happened over a period of time. For example, this fact table may describe the total sales by product by store by day. The facts for this type of fact tables are mostly additive facts. The first example presented here is a cumulative fact table. * Snapshot: This type of fact table describes the state of things in a particular instance of time, and usually includes more semi-additive and non-additive facts. The second example presented here is a snapshot fact table. Slowly Changing Dimensions The \"Slowly Changing Dimension\" problem is a common one particular to data warehousing. In a nutshell, this applies to cases where the attribute for a record varies over time. There are in general three ways to solve this type of problem, and they are categorized as follows: Type 1 The new record replaces the original record. No trace of the old record exists. In other words, no history is kept. Advantage - * easiest to handle as no need to maintain history. Disadvantage- *History is lost. Cant track past behavior. So, Type 1 slowly changing dimension should be used when it is not necessary for the data warehouse to keep track of historical changes. Type 2 A new record is added into the customer dimension table. Therefore, the customer is treated essentially as two people. Both the original and the new record will be present. The new record gets its own primary key. Advantages: - This allows us to accurately keep all historical information. Disadvantages: - This will cause the size of the table to grow fast. In cases where the number of rows for the table is very high to start with, storage and performance can become a concern. - This necessarily complicates the ETL process. Type 3 The original record is modified to reflect the change. We add more column to track change. But this is feasible only if changes to be tracked are finite. For example, phone or address changes more than once will complicate things. Data Integrity Data integrity refers to the validity of data, meaning data is consistent and correct. In the data warehousing field, we frequently hear the term, \"Garbage In, Garbage Out.\" If there is no data integrity in the data warehouse, any resulting report and analysis will not be useful. In a data warehouse or a data mart, there are 3 areas of where data integrity needs to be enforced: a. Database level We can enforce data integrity at the database level. Common ways of enforcing data integrity include: i. Referential integrity The relationship between the primary key of one table and the foreign key of another table must always be maintained. For example, a primary key cannot be deleted if there is still a foreign key that refers to this primary key. ii. Primary key / Unique constraint Primary keys and the UNIQUE constraint are used to make sure every row in a table can be uniquely identified. iii. Not NULL vs. NULL-able For columns identified as NOT NULL, they may not have a NULL value. iv. Valid Values Only allowed values are permitted in the database. For example, if a column can only have positive integers, a value of '-1' cannot be allowed. b. ETL process For each step of the ETL process, data integrity checks should be put in place to ensure that source data is the same as the data in the destination. Most common checks include record counts or record sums. c. Access level We need to ensure that data is not altered by any unauthorized means either during the ETL process or in the data warehouse. To do this, there needs to be safeguards against unauthorized access to data (including physical access to the servers), as well as logging of all data access history. Data integrity can only ensured if there is no unauthorized access to the data. 4F. Factless Fact Table A factless fact table is a fact table that does not have any measures. It is essentially an intersection of dimensions. On the surface, a factless fact table does not make sense, since a fact table is, after all, about facts. However, there are situations where having this kind of relationship makes sense in data warehousing. eg1 - student class attendance record. In this case, the fact table would consist of 3 dimensions: the student dimension, the time dimension, and the class dimension. This factless fact table would look like the following: Fact Table \"school_attendance\" date_id classId student_id 02-02-2020 1 101 02-02-2020 1 102 02-02-2020 1 103 ---------------------------------- The only measure that you can possibly attach to each combination is \"1\" to show the presence of that particular combination. However, adding a fact that always shows 1 is redundant because we can simply use the COUNT function in SQL to answer the same questions. eg 2 - online sales in CRV. columns - date_id, store_id, till_id, pos_id In essence it contains only 1 column = basket_key. If a basket is in this table, it means its online sale, else offline sale. eg3 - Promotion data. Table structure could be - date_id | store_id| promo_type| promo_id| basket_key promo_type = Promotion can be online, in-store, flat discount, coupon, voucher, etc. Above table contains info of promotion applied on a basket. No measurable fact exists here. But why needed ? Transaction data contains info of what item was sold on promotion. But promotion data contains information of all the promotion during the purchase period. That is, all products having promotion applied on them, including those which were not sold in spite of promotion. And so, this promotion table becomes pivotal even though it contains no measurable fact. Why need factless facts? Factless fact tables offer the most flexibility in data warehouse design. For example, one can easily answer the following questions with this factless fact table: * How many students attended a particular class on a particular day? * How many classes on average does a student attend on a given day? Without using a factless fact table, we will need two separate fact tables to answer the above two questions. With the above factless fact table, it becomes the only fact table that's needed. Junk Dimension There are columns in Fact table which can have only a few or 2 kind of values - true or false, 1 or 0, etc. eg = bulk Vs non-bulk online vs offline promo vs non-promo vs hybrid sale etc. From business point of view, capturing above info in Fact table is very important. Issue -having these info will only make our fact table bulky and eventually unmanageable. Soln - junk dimension. eg - CRV basket channel seg - shop_channel_code in 0,1,2 or 3 - covers both bulk/non-bulk and online/offline. this would reduce 2 columns in fact table to 1. we can expand scope of above column to include promo info, and in that way we replace 3 fact columns by 1. This will result in a data warehousing environment that offer better performance as well as being easier to manage. [reference] (https://www.1keydata.com/datawarehousing/junk-dimension.html)","title":"Data Warehousing Concepts"},{"location":"SQL/docs/Data-Warehousing-basics/#data-warehouse","text":"","title":"Data Warehouse"},{"location":"SQL/docs/Data-Warehousing-basics/#what-is-data-warehouse","text":"Put simply, it\u2019s a central place where data is stored for the purpose of analysis and reporting. The data may be collected from a variety of sources. It\u2019s organised to provide complete data for users that can be easily understood in a business context. Different from databases in that it\u2019s purpose is for analysis -In companies, we use dimensional modelling to design our data warehouses. Dimensional modelling always uses two types of tables \u2013 facts and dimensions. a. Fact Tables- Contain the measurements, metrics or facts of a business process i.e. Transactions (items & baskets) Each fact will include measures (e.g. spend) and context data. Describe the measurements/facts in a transaction table \u2013 what was bought/how much it cost, etc. b. Dimension Tables- Stores attributes that describe the objects in a fact table i.e. Stores, Products, Customers They are linked together \u2013 this is a relational model - a. Primary key - Is the attribute or a set of attributes in an entity whose value(s) guarantee only one tuple (row) exists for each value. The fact table will also include foreign keys that relate each fact to our dimension tables. b. Foreign key The primary key of another table referenced here. Each entity in a dimension table will contain an attributes that describe that entity. There will also be a key that is used to join the dimension table to the fact table.","title":"What is Data Warehouse?"},{"location":"SQL/docs/Data-Warehousing-basics/#characteristics-of-data-warehouse","text":"Data warehouse is a database which is separate from operational database which stores historical information also. Data warehouse database contains transaction(OLTP) as well as analytical data(OLAP). Data warehouse helps higher management to take strategic as well as tactical decisions using historical or current data. Data warehouse helps consolidated historical data analysis. Data warehouse helps business user to see the current trends to run the business. Data warehouse is used for reporting and data analysis purpose.","title":"Characteristics of Data warehouse"},{"location":"SQL/docs/Data-Warehousing-basics/#types-of-data-warehouse-systems","text":"a. Data Mart Data Mart is a simplest set of Data warehouse which is used to focus on single functional area of the business. b. Online Analytical Processing (OLAP) - Refer OLAP is used at strategic level and contains aggregated data, covering number of years. The key purpose to use OLAP system is to reduce the query response time and increase the effectiveness of reporting. So, data is denormalized. OLAP uses Star-Schema,Snowflakes schema or Fact-Dimensions. OLAP database stores aggregated historical data in multidimensional schema. eg - summaries data. c. Online Transactional Processing (OLTP) - Refer It is operational database, maintaining large number of small daily transactions like insert,update and delete. Data is normailzed. OLTP uses Entity Relations. OLTP system maintains concurrency and it avoids the centralization so as to avoid the single point of failures. Data concurrency and integrity = focus. d. Predictive Analysis","title":"Types of Data warehouse systems"},{"location":"SQL/docs/Data-Warehousing-basics/#difference-between-data-warehouse-and-data-mart","text":"a. Definition - The Data Warehouse is a large repository of data collected from different organizations or departments within a corporation. The data mart is an only sub-type of a Data Warehouse. It is designed to meet the need of a certain user group. b. Focus - Data warehouse focuses on multiple business areas. Data mart focuses only on single subject area. c. Usage - DW - It helps to take a strategic decision. DM - The data mart is used to take tactical decisions for growth of business. d. Type of system - DW - This is centralized system where one fact is at center surrounded by dimension tables. DM - Data mart system is de-centralized system e. data model - DW = top down DM - bottom up f. source - Data warehouse data comes from multiple heterogeneous data sources. Data mart data is data of only one business area.Many times it will come from only one data source. g. Implementation Time - Data warehouse contains all data which will come from multiple data sources. It will take time to build data warehouse. The Time to build data warehouse is months to years. Data mart is small data warehouse which will contain the data of only a single business area. The implementation time to build data mart is in months.","title":"Difference between Data Warehouse and Data Mart"},{"location":"SQL/docs/Data-Warehousing-basics/#data-warehousing","text":"-It\u2019s the process of TRANSFORMING data into information and making it available to users in a TIMELY enough manner to make a difference.","title":"Data Warehousing"},{"location":"SQL/docs/Data-Warehousing-basics/#different-use-cases-of-etl","text":"a. Data Warehousing - User needs to fetch the historical data as well as current data for developing data warehouse. The Data warehouse data is nothing but combination of historical data as well as transactional data. Its data sources might be different.User needs to fetch the data from multiple heterogeneous systems and load it in to single target system which is also called as data warehouse. b. Data Migration ETL tools are widely used in data migration projects. If the organization is managing the data in oracle 10 g previously and now organization wants to go for SQL server cloud database then there is need to migrate the data from Source to Target. To do this kind of migration the ETL tools are very useful. If user wants to write the code of ETL it is very time consuming process. To make this simple the ETL tools are very useful in which the coding is simple as compare to PL SQL or T-SQL code. So ETL process i very useful in Data migration projects. c. Data Integration Now a days big organizations are acquiring small firms. Obviously the data source for the different organizations may be different.We need to integrate the data from one organization to other organization. These kind of integration projects need the ETL process to extract the data,transform the data and load the data. d. Third Party data management- many a times company outsources process to different vendors. eg - telecom - billing managed by one and CRM by other vendor. If CRM company needs some data from the company who is managing the Billing. That company will receive a data feed from the other company. To load the data from the feed ETL process is used.","title":"Different use cases of ETL"},{"location":"SQL/docs/Data-Warehousing-basics/#etl","text":"","title":"ETL"},{"location":"SQL/docs/Data-Warehousing-basics/#extract","text":"Extract source data from our client- Once the data we will receive has been agreed, it is transferred from the client to us via our secure FTP system called Axway. This data is known as source data. Once the data has been received, it is validated according to the retailer-specific rules which are outlined in the DIS. If the data does not reflect what is outlined in the DIS changes may need to be made \u2013 either by updating the DIS or requesting a resupply. Once we agree the data is in the correct format it can be read in. Data Extraction can be - Full or Partial (Delta). DIS - The data we receive is mapped in a document known as Data Interface Specification (DIS). QA check in Source / RAW layer- A key section of RAW is quality assurance (QA). We carry out standard checks to ensure data is \u201chealthy\u201d and without errors Main focus of the checks is the fact tables, and relation on the fact data with the key dimension tables. Checks include: * Number of baskets in the basket and item tables * Levels of spend * Missing foreign/primary keys Any issues found are recorded and can either be resolved with the solution DSG or may require input from the client Once these checks are successfully completed the build moves forward into the PREP stage.","title":"Extract"},{"location":"SQL/docs/Data-Warehousing-basics/#transform","text":"Transform this client data to meet the operational and business needs of our internal database. Within prep we transform the client data into a standardized format within the guidelines of Marketplace What types of transformations do we perform? i. Reject bad data - Bad data i.e. record where key info is missing. What is rejected depends on the business rules. We keep a record of rejected data by extracting them to a separate table, mark the missing field. ii. Remove duplicates Duplicate data can have negative impact on results. Important to understand if it is really a duplicate before removing. iii. Convert fields Convert from character to date and numeric fields where relevant e.g. spends/quantities. iv. Text manipulations Changes format e.g. change lookup value to descriptive form. v. Merges with other tables Merge lookups/useful fields that should be on specific table. vi. Aggregate data Sometimes need to roll up products in same basket or even create basket table - involves summations. vii. Rename fields Rename to make more meaningful \u2013 esp if in a foreign language. viii. Create standard fields Essential to marketplace, same naming conventions e.g. dib_bask_code.","title":"Transform"},{"location":"SQL/docs/Data-Warehousing-basics/#load","text":"Load into our analytical data mart within Marketplace The load is automated, so you will not be expected to know exactly what occurs. Here is an overview: Inbound Outbound -extracts and updates required data into standard structure Staging - manage slowly changing dimensions, generate surrogate keys and *create skeleton records SCD - a dimension is considered a SCD when its attributes remain almost constant over time, requiring relatively minor alterations to represent the evolved state. Surrogate Keys - system-generated and non-persistent integer keys which replace foreign keys. Skeleton records - Generated when a foreign key in a fact table does not have a match in the dimension table. A dummy or \u2018skeleton\u2019 record is created in the dimension table. There are following 3 Types of Data Loading Strategies : i. Initial load : Populating all the data tables from source system and loads it in to data warehouse table. ii. Incremental Load : Applying the ongoing changes as necessary in periodic manner. iii. Full Refresh : Completely erases the data from one or more tables and reload the fresh data.","title":"Load"},{"location":"SQL/docs/Data-Warehousing-basics/#star-and-snowflake-schema","text":"","title":"Star and Snowflake schema"},{"location":"SQL/docs/Data-Warehousing-basics/#star-schema","text":"In the star schema design, a single object (the fact table) sits in the middle and is radically connected to other surrounding objects (dimension lookup tables) like a star. Each dimension is represented as a single table. The primary key in each dimension table is related to a foreign key in the fact table. All measures in the fact table are related to all the dimensions that fact table is related to. In other words, they all have the same level of granularity. A star schema can be simple or complex. A simple star consists of one fact table; a complex star can have more than one fact table.","title":"Star Schema"},{"location":"SQL/docs/Data-Warehousing-basics/#snowflake-schema","text":"It is an extension of star schema. In a star schema, each dimension is represented by a single dimensional table, whereas in a snowflake schema, that dimensional table is normalized into multiple lookup tables, each representing a level in the dimensional hierarchy. Adv - improvement in query performance due to minimized disk storage requirements and joining smaller lookup tables. Disadvantage- additional maintenance efforts needed due to the increase number of lookup tables.","title":"Snowflake Schema"},{"location":"SQL/docs/Data-Warehousing-basics/#fact-table-granularity","text":"The first step in designing a fact table is to determine the granularity of the fact table. By granularity, we mean the lowest level of information that will be stored in the fact table. This constitutes two steps: i. Determine which dimensions will be included - this depends on business process being targetted. ii. Determine where along the hierarchy of each dimension the information will be kept - This depends on requirements. Eg - if client wants hourly reports, then fact table will keep hour as lowest level of granularity. If daily reports are fine, then date_id is lowest level of granularity. The determining factors usually goes back to the requirements.","title":"Fact Table Granularity"},{"location":"SQL/docs/Data-Warehousing-basics/#fact-and-fact-table-types","text":"There are three types of facts: i. Additive: Additive facts are facts that can be summed up through all of the dimensions in the fact table. ii. Semi-Additive: Semi-additive facts are facts that can be summed up for some of the dimensions in the fact table, but not the others. iii. Non-Additive: Non-additive facts are facts that cannot be summed up for any of the dimensions present in the fact table. eg1 - Additive Fact - Consider a retailer fact table with following columns - - Date - Store - Product - Sales_Amount The purpose of this table is to record the sales amount for each product in each store on a daily basis. Sales_Amount is an additive fact, because you can sum up this fact along any of the three dimensions present in the fact table -- date, store, and product. eg2A - Semi-Additive Fact and Non-Additive Fact - Say we are a bank with the following fact table: - Date - Account - Current_Balance - Profit_Margin The purpose of this table is to record the current balance for each account at the end of each day, as well as the profit margin for each account for each day. Current_Balance and Profit_Margin are the facts. Current_Balance is a semi-additive fact, as it makes sense to add them up for all accounts (what's the total current balance for all accounts in the bank?), but it does not make sense to add them up through time (adding up all current balances for a given account for each day of the month does not give us any useful information). Profit_Margin is a non-additive fact, for it does not make sense to add them up for the account level or the day level. eg 2B - semi -additive - distinct customers who shopped in a day = semi additive. Across all stores, this number can be aggregated. For example, store A has 300 customers and store B has 200 customers. So total 500 customers. But cant add across date dimension. So no summation possible across days in a week. non-additive = %age loyalty transaction in a day. For example, store A has 30% sales as loyalty count, and store B has 40%. But we cant add these two figures to find overall loyalty sales. Based on the above classifications, there are two types of Fact TABLES: * Cumulative: This type of fact table describes what has happened over a period of time. For example, this fact table may describe the total sales by product by store by day. The facts for this type of fact tables are mostly additive facts. The first example presented here is a cumulative fact table. * Snapshot: This type of fact table describes the state of things in a particular instance of time, and usually includes more semi-additive and non-additive facts. The second example presented here is a snapshot fact table.","title":"Fact And Fact Table Types "},{"location":"SQL/docs/Data-Warehousing-basics/#slowly-changing-dimensions","text":"The \"Slowly Changing Dimension\" problem is a common one particular to data warehousing. In a nutshell, this applies to cases where the attribute for a record varies over time. There are in general three ways to solve this type of problem, and they are categorized as follows:","title":"Slowly Changing Dimensions"},{"location":"SQL/docs/Data-Warehousing-basics/#type-1","text":"The new record replaces the original record. No trace of the old record exists. In other words, no history is kept. Advantage - * easiest to handle as no need to maintain history. Disadvantage- *History is lost. Cant track past behavior. So, Type 1 slowly changing dimension should be used when it is not necessary for the data warehouse to keep track of historical changes.","title":"Type 1"},{"location":"SQL/docs/Data-Warehousing-basics/#type-2","text":"A new record is added into the customer dimension table. Therefore, the customer is treated essentially as two people. Both the original and the new record will be present. The new record gets its own primary key. Advantages: - This allows us to accurately keep all historical information. Disadvantages: - This will cause the size of the table to grow fast. In cases where the number of rows for the table is very high to start with, storage and performance can become a concern. - This necessarily complicates the ETL process.","title":"Type 2"},{"location":"SQL/docs/Data-Warehousing-basics/#type-3","text":"The original record is modified to reflect the change. We add more column to track change. But this is feasible only if changes to be tracked are finite. For example, phone or address changes more than once will complicate things.","title":"Type 3"},{"location":"SQL/docs/Data-Warehousing-basics/#data-integrity","text":"Data integrity refers to the validity of data, meaning data is consistent and correct. In the data warehousing field, we frequently hear the term, \"Garbage In, Garbage Out.\" If there is no data integrity in the data warehouse, any resulting report and analysis will not be useful. In a data warehouse or a data mart, there are 3 areas of where data integrity needs to be enforced: a. Database level We can enforce data integrity at the database level. Common ways of enforcing data integrity include: i. Referential integrity The relationship between the primary key of one table and the foreign key of another table must always be maintained. For example, a primary key cannot be deleted if there is still a foreign key that refers to this primary key. ii. Primary key / Unique constraint Primary keys and the UNIQUE constraint are used to make sure every row in a table can be uniquely identified. iii. Not NULL vs. NULL-able For columns identified as NOT NULL, they may not have a NULL value. iv. Valid Values Only allowed values are permitted in the database. For example, if a column can only have positive integers, a value of '-1' cannot be allowed. b. ETL process For each step of the ETL process, data integrity checks should be put in place to ensure that source data is the same as the data in the destination. Most common checks include record counts or record sums. c. Access level We need to ensure that data is not altered by any unauthorized means either during the ETL process or in the data warehouse. To do this, there needs to be safeguards against unauthorized access to data (including physical access to the servers), as well as logging of all data access history. Data integrity can only ensured if there is no unauthorized access to the data. 4F. Factless Fact Table A factless fact table is a fact table that does not have any measures. It is essentially an intersection of dimensions. On the surface, a factless fact table does not make sense, since a fact table is, after all, about facts. However, there are situations where having this kind of relationship makes sense in data warehousing. eg1 - student class attendance record. In this case, the fact table would consist of 3 dimensions: the student dimension, the time dimension, and the class dimension. This factless fact table would look like the following: Fact Table \"school_attendance\" date_id classId student_id 02-02-2020 1 101 02-02-2020 1 102 02-02-2020 1 103 ---------------------------------- The only measure that you can possibly attach to each combination is \"1\" to show the presence of that particular combination. However, adding a fact that always shows 1 is redundant because we can simply use the COUNT function in SQL to answer the same questions. eg 2 - online sales in CRV. columns - date_id, store_id, till_id, pos_id In essence it contains only 1 column = basket_key. If a basket is in this table, it means its online sale, else offline sale. eg3 - Promotion data. Table structure could be - date_id | store_id| promo_type| promo_id| basket_key promo_type = Promotion can be online, in-store, flat discount, coupon, voucher, etc. Above table contains info of promotion applied on a basket. No measurable fact exists here. But why needed ? Transaction data contains info of what item was sold on promotion. But promotion data contains information of all the promotion during the purchase period. That is, all products having promotion applied on them, including those which were not sold in spite of promotion. And so, this promotion table becomes pivotal even though it contains no measurable fact. Why need factless facts? Factless fact tables offer the most flexibility in data warehouse design. For example, one can easily answer the following questions with this factless fact table: * How many students attended a particular class on a particular day? * How many classes on average does a student attend on a given day? Without using a factless fact table, we will need two separate fact tables to answer the above two questions. With the above factless fact table, it becomes the only fact table that's needed.","title":"Data Integrity"},{"location":"SQL/docs/Data-Warehousing-basics/#junk-dimension","text":"There are columns in Fact table which can have only a few or 2 kind of values - true or false, 1 or 0, etc. eg = bulk Vs non-bulk online vs offline promo vs non-promo vs hybrid sale etc. From business point of view, capturing above info in Fact table is very important. Issue -having these info will only make our fact table bulky and eventually unmanageable. Soln - junk dimension. eg - CRV basket channel seg - shop_channel_code in 0,1,2 or 3 - covers both bulk/non-bulk and online/offline. this would reduce 2 columns in fact table to 1. we can expand scope of above column to include promo info, and in that way we replace 3 fact columns by 1. This will result in a data warehousing environment that offer better performance as well as being easier to manage. [reference] (https://www.1keydata.com/datawarehousing/junk-dimension.html)","title":"Junk Dimension"},{"location":"SQL/docs/sql-analytical-functions/","text":"SQL Analytical Functions Source over (partition by / order by ) eg - SELECT o.region_id region_id, o.cust_nbr cust_nbr, SUM(o.tot_sales) tot_sales, SUM(SUM(o.tot_sales)) OVER (PARTITION BY o.region_id) region_sales FROM orders o WHERE o.year = 2001 GROUP BY o.region_id, o.cust_nbr; Ranking Functions source ROW_NUMBER, RANK and DENSE_RANK ROW_NUMBER - Returns a unique number for each row starting with 1. For rows that have duplicate values, numbers are arbitrarily assigned. DENSE_RANK - Assigns a unique number for each row starting with 1, except for rows that have duplicate values, in which case the same ranking is assigned. RANK - Assigns a unique number for each row starting with 1, except for rows that have duplicate values, in which case the same ranking is assigned and a gap appears in the sequence for each duplicate ranking. eg 1 - SELECT region_id, cust_nbr, SUM(tot_sales) cust_sales, RANK( ) OVER (ORDER BY SUM(tot_sales) DESC) sales_rank, DENSE_RANK( ) OVER (ORDER BY SUM(tot_sales) DESC) sales_dense_rank, ROW_NUMBER( ) OVER (ORDER BY SUM(tot_sales) DESC) sales_number FROM orders WHERE year = 2001 GROUP BY region_id, cust_nbr ORDER BY sales_number; REGION_ID CUST_NBR CUST_SALES SALES_RANK SALES_DENSE_RANK SALES_NUMBER ---------- ---------- ---------- ---------- ---------------- ------------ 8 18 1253840 11 11 11 5 2 1224992 12 12 12 9 23 1224992 12 12 13 9 24 1224992 12 12 14 10 30 1216858 15 13 15 eg 2 - The following query generates rankings for customer sales within each region rather than across all regions. Note the addition of the PARTITION BY clause: SELECT region_id, cust_nbr, SUM(tot_sales) cust_sales, RANK( ) OVER (PARTITION BY region_id ORDER BY SUM(tot_sales) DESC) sales_rank, DENSE_RANK( ) OVER (PARTITION BY region_id ORDER BY SUM(tot_sales) DESC) sales_dense_rank, ROW_NUMBER( ) OVER (PARTITION BY region_id ORDER BY SUM(tot_sales) DESC) sales_number FROM orders WHERE year = 2001 GROUP BY region_id, cust_nbr ORDER BY region_id, sales_number; REGION_ID CUST_NBR CUST_SALES SALES_RANK SALES_DENSE_RANK SALES_NUMBER ---------- ---------- ---------- ---------- ---------------- ------------ 5 4 1878275 1 1 1 5 2 1224992 2 2 2 5 5 1169926 3 3 3 5 3 1161286 4 4 4 5 1 1151162 5 5 5 6 6 1788836 1 1 1 6 9 1208959 2 2 2 Handling NULLs All ranking functions allow you to specify where in the ranking order NULL values should appear. This is accomplished by appending either NULLS FIRST or NULLS LAST after the ORDER BY clause of the function, as in: SELECT region_id, cust_nbr, SUM(tot_sales) cust_sales, RANK( ) OVER (ORDER BY SUM(tot_sales) DESC NULLS LAST) sales_rank FROM orders WHERE year = 2001 GROUP BY region_id, cust_nbr; NTILE Another way rankings are commonly used is to generate buckets into which sets of rankings are grouped. For example, you may want to find those customers whose total sales ranked in the top 25%. The following query uses the NTILE function to group the customers into four buckets (or quartiles): SELECT region_id, cust_nbr, SUM(tot_sales) cust_sales, NTILE(4) OVER (ORDER BY SUM(tot_sales) DESC) sales_quartile FROM orders WHERE year = 2001 GROUP BY region_id, cust_nbr ORDER BY sales_quartile, cust_sales DESC; REGION_ID CUST_NBR CUST_SALES SALES_QUARTILE ---------- ---------- ---------- -------------- 9 25 2232703 1 8 17 1944281 1 7 14 1929774 1 CUME_DIST and PERCENT_RANK CUME_DIST function (short for Cumulative Distribution) calculates the ratio of the number of rows that have a lesser or equal ranking to the total number of rows in the partition. PERCENT_RANK function calculates the ratio of a row's ranking to the number of rows in the partition using the formula: (RRP -- 1) / (NRP -- 1) where RRP is the \"rank of row in partition,\" and NRP is the \"number of rows in partition.\" Windowing Functions ROWS BETWEEN <> AND <> Some of the sample values can be - ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING eg - SELECT month, SUM(tot_sales) monthly_sales, SUM(SUM(tot_sales)) OVER ( ORDER BY month ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING ) total_sales FROM orders WHERE year = 2001 AND region_id = 6 GROUP BY month ORDER BY month; MONTH MONTHLY_SALES TOTAL_SALES 1 610697 6307766 2 428676 6307766 3 637031 6307766 4 541146 6307766 ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW eg 1 - max current value- SELECT month, SUM(tot_sales) monthly_sales, MAX(SUM(tot_sales)) OVER (ORDER BY month ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) max_preceeding FROM orders WHERE year = 2001 AND region_id = 6 GROUP BY month ORDER BY month; MONTH MONTHLY_SALES MAX_PRECEEDING 1 610697 610697 2 428676 610697 3 637031 637031 4 541146 637031 5 592935 637031 6 501485 637031 eg 2 - cumulative SUM - SELECT month, SUM(tot_sales) monthly_sales, SUM(SUM(tot_sales)) OVER (ORDER BY month ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) max_preceeding FROM orders WHERE year = 2001 AND region_id = 6 GROUP BY month ORDER BY month; MONTH MONTHLY_SALES RUNNING_TOTAL 1 610697 610697 2 428676 1039373 3 637031 1676404 4 541146 2217550 5 592935 2810485 ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING - eg -calculate avg of rolling 3 values (current row, rev row and next row); SELECT month, SUM(tot_sales) monthly_sales, AVG(SUM(tot_sales)) OVER (ORDER BY month ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING) rolling_avg FROM orders WHERE year = 2001 AND region_id = 6 GROUP BY month ORDER BY month; MONTH MONTHLY_SALES ROLLING_AVG 1 610697 519686.5 2 428676 558801.333 3 637031 535617.667 4 541146 590370.667 5 592935 545188.667 RANGE BETWEEN ROWS BETWEEN - based on row ordered as per condition of ordering insdie over() clause. RANGE BETWEEN - based on value ranges specified under over() clause. eg1 - to generate a three-month rolling average (similar to above ROWS BETWEEN question). In our table, month is numeric integer value, and so RANGE works perfectly fine here. This substitution works because the month column contains integer values, so adding and subtracting 1 from the current month yields a three-month range. But if its character, then below wont be suited. SELECT month, SUM(tot_sales) monthly_sales, AVG(SUM(tot_sales)) OVER (ORDER BY month RANGE BETWEEN 1 PRECEDING AND 1 FOLLOWING) rolling_avg FROM orders WHERE year = 2001 AND region_id = 6 GROUP BY month ORDER BY month; ---------- ------------- ----------- MONTH MONTHLY_SALES ROLLING_AVG ---------- ------------- ----------- 1 610697 519686.5 2 428676 558801.333 3 637031 535617.667 eg2 - if we do a range of +/- 1.999, then also we get same values: SELECT month, SUM(tot_sales) monthly_sales, AVG(SUM(tot_sales)) OVER (ORDER BY month RANGE BETWEEN 1.99 PRECEDING AND 1.99 FOLLOWING) rolling_avg FROM orders WHERE year = 2001 AND region_id = 6 GROUP BY month ORDER BY month; ---------- ------------- ----------- MONTH MONTHLY_SALES ROLLING_AVG ---------- ------------- ----------- 1 610697 519686.5 2 428676 558801.333 3 637031 535617.667 eg3 - working with DATE Range . ROWS wont be of much use if we are working on Date Range. If you are generating a window based on a DATE column, you can specify a range in increments of days, months, or years. Since the orders table has no DATE columns, the next example shows how a date range can be specified against the order_dt column of the cust_order table: SELECT TRUNC(order_dt) day, SUM(sale_price) daily_sales, AVG(SUM(sale_price)) OVER ( ORDER BY TRUNC(order_dt) RANGE BETWEEN INTERVAL '2' DAY PRECEDING AND INTERVAL '2' DAY FOLLOWING ) five_day_avg FROM cust_order WHERE sale_price IS NOT NULL AND order_dt BETWEEN TO_DATE('01-JUL-2001','DD-MON-YYYY') AND TO_DATE('31-JUL-2001','DD-MON-YYYY') GROUP BY TRUNC(order_dt); --------- ----------- ------------ DAY DAILY_SALES FIVE_DAY_AVG --------- ----------- ------------ 16-JUL-01 112 146 18-JUL-01 180 114 20-JUL-01 50 169 21-JUL-01 50 165.333333 22-JUL-01 396 165.333333 FIRST_VALUE and LAST_VALUE They are used with windowing functions to identify the values of the first and last values in the window. sample que :\"How did each month's sales compare to the first month?\" eg - In the case of the three-month rolling average query shown previously, you could display the values of all three months along with the average of the three, as in: SELECT month, FIRST_VALUE(SUM(tot_sales)) OVER (ORDER BY month ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING) prev_month, SUM(tot_sales) monthly_sales, LAST_VALUE(SUM(tot_sales)) OVER (ORDER BY month ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING) next_month, AVG(SUM(tot_sales)) OVER (ORDER BY month ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING) rolling_avg FROM orders WHERE year = 2001 AND region_id = 6 GROUP BY month ORDER BY month; MONTH PREV_MONTH MONTHLY_SALES NEXT_MONTH ROLLING_AVG 1 610697 610697 428676 519686.5 2 610697 428676 637031 558801.333 3 428676 637031 541146 535617.667 4 637031 541146 592935 590370.667 5 541146 592935 501485 545188.667 LAG/LEAD Functions \"LAG(VAL, N, <default value>) OVER () \" -- N=1 by default. \"LEAD(VAL, N, <default value>) OVER () \" -- N=1 by default. Query - \"Compute the total sales per month for the Mid-Atlantic region, including the percent change from the previous month\" requires data from both the current and preceding rows to calculate the answer. Step 1 - get prev month's data using LAG function. SELECT month, SUM(tot_sales) monthly_sales, LAG(SUM(tot_sales), 1) OVER (ORDER BY month) prev_month_sales FROM orders WHERE year = 2001 AND region_id = 6 GROUP BY month ORDER BY month; ---------- ------------- ---------------- MONTH MONTHLY_SALES PREV_MONTH_SALES ---------- ------------- ---------------- 1 610697 2 428676 610697 3 637031 428676 Step 2 - handle NULL values. If you see above, for month 1, PREV_MONTH_SALES is NULL. So, can calculate % change in sales. Here we keep current month sales as default value, and % sales in this case is 0%. SELECT months.month month, months.monthly_sales monthly_sales, ROUND((months.monthly_sales - months.prev_month_sales)/ months.prev_month_sales, 3) * 100 percent_change FROM ( SELECT month, SUM(tot_sales) monthly_sales, LAG(SUM(tot_sales), 1, SUM(tot_sales)) OVER (ORDER BY month) prev_month_sales FROM orders WHERE year = 2001 AND region_id = 6 GROUP BY month) months ORDER BY month; ---------- ------------- -------------- MONTH MONTHLY_SALES PERCENT_CHANGE ---------- ------------- -------------- 1 610697 0 2 428676 -29.8 3 637031 48.6","title":"SQL Analytical Functions"},{"location":"SQL/docs/sql-analytical-functions/#sql-analytical-functions","text":"Source","title":"SQL Analytical Functions"},{"location":"SQL/docs/sql-analytical-functions/#over-partition-by-order-by","text":"eg - SELECT o.region_id region_id, o.cust_nbr cust_nbr, SUM(o.tot_sales) tot_sales, SUM(SUM(o.tot_sales)) OVER (PARTITION BY o.region_id) region_sales FROM orders o WHERE o.year = 2001 GROUP BY o.region_id, o.cust_nbr;","title":"over (partition by / order by )"},{"location":"SQL/docs/sql-analytical-functions/#ranking-functions","text":"source","title":"Ranking Functions"},{"location":"SQL/docs/sql-analytical-functions/#row_number-rank-and-dense_rank","text":"ROW_NUMBER - Returns a unique number for each row starting with 1. For rows that have duplicate values, numbers are arbitrarily assigned. DENSE_RANK - Assigns a unique number for each row starting with 1, except for rows that have duplicate values, in which case the same ranking is assigned. RANK - Assigns a unique number for each row starting with 1, except for rows that have duplicate values, in which case the same ranking is assigned and a gap appears in the sequence for each duplicate ranking. eg 1 - SELECT region_id, cust_nbr, SUM(tot_sales) cust_sales, RANK( ) OVER (ORDER BY SUM(tot_sales) DESC) sales_rank, DENSE_RANK( ) OVER (ORDER BY SUM(tot_sales) DESC) sales_dense_rank, ROW_NUMBER( ) OVER (ORDER BY SUM(tot_sales) DESC) sales_number FROM orders WHERE year = 2001 GROUP BY region_id, cust_nbr ORDER BY sales_number; REGION_ID CUST_NBR CUST_SALES SALES_RANK SALES_DENSE_RANK SALES_NUMBER ---------- ---------- ---------- ---------- ---------------- ------------ 8 18 1253840 11 11 11 5 2 1224992 12 12 12 9 23 1224992 12 12 13 9 24 1224992 12 12 14 10 30 1216858 15 13 15 eg 2 - The following query generates rankings for customer sales within each region rather than across all regions. Note the addition of the PARTITION BY clause: SELECT region_id, cust_nbr, SUM(tot_sales) cust_sales, RANK( ) OVER (PARTITION BY region_id ORDER BY SUM(tot_sales) DESC) sales_rank, DENSE_RANK( ) OVER (PARTITION BY region_id ORDER BY SUM(tot_sales) DESC) sales_dense_rank, ROW_NUMBER( ) OVER (PARTITION BY region_id ORDER BY SUM(tot_sales) DESC) sales_number FROM orders WHERE year = 2001 GROUP BY region_id, cust_nbr ORDER BY region_id, sales_number; REGION_ID CUST_NBR CUST_SALES SALES_RANK SALES_DENSE_RANK SALES_NUMBER ---------- ---------- ---------- ---------- ---------------- ------------ 5 4 1878275 1 1 1 5 2 1224992 2 2 2 5 5 1169926 3 3 3 5 3 1161286 4 4 4 5 1 1151162 5 5 5 6 6 1788836 1 1 1 6 9 1208959 2 2 2","title":"ROW_NUMBER, RANK and DENSE_RANK"},{"location":"SQL/docs/sql-analytical-functions/#handling-nulls","text":"All ranking functions allow you to specify where in the ranking order NULL values should appear. This is accomplished by appending either NULLS FIRST or NULLS LAST after the ORDER BY clause of the function, as in: SELECT region_id, cust_nbr, SUM(tot_sales) cust_sales, RANK( ) OVER (ORDER BY SUM(tot_sales) DESC NULLS LAST) sales_rank FROM orders WHERE year = 2001 GROUP BY region_id, cust_nbr;","title":"Handling NULLs"},{"location":"SQL/docs/sql-analytical-functions/#ntile","text":"Another way rankings are commonly used is to generate buckets into which sets of rankings are grouped. For example, you may want to find those customers whose total sales ranked in the top 25%. The following query uses the NTILE function to group the customers into four buckets (or quartiles): SELECT region_id, cust_nbr, SUM(tot_sales) cust_sales, NTILE(4) OVER (ORDER BY SUM(tot_sales) DESC) sales_quartile FROM orders WHERE year = 2001 GROUP BY region_id, cust_nbr ORDER BY sales_quartile, cust_sales DESC; REGION_ID CUST_NBR CUST_SALES SALES_QUARTILE ---------- ---------- ---------- -------------- 9 25 2232703 1 8 17 1944281 1 7 14 1929774 1","title":"NTILE"},{"location":"SQL/docs/sql-analytical-functions/#cume_dist-and-percent_rank","text":"CUME_DIST function (short for Cumulative Distribution) calculates the ratio of the number of rows that have a lesser or equal ranking to the total number of rows in the partition. PERCENT_RANK function calculates the ratio of a row's ranking to the number of rows in the partition using the formula: (RRP -- 1) / (NRP -- 1) where RRP is the \"rank of row in partition,\" and NRP is the \"number of rows in partition.\"","title":"CUME_DIST and PERCENT_RANK"},{"location":"SQL/docs/sql-analytical-functions/#windowing-functions","text":"","title":"Windowing Functions"},{"location":"SQL/docs/sql-analytical-functions/#rows-between-and","text":"Some of the sample values can be -","title":"ROWS BETWEEN &lt;&gt; AND &lt;&gt;"},{"location":"SQL/docs/sql-analytical-functions/#rows-between-unbounded-preceding-and-unbounded-following","text":"eg - SELECT month, SUM(tot_sales) monthly_sales, SUM(SUM(tot_sales)) OVER ( ORDER BY month ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING ) total_sales FROM orders WHERE year = 2001 AND region_id = 6 GROUP BY month ORDER BY month; MONTH MONTHLY_SALES TOTAL_SALES 1 610697 6307766 2 428676 6307766 3 637031 6307766 4 541146 6307766","title":"ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING"},{"location":"SQL/docs/sql-analytical-functions/#rows-between-unbounded-preceding-and-current-row","text":"eg 1 - max current value- SELECT month, SUM(tot_sales) monthly_sales, MAX(SUM(tot_sales)) OVER (ORDER BY month ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) max_preceeding FROM orders WHERE year = 2001 AND region_id = 6 GROUP BY month ORDER BY month; MONTH MONTHLY_SALES MAX_PRECEEDING 1 610697 610697 2 428676 610697 3 637031 637031 4 541146 637031 5 592935 637031 6 501485 637031 eg 2 - cumulative SUM - SELECT month, SUM(tot_sales) monthly_sales, SUM(SUM(tot_sales)) OVER (ORDER BY month ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) max_preceeding FROM orders WHERE year = 2001 AND region_id = 6 GROUP BY month ORDER BY month; MONTH MONTHLY_SALES RUNNING_TOTAL 1 610697 610697 2 428676 1039373 3 637031 1676404 4 541146 2217550 5 592935 2810485","title":"ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW"},{"location":"SQL/docs/sql-analytical-functions/#rows-between-1-preceding-and-1-following-","text":"eg -calculate avg of rolling 3 values (current row, rev row and next row); SELECT month, SUM(tot_sales) monthly_sales, AVG(SUM(tot_sales)) OVER (ORDER BY month ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING) rolling_avg FROM orders WHERE year = 2001 AND region_id = 6 GROUP BY month ORDER BY month; MONTH MONTHLY_SALES ROLLING_AVG 1 610697 519686.5 2 428676 558801.333 3 637031 535617.667 4 541146 590370.667 5 592935 545188.667","title":"ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING -"},{"location":"SQL/docs/sql-analytical-functions/#range-between","text":"ROWS BETWEEN - based on row ordered as per condition of ordering insdie over() clause. RANGE BETWEEN - based on value ranges specified under over() clause. eg1 - to generate a three-month rolling average (similar to above ROWS BETWEEN question). In our table, month is numeric integer value, and so RANGE works perfectly fine here. This substitution works because the month column contains integer values, so adding and subtracting 1 from the current month yields a three-month range. But if its character, then below wont be suited. SELECT month, SUM(tot_sales) monthly_sales, AVG(SUM(tot_sales)) OVER (ORDER BY month RANGE BETWEEN 1 PRECEDING AND 1 FOLLOWING) rolling_avg FROM orders WHERE year = 2001 AND region_id = 6 GROUP BY month ORDER BY month; ---------- ------------- ----------- MONTH MONTHLY_SALES ROLLING_AVG ---------- ------------- ----------- 1 610697 519686.5 2 428676 558801.333 3 637031 535617.667 eg2 - if we do a range of +/- 1.999, then also we get same values: SELECT month, SUM(tot_sales) monthly_sales, AVG(SUM(tot_sales)) OVER (ORDER BY month RANGE BETWEEN 1.99 PRECEDING AND 1.99 FOLLOWING) rolling_avg FROM orders WHERE year = 2001 AND region_id = 6 GROUP BY month ORDER BY month; ---------- ------------- ----------- MONTH MONTHLY_SALES ROLLING_AVG ---------- ------------- ----------- 1 610697 519686.5 2 428676 558801.333 3 637031 535617.667 eg3 - working with DATE Range . ROWS wont be of much use if we are working on Date Range. If you are generating a window based on a DATE column, you can specify a range in increments of days, months, or years. Since the orders table has no DATE columns, the next example shows how a date range can be specified against the order_dt column of the cust_order table: SELECT TRUNC(order_dt) day, SUM(sale_price) daily_sales, AVG(SUM(sale_price)) OVER ( ORDER BY TRUNC(order_dt) RANGE BETWEEN INTERVAL '2' DAY PRECEDING AND INTERVAL '2' DAY FOLLOWING ) five_day_avg FROM cust_order WHERE sale_price IS NOT NULL AND order_dt BETWEEN TO_DATE('01-JUL-2001','DD-MON-YYYY') AND TO_DATE('31-JUL-2001','DD-MON-YYYY') GROUP BY TRUNC(order_dt); --------- ----------- ------------ DAY DAILY_SALES FIVE_DAY_AVG --------- ----------- ------------ 16-JUL-01 112 146 18-JUL-01 180 114 20-JUL-01 50 169 21-JUL-01 50 165.333333 22-JUL-01 396 165.333333","title":"RANGE BETWEEN"},{"location":"SQL/docs/sql-analytical-functions/#first_value-and-last_value","text":"They are used with windowing functions to identify the values of the first and last values in the window. sample que :\"How did each month's sales compare to the first month?\" eg - In the case of the three-month rolling average query shown previously, you could display the values of all three months along with the average of the three, as in: SELECT month, FIRST_VALUE(SUM(tot_sales)) OVER (ORDER BY month ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING) prev_month, SUM(tot_sales) monthly_sales, LAST_VALUE(SUM(tot_sales)) OVER (ORDER BY month ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING) next_month, AVG(SUM(tot_sales)) OVER (ORDER BY month ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING) rolling_avg FROM orders WHERE year = 2001 AND region_id = 6 GROUP BY month ORDER BY month; MONTH PREV_MONTH MONTHLY_SALES NEXT_MONTH ROLLING_AVG 1 610697 610697 428676 519686.5 2 610697 428676 637031 558801.333 3 428676 637031 541146 535617.667 4 637031 541146 592935 590370.667 5 541146 592935 501485 545188.667","title":"FIRST_VALUE and LAST_VALUE"},{"location":"SQL/docs/sql-analytical-functions/#laglead-functions","text":"\"LAG(VAL, N, <default value>) OVER () \" -- N=1 by default. \"LEAD(VAL, N, <default value>) OVER () \" -- N=1 by default. Query - \"Compute the total sales per month for the Mid-Atlantic region, including the percent change from the previous month\" requires data from both the current and preceding rows to calculate the answer. Step 1 - get prev month's data using LAG function. SELECT month, SUM(tot_sales) monthly_sales, LAG(SUM(tot_sales), 1) OVER (ORDER BY month) prev_month_sales FROM orders WHERE year = 2001 AND region_id = 6 GROUP BY month ORDER BY month; ---------- ------------- ---------------- MONTH MONTHLY_SALES PREV_MONTH_SALES ---------- ------------- ---------------- 1 610697 2 428676 610697 3 637031 428676 Step 2 - handle NULL values. If you see above, for month 1, PREV_MONTH_SALES is NULL. So, can calculate % change in sales. Here we keep current month sales as default value, and % sales in this case is 0%. SELECT months.month month, months.monthly_sales monthly_sales, ROUND((months.monthly_sales - months.prev_month_sales)/ months.prev_month_sales, 3) * 100 percent_change FROM ( SELECT month, SUM(tot_sales) monthly_sales, LAG(SUM(tot_sales), 1, SUM(tot_sales)) OVER (ORDER BY month) prev_month_sales FROM orders WHERE year = 2001 AND region_id = 6 GROUP BY month) months ORDER BY month; ---------- ------------- -------------- MONTH MONTHLY_SALES PERCENT_CHANGE ---------- ------------- -------------- 1 610697 0 2 428676 -29.8 3 637031 48.6","title":"LAG/LEAD Functions"},{"location":"SQL/docs/sql-practise-questions/","text":"Leetcode SQL The Most Recent Three Orders Table: Customers +---------------+---------+ | Column Name | Type | +---------------+---------+ | customer_id | int | | name | varchar | +---------------+---------+ customer_id is the primary key for this table. This table contains information about customers. Table: Orders +---------------+---------+ | Column Name | Type | +---------------+---------+ | order_id | int | | order_date | date | | customer_id | int | | cost | int | +---------------+---------+ order_id is the primary key for this table. This table contains information about the orders made customer_id. Each customer has one order per day. Write an SQL query to find the most recent 3 orders of each user. If a user ordered less than 3 orders return all of their orders. Return the result table sorted by customer_name in ascending order and in case of a tie by the customer_id in ascending order. If there still a tie, order them by the order_date in descending order. The query result format is in the following example: Customers +-------------+-----------+ | customer_id | name | +-------------+-----------+ | 1 | Winston | | 2 | Jonathan | | 3 | Annabelle | | 4 | Marwan | | 5 | Khaled | +-------------+-----------+ Orders +----------+------------+-------------+------+ | order_id | order_date | customer_id | cost | +----------+------------+-------------+------+ | 1 | 2020-07-31 | 1 | 30 | | 2 | 2020-07-30 | 2 | 40 | | 3 | 2020-07-31 | 3 | 70 | | 4 | 2020-07-29 | 4 | 100 | | 5 | 2020-06-10 | 1 | 1010 | | 6 | 2020-08-01 | 2 | 102 | | 7 | 2020-08-01 | 3 | 111 | | 8 | 2020-08-03 | 1 | 99 | | 9 | 2020-08-07 | 2 | 32 | | 10 | 2020-07-15 | 1 | 2 | +----------+------------+-------------+------+ Result table: +---------------+-------------+----------+------------+ | customer_name | customer_id | order_id | order_date | +---------------+-------------+----------+------------+ | Annabelle | 3 | 7 | 2020-08-01 | | Annabelle | 3 | 3 | 2020-07-31 | | Jonathan | 2 | 9 | 2020-08-07 | | Jonathan | 2 | 6 | 2020-08-01 | | Jonathan | 2 | 2 | 2020-07-30 | | Marwan | 4 | 4 | 2020-07-29 | | Winston | 1 | 8 | 2020-08-03 | | Winston | 1 | 1 | 2020-07-31 | | Winston | 1 | 10 | 2020-07-15 | +---------------+-------------+----------+------------+ Winston has 4 orders, we discard the order of \"2020-06-10\" because it is the oldest order. Annabelle has only 2 orders, we return them. Jonathan has exactly 3 orders. Marwan ordered only one time. We sort the result table by customer_name in ascending order, by customer_id in ascending order and by order_date in descending order in case of a tie. Ans. select customer_name, customer_id, order_id, order_date from (select a.name as customer_name, a.customer_id, b.order_id, to_char(b.order_date, 'YYYY-MM-DD') as order_date, rank() over ( partition by a.customer_id order by b.order_date desc) date_rank from customers a inner join orders b on a.customer_id = b.customer_id order by a.customer_id, b.order_date ) where date_rank<=3 order by customer_name, customer_id, order_date desc ; 2A. Shortest Distance in a Plane https://leetcode.com/problems/shortest-distance-in-a-plane/ Table point_2d holds the coordinates (x,y) of some unique points (more than two) in a plane. Write a query to find the shortest distance between these points rounded to 2 decimals. x y -1 -1 0 0 -1 -2 The shortest distance is 1.00 from point (-1,-1) to (-1,2). So the output should be: shortest 1.00 Note: The longest distance among all the points are less than 10000. Ans. select min(dist) as shortest from ( select a.x as x1, a.y as y1, b.x as x2, b.y as y2, round(sqrt((a.x - b.x)*(a.x - b.x) + (a.y - b.y)*(a.y - b.y) ),2) as dist from point_2d a, point_2d b where concat(a.x,a.y)!=concat(b.x,b.y) -- this bit of matching coordinates is IMP to ensure same coordinates are not being captured. ); Investments in 2016 https://leetcode.com/problems/investments-in-2016/ Write a query to print the sum of all total investment values in 2016 (TIV_2016), to a scale of 2 decimal places, for all policy-holders who meet the following criteria: - Have the same TIV_2015 value as one or more other policyholders. - Are not located in the same city as any other policyholder (i.e.: the (latitude, longitude) attribute pairs must be unique). Input Format: The insurance table is described as follows: Column Name Type PID INTEGER(11) TIV_2015 NUMERIC(15,2) TIV_2016 NUMERIC(15,2) LAT NUMERIC(5,2) LON NUMERIC(5,2) where PID is the policyholder's policy ID, TIV_2015 is the total investment value in 2015, TIV_2016 is the total investment value in 2016, LAT is the latitude of the policy holder's city, and LON is the longitude of the policy holder's city. Sample Input PID TIV_2015 TIV_2016 LAT LON 1 10 5 10 10 2 20 20 20 20 3 10 30 20 20 4 10 40 40 40 Sample Output | TIV_2016 | |----------| | 45.00 | Explanation The first record in the table, like the last record, meets both of the two criteria. The TIV_2015 value '10' is as the same as the third and forth record, and its location unique. The second record does not meet any of the two criteria. Its TIV_2015 is not like any other policyholders. And its location is the same with the third record, which makes the third record fail, too. So, the result is the sum of TIV_2016 of the first and last record, which is 45. Ans. select round(sum(tiv_2016),2) as \"TIV_2016\" from ( select pid, count(pid) over (partition by tiv_2015) as tiv_2015_count, tiv_2016, count(*) over (partition by concat(concat(lat,'_'),lon)) as locn_count from insurance order by lat,lon) where locn_count=1 and tiv_2015_count>1; Calculate Salaries https://leetcode.com/problems/calculate-salaries/ Table Salaries: Column Name Type company_id int employee_id int employee_name varchar salary int (company_id, employee_id) is the primary key for this table. This table contains the company id, the id, the name and the salary for an employee. Write an SQL query to find the salaries of the employees after applying taxes. The tax rate is calculated for each company based on the following criteria: 0% If the max salary of any employee in the company is less than 1000$. 24% If the max salary of any employee in the company is in the range [1000, 10000] inclusive. 49% If the max salary of any employee in the company is greater than 10000$. Return the result table in any order. Round the salary to the nearest integer. The query result format is in the following example: Salaries table: +------------+-------------+---------------+--------+ | company_id | employee_id | employee_name | salary | +------------+-------------+---------------+--------+ | 1 | 1 | Tony | 2000 | | 1 | 2 | Pronub | 21300 | | 1 | 3 | Tyrrox | 10800 | | 2 | 1 | Pam | 300 | | 2 | 7 | Bassem | 450 | | 2 | 9 | Hermione | 700 | | 3 | 7 | Bocaben | 100 | | 3 | 2 | Ognjen | 2200 | | 3 | 13 | Nyancat | 3300 | | 3 | 15 | Morninngcat | 1866 | +------------+-------------+---------------+--------+ Result table: +------------+-------------+---------------+--------+ | company_id | employee_id | employee_name | salary | +------------+-------------+---------------+--------+ | 1 | 1 | Tony | 1020 | | 1 | 2 | Pronub | 10863 | | 1 | 3 | Tyrrox | 5508 | | 2 | 1 | Pam | 300 | | 2 | 7 | Bassem | 450 | | 2 | 9 | Hermione | 700 | | 3 | 7 | Bocaben | 76 | | 3 | 2 | Ognjen | 1672 | | 3 | 13 | Nyancat | 2508 | | 3 | 15 | Morninngcat | 5911 | +------------+-------------+---------------+--------+ For company 1, Max salary is 21300. Employees in company 1 have taxes = 49% For company 2, Max salary is 700. Employees in company 2 have taxes = 0% For company 3, Max salary is 7777. Employees in company 3 have taxes = 24% The salary after taxes = salary - (taxes percentage / 100) * salary For example, Salary for Morninngcat (3, 15) after taxes = 7777 - 7777 * (24 / 100) = 7777 - 1866.48 = 5910.52, which is rounded to 5911. Ans. select company_id, employee_id, employee_name, round( salary*(case when max_sal_per_company<1000 then 1 when max_sal_per_company>=1000 and max_sal_per_company<=10000 then 0.76 else 0.51 end ),0) as salary from ( select company_id , employee_id , employee_name , salary, max(salary) over (partition by company_id ) as max_sal_per_company from salaries); ``` 5. Countries You Can Safely Invest In<br> https://leetcode.com/problems/countries-you-can-safely-invest-in/ <br> Table Person: | Column Name | Type | |----------------|---------| | id | int | | name | varchar | | phone_number | varchar | id is the primary key for this table.<br> Each row of this table contains the name of a person and their phone number.<br> Phone number will be in the form 'xxx-yyyyyyy' where xxx is the country code (3 characters) and yyyyyyy is the phone number (7 characters) where x and y are digits. <br> Both can contain leading zeros.<br> Table Country: | Column Name | Type | |----------------|---------| | name | varchar | | country_code | varchar | country_code is the primary key for this table.<br> Each row of this table contains the country name and its code. country_code will be in the form 'xxx' where x is digits.<br> Table Calls:<br> | Column Name | Type | |-------------|------| | caller_id | int | | callee_id | int | | duration | int | There is no primary key for this table, it may contain duplicates.<br> Each row of this table contains the caller id, callee id and the duration of the call in minutes. caller_id != callee_id<br> A telecommunications company wants to invest in new countries. The company intends to invest in the countries where the average call duration of the calls in this country is strictly greater than the global average call duration.<br> Write an SQL query to find the countries where this company can invest.<br> Return the result table in any order.<br> The query result format is in the following example.<br> Person table: +----+----------+--------------+ | id | name | phone_number | +----+----------+--------------+ | 3 | Jonathan | 051-1234567 | | 12 | Elvis | 051-7654321 | | 1 | Moncef | 212-1234567 | | 2 | Maroua | 212-6523651 | | 7 | Meir | 972-1234567 | | 9 | Rachel | 972-0011100 | +----+----------+--------------+ Country table: +----------+--------------+ | name | country_code | +----------+--------------+ | Peru | 051 | | Israel | 972 | | Morocco | 212 | | Germany | 049 | | Ethiopia | 251 | +----------+--------------+ Calls table: +-----------+-----------+----------+ | caller_id | callee_id | duration | +-----------+-----------+----------+ | 1 | 9 | 33 | | 2 | 9 | 4 | | 1 | 2 | 59 | | 3 | 12 | 102 | | 3 | 12 | 330 | | 12 | 3 | 5 | | 7 | 9 | 13 | | 7 | 1 | 3 | | 9 | 7 | 1 | | 1 | 7 | 7 | +-----------+-----------+----------+ Result table: +----------+ | country | +----------+ | Peru | +----------+ The average call duration for Peru is (102 + 102 + 330 + 330 + 5 + 5) / 6 = 145.666667 The average call duration for Israel is (33 + 4 + 13 + 13 + 3 + 1 + 1 + 7) / 8 = 9.37500 The average call duration for Morocco is (33 + 4 + 59 + 59 + 3 + 7) / 6 = 27.5000 Global call duration average = (2 * (33 + 3 + 59 + 102 + 330 + 5 + 13 + 3 + 1 + 7)) / 20 = 55.70000 Since Peru is the only country where average call duration is greater than the global average, it's the only recommended country. Ans. with call_table as (select distinct caller_id, callee_id, duration from calls ) select distinct country from ( select t.name as country, c.country_code, avg(c.duration) over (partition by c.country_code) as country_avg, avg(c.duration) over () as overall_avg from ( select substr(b.phone_number,1,3) as country_code, a.duration from call_table a left join Person b on a.caller_id = b.id union all select substr(b.phone_number,1,3) as country_code, a.duration from call_table a left join Person b on a.callee_id = b.id ) c left join Country t on trim(c.country_code) = trim(t.country_code) ) where country_avg>overall_avg; 5. Rectangles Area<br> https://leetcode.com/problems/rectangles-area/ <br> Table: Points<br> +---------------+---------+ | Column Name | Type | +---------------+---------+ | id | int | | x_value | int | | y_value | int | +---------------+---------+ id is the primary key for this table.<br> Each point is represented as a 2D Dimensional (x_value, y_value).<br> Write an SQL query to report of all possible rectangles which can be formed by any two points of the table.<br> Each row in the result contains three columns (p1, p2, area) where:<br> p1 and p2 are the id of two opposite corners of a rectangle and p1 < p2.<br> Area of this rectangle is represented by the column area.<br> Report the query in descending order by area in case of tie in ascending order by p1 and p2.<br> Points table: +----------+-------------+-------------+ | id | x_value | y_value | +----------+-------------+-------------+ | 1 | 2 | 8 | | 2 | 4 | 7 | | 3 | 2 | 10 | +----------+-------------+-------------+ Result table: +----------+-------------+-------------+ | p1 | p2 | area | +----------+-------------+-------------+ | 2 | 3 | 6 | | 1 | 2 | 2 | +----------+-------------+-------------+ p1 should be less than p2 and area greater than 0. p1 = 1 and p2 = 2, has an area equal to |2-4| * |8-7| = 2. p1 = 2 and p2 = 3, has an area equal to |4-2| * |7-10| = 6. p1 = 1 and p2 = 3 It's not possible because the rectangle has an area equal to 0. Ans. -- Approach 1: select a.id as P1, b.id as P2, abs(a.x_value - b.x_value) abs(a.y_value - b.y_value) as area from points a, points b where a.id < b.id and abs(a.x_value - b.x_value) abs(a.y_value - b.y_value)>0 order by abs(a.x_value - b.x_value) abs(a.y_value - b.y_value) desc, a.id, b.id; -- Approach 2: select a.p1, b.p2, abs(a.x1-b.x2) abs(a.y1-b.y2) as area from (select id as P1, x_value as x1, y_value as y1, concat(x_value, y_value) as p1_coordinate from points) a, (select id as P2, x_value as x2, y_value as y2, concat(x_value, y_value) as p2_coordinate from points ) b where p1 0 order by abs(a.x1-b.x2)*abs(a.y1-b.y2) desc, a.p1, b.p2 6. Active users : Imp -- date diff between consecutive days. <br> https://leetcode.com/problems/active-users/ <br> Table Accounts: <br> | Column Name | Type | |---------------|---------| | id | int | | name | varchar | the id is the primary key for this table. <br> This table contains the account id and the user name of each account. <br> Table Logins: <br> | Column Name | Type | |---------------|---------| | id | int | | login_date | date | +---------------+---------+ There is no primary key for this table, it may contain duplicates. <br> This table contains the account id of the user who logged in and the login date. A user may log in multiple times in the day. <br> Write an SQL query to find the id and the name of active users. <br> Active users are those who logged in to their accounts for 5 or more consecutive days. <br> Return the result table ordered by the id. <br> The query result format is in the following example: <br> Accounts table: | id | name | |----|----------| | 1 | Winston | | 7 | Jonathan | Logins table: <br> | id | login_date | |----|------------| | 7 | 2020-05-30 | | 1 | 2020-05-30 | | 7 | 2020-05-31 | | 7 | 2020-06-01 | | 7 | 2020-06-02 | | 7 | 2020-06-02 | | 7 | 2020-06-03 | | 1 | 2020-06-07 | | 7 | 2020-06-10 | Result table: <br> | id | name | |----|----------| | 7 | Jonathan | User Winston with id = 1 logged in 2 times only in 2 different days, so, Winston is not an active user. <br> User Jonathan with id = 7 logged in 7 times in 6 different days, five of them were consecutive days, so, Jonathan is an active user. <br> Ans. select distinct d.id, d.name from ( select b.id, c.name, b.login_date, b.found_recs_old_login_date, to_date(b.login_date,'YYYY-MM-DD') - to_date(b.found_recs_old_login_date,'YYYY-MM-DD') as date_diff from ( select a.id, a.login_date, lag( a.login_date,4,'1990-01-01') over (partition by a.id order by a.login_date) as found_recs_old_login_date from (select distinct id, to_char(login_date,'YYYY-MM-DD') as login_date from Logins ) a ) b left join Accounts c on b.id = c.id ) d where d.date_diff=4 order by d.id; 7. Apples & Oranges <br> https://leetcode.com/problems/apples-oranges/ <br> Table: Sales <br> | Column Name | Type | |---------------|---------| | sale_date | date | | fruit | enum | | sold_num | int | (sale_date,fruit) is the primary key for this table. <br> This table contains the sales of \"apples\" and \"oranges\" sold each day. <br> Write an SQL query to report the difference between number of apples and oranges sold each day. <br> Return the result table ordered by sale_date in format ('YYYY-MM-DD'). <br> The query result format is in the following example: <br> Sales table: +------------+------------+-------------+ | sale_date | fruit | sold_num | +------------+------------+-------------+ | 2020-05-01 | apples | 10 | | 2020-05-01 | oranges | 8 | | 2020-05-02 | apples | 15 | | 2020-05-02 | oranges | 15 | | 2020-05-03 | apples | 20 | | 2020-05-03 | oranges | 0 | | 2020-05-04 | apples | 15 | | 2020-05-04 | oranges | 16 | +------------+------------+-------------+ Result table: +------------+--------------+ | sale_date | diff | +------------+--------------+ | 2020-05-01 | 2 | | 2020-05-02 | 0 | | 2020-05-03 | 20 | | 2020-05-04 | -1 | +------------+--------------+ Day 2020-05-01, 10 apples and 8 oranges were sold (Difference 10 - 8 = 2). Day 2020-05-02, 15 apples and 15 oranges were sold (Difference 15 - 15 = 0). Day 2020-05-03, 20 apples and 0 oranges were sold (Difference 20 - 0 = 20). Day 2020-05-04, 15 apples and 16 oranges were sold (Difference 15 - 16 = -1). Ans. KL - select coalesce(to_char(a.sale_date,'YYYY-MM-DD'),to_char(b.sale_date,'YYYY-MM-DD')) as sale_date, coalesce(a.sold_num,0) - coalesce(b.sold_num,0) as diff from (select sale_date , sold_num from sales where fruit='apples') a full join (select sale_date , sold_num from sales where fruit='oranges') b on to_char(a.sale_date,'YYYY-MM-DD')=to_char(b.sale_date,'YYYY-MM-DD') order by coalesce(to_char(a.sale_date,'YYYY-MM-DD'),to_char(b.sale_date,'YYYY-MM-DD')); NG - select sale_date, sum(case when fruit = 'apples' then sold_num else 0-sold_num end) as \"diff\" from sales group by sale_date 8. Evaluate Boolean Expression <br> https://leetcode.com/problems/evaluate-boolean-expression/ <br> Table Variables: <br> | Column Name | Type | |---------------|---------| | name | varchar | | value | int | name is the primary key for this table. <br> This table contains the stored variables and their values. <br> Table Expressions: <br> | Column Name | Type | |---------------|---------| | left_operand | varchar | | operator | enum | | right_operand | varchar | (left_operand, operator, right_operand) is the primary key for this table. <br> This table contains a boolean expression that should be evaluated. <br> operator is an enum that takes one of the values ('<', '>', '=') <br> The values of left_operand and right_operand are guaranteed to be in the Variables table. <br> Write an SQL query to evaluate the boolean expressions in Expressions table. <br> Return the result table in any order. <br> The query result format is in the following example. <br> Variables table: +------+-------+ | name | value | +------+-------+ | x | 66 | | y | 77 | +------+-------+ Expressions table: +--------------+----------+---------------+ | left_operand | operator | right_operand | +--------------+----------+---------------+ | x | > | y | | x | < | y | | x | = | y | | y | > | x | | y | < | x | | x | = | x | +--------------+----------+---------------+ Result table: +--------------+----------+---------------+-------+ | left_operand | operator | right_operand | value | +--------------+----------+---------------+-------+ | x | > | y | false | | x | < | y | true | | x | = | y | false | | y | > | x | true | | y | < | x | false | | x | = | x | true | +--------------+----------+---------------+-------+ As shown, you need find the value of each boolean exprssion in the table using the variables table. Ans. select left_operand as \"left_operand\", operator as \"operator\", right_operand as \"right_operand\", case when operator='>' then case when left_value>right_value then 'true' else 'false' end when operator='<' then case when left_value<right_value then 'true' else 'false' end when operator='=' then case when left_value=right_value then 'true' else 'false' end end as \"value\" from ( select a.left_operand , b.value as left_value, a.operator , a.right_operand , c.value as right_value from expressions a left join variables b on a.left_operand = b.name left join variables c on a.right_operand = c.name ); 9. Customers Who Bought Products A and B but Not C <br> https://leetcode.com/problems/customers-who-bought-products-a-and-b-but-not-c/submissions/ <br> Table: Customers <br> | Column Name | Type | |---------------------|---------| | customer_id | int | | customer_name | varchar | customer_id is the primary key for this table. <br> customer_name is the name of the customer. <br> Table: Orders | Column Name | Type | |---------------|---------| | order_id | int | | customer_id | int | | product_name | varchar | order_id is the primary key for this table. <br> customer_id is the id of the customer who bought the product \"product_name\". <br> Write an SQL query to report the customer_id and customer_name of customers who bought products \"A\", \"B\" but did not buy the product \"C\" since we want to recommend them buy this product. <br> Return the result table ordered by customer_id. <br> The query result format is in the following example. <br> Customers table: +-------------+---------------+ | customer_id | customer_name | +-------------+---------------+ | 1 | Daniel | | 2 | Diana | | 3 | Elizabeth | | 4 | Jhon | +-------------+---------------+ Orders table: +------------+--------------+---------------+ | order_id | customer_id | product_name | +------------+--------------+---------------+ | 10 | 1 | A | | 20 | 1 | B | | 30 | 1 | D | | 40 | 1 | C | | 50 | 2 | A | | 60 | 3 | A | | 70 | 3 | B | | 80 | 3 | D | | 90 | 4 | C | +------------+--------------+---------------+ Result table: +-------------+---------------+ | customer_id | customer_name | +-------------+---------------+ | 3 | Elizabeth | +-------------+---------------+ Only the customer_id with id 3 bought the product A and B but not the product C. Ans. <br> select customer_id, customer_name from ( select distinct a.customer_id, b.customer_name, a.product_name from orders a left join customers b on a.customer_id = b.customer_id where a.product_name in ('A','B','C') ) group by customer_id,customer_name having sum(case when product_name='A' then 1 when product_name='B' then 2 else 100 end)=3 order by customer_id; 10. Capital Gain/Loss <br> https://leetcode.com/problems/capital-gainloss/ <br> Table: Stocks <br> | Column Name | Type | |---------------|---------| | stock_name | varchar | | operation | enum | | operation_day | int | | price | int | (stock_name, day) is the primary key for this table. <br> The operation column is an ENUM of type ('Sell', 'Buy') <br> Each row of this table indicates that the stock which has stock_name had an operation on the day operation_day with the price. <br> It is guaranteed that each 'Sell' operation for a stock has a corresponding 'Buy' operation in a previous day. <br> Write an SQL query to report the Capital gain/loss for each stock. <br> The capital gain/loss of a stock is total gain or loss after buying and selling the stock one or many times. <br> Return the result table in any order. <br> The query result format is in the following example: <br> Stocks table: +---------------+-----------+---------------+--------+ | stock_name | operation | operation_day | price | +---------------+-----------+---------------+--------+ | Leetcode | Buy | 1 | 1000 | | Corona Masks | Buy | 2 | 10 | | Leetcode | Sell | 5 | 9000 | | Handbags | Buy | 17 | 30000 | | Corona Masks | Sell | 3 | 1010 | | Corona Masks | Buy | 4 | 1000 | | Corona Masks | Sell | 5 | 500 | | Corona Masks | Buy | 6 | 1000 | | Handbags | Sell | 29 | 7000 | | Corona Masks | Sell | 10 | 10000 | +---------------+-----------+---------------+--------+ Result table: +---------------+-------------------+ | stock_name | capital_gain_loss | +---------------+-------------------+ | Corona Masks | 9500 | | Leetcode | 8000 | | Handbags | -23000 | +---------------+-------------------+ Leetcode stock was bought at day 1 for 1000$ and was sold at day 5 for 9000$. Capital gain = 9000 - 1000 = 8000$. <br> Handbags stock was bought at day 17 for 30000$ and was sold at day 29 for 7000$. Capital loss = 7000 - 30000 = -23000$. <br> Corona Masks stock was bought at day 1 for 10$ and was sold at day 3 for 1010$. It was bought again at day 4 for 1000$ and was sold at day 5 for 500$. <br> At last, it was bought at day 6 for 1000$ and was sold at day 10 for 10000$. <br> Capital gain/loss is the sum of capital gains/losses for each ('Buy' --> 'Sell') operation <br> = (1010 - 10) + (500 - 1000) + (10000 - 1000) = 1000 - 500 + 9000 = 9500$. Ans. select stock_name, sum(case when operation ='Sell' then price else 0-price end) as capital_gain_loss from stocks group by stock_name order by stock_name; 12. Number of Trusted Contacts of a Customer <br> https://leetcode.com/problems/number-of-trusted-contacts-of-a-customer/ <br> Table: Customers <br> | Column Name | Type | |--------------|---------| | customer_id | int | | customer_name | varchar | | email | varchar | customer_id is the primary key for this table. <br> Each row of this table contains the name and the email of a customer of an online shop. <br> Table: Contacts <br> | Column Name | Type | |--------------|---------| | user_id | id | | contact_name | varchar | | contact_email | varchar | (user_id, contact_email) is the primary key for this table. <br> Each row of this table contains the name and email of one contact of customer with user_id. <br> This table contains information about people each customer trust. The contact may or may not exist in the Customers table. <br> Table: Invoices <br> | Column Name | Type | |--------------|---------| | invoice_id | int | | price | int | | user_id | int | invoice_id is the primary key for this table. <br> Each row of this table indicates that user_id has an invoice with invoice_id and a price. <br> Write an SQL query to find the following for each invoice_id: <br> customer_name: The name of the customer the invoice is related to. <br> price: The price of the invoice. <br> contacts_cnt: The number of contacts related to the customer. <br> trusted_contacts_cnt: The number of contacts related to the customer and at the same time they are customers to the shop. <br> (i.e His/Her email exists in the Customers table.) <br> Order the result table by invoice_id. <br> The query result format is in the following example: <br> Customers table: <br> | customer_id | customer_name | email | |-------------|---------------|--------------------| | 1 | Alice | alice@leetcode.com | | 2 | Bob | bob@leetcode.com | | 13 | John | john@leetcode.com | | 6 | Alex | alex@leetcode.com | Contacts table: | user_id | contact_name | contact_email | |-------------|--------------|--------------------| | 1 | Bob | bob@leetcode.com | | 1 | John | john@leetcode.com | | 1 | Jal | jal@leetcode.com | | 2 | Omar | omar@leetcode.com | | 2 | Meir | meir@leetcode.com | | 6 | Alice | alice@leetcode.com | Invoices table: <br> | invoice_id | price | user_id | |------------|-------|---------| | 77 | 100 | 1 | | 88 | 200 | 1 | | 99 | 300 | 2 | | 66 | 400 | 2 | | 55 | 500 | 13 | | 44 | 60 | 6 | Result table: | invoice_id | customer_name | price | contacts_cnt | trusted_contacts_cnt | |------------|---------------|-------|--------------|----------------------| | 44 | Alex | 60 | 1 | 1 | | 55 | John | 500 | 0 | 0 | | 66 | Bob | 400 | 2 | 0 | | 77 | Alice | 100 | 3 | 2 | | 88 | Alice | 200 | 3 | 2 | | 99 | Bob | 300 | 2 | 0 | Alice has three contacts, two of them are trusted contacts (Bob and John). <br> Bob has two contacts, none of them is a trusted contact. <br> Alex has one contact and it is a trusted contact (Alice). <br> John doesn't have any contacts. <br> Ans. select a.invoice_id , b.customer_name, a.price, (select count( ) from contacts where user_id = a.user_id) as contacts_cnt , (select count( ) from contacts where user_id = a.user_id and contact_email in (select email from customers) ) as trusted_contacts_cnt from invoices a left join customers b on a.user_id = b.customer_id order by invoice_id; -- Other Approach: Using Joins select a.invoice_id , b.customer_name, a.price, count(e.contact_email) as contacts_cnt , sum(case when e.email is not NULL then 1 else 0 end) as trusted_contacts_cnt from invoices a left join customers b on a.user_id = b.customer_id left join ( select c.user_id, c.contact_email, d.email from contacts c left join customers d on c.contact_email = d.email ) e on a.user_id = e.user_id group by a.invoice_id , b.customer_name, a.price order by a.invoice_id; 12. Activity Participants <br> https://leetcode.com/problems/activity-participants/ <br> Table: Friends <br> | Column Name | Type | |---------------|---------| | id | int | | name | varchar | | activity | varchar | id is the id of the friend and primary key for this table.<br> name is the name of the friend.<br> activity is the name of the activity which the friend takes part in.<br> Table: Activities <br> | Column Name | Type | |---------------|---------| | id | int | | name | varchar | id is the primary key for this table.<br> name is the name of the activity.<br> Write an SQL query to find the names of all the activities with neither maximum, nor minimum number of participants.<br> Return the result table in any order. Each activity in table Activities is performed by any person in the table Friends.<br> The query result format is in the following example:<br> Friends table: +------+--------------+---------------+ | id | name | activity | +------+--------------+---------------+ | 1 | Jonathan D. | Eating | | 2 | Jade W. | Singing | | 3 | Victor J. | Singing | | 4 | Elvis Q. | Eating | | 5 | Daniel A. | Eating | | 6 | Bob B. | Horse Riding | +------+--------------+---------------+ Activities table: +------+--------------+ | id | name | +------+--------------+ | 1 | Eating | | 2 | Singing | | 3 | Horse Riding | +------+--------------+ Result table: +--------------+ | activity | +--------------+ | Singing | +--------------+ Eating activity is performed by 3 friends, maximum number of participants, (Jonathan D. , Elvis Q. and Daniel A.) Horse Riding activity is performed by 1 friend, minimum number of participants, (Bob B.) Singing is performed by 2 friends (Victor J. and Jade W.) Ans. select activity from ( select activity, count( ) as act_count, min(count( )) over () as min_cnt, max(count(*)) over () as max_cnt from friends group by activity order by activity ) where min_cnt<act_count and act_count<max_cnt; 13. Second Degree Follower <br> https://leetcode.com/problems/second-degree-follower/submissions/ <br> In facebook, there is a follow table with two columns: followee, follower. <br> Please write a sql query to get the amount of each follower\u2019s follower if he/she has one. <br> For example: <br> | followee | follower | |-------------|------------| | A | B | | B | C | | B | D | | D | E | should output: | follower | num | |-------------|------------| | B | 2 | | D | 1 | Explaination: <br> Both B and D exist in the follower list, when as a followee, B's follower is C and D, and D's follower is E. <br> A does not exist in follower list. <br> Note: <br> Followee would not follow himself/herself in all cases. <br> Ans. <br> there could be duplicates in table, so use count distinct for counting followers. <br> select main as follower, count(distinct follower) as num from ( select a.follower as main, b.follower as follower from follow a inner join follow b on a.follower = b.followee ) group by main order by main; ```","title":"SQL Practise Questions"},{"location":"SQL/docs/sql_performance_tuning/","text":"SQL Performance Tuning : Summary Tip 1: Never use *(Star) to fetch all records from table Sql query become fast if you use actual columns instead of * to fetch all the records from the table. Not Recommended - Select * from Employee; Recommended Select Eno,Ename,Address from Employee; Tip 2: Try to avoid DISTINCT keyword from the query Try to avoid DISTINCT keyword from select statements. DISTINCT keyword has high cost and low performance. When anyone uses DISTINCT keyword, it first SORTS the data from column and then fetches the distinct values. Use EXISTS operator instead of DISTINCT keyword. Not Recommended: SELECT DISTINCT d.dept_no, d.department_name FROM Department d,Employee e WHERE d.dept_no= e.dept_no; Recommended: SELECT d.dept_no, d.department_name FROM Department d WHERE EXISTS ( SELECT \u2018X\u2019 FROM Employee e WHERE d.dept_no= e.dept_no); Tip 3: Carefully use WHERE conditions in sql Try to use correct operator as per requirement given. Not Recommended: Select * from Employee WHERE salary != 65000; Recommended: Select * from Employee WHERE salary > 65000 and salary < 65000; Tip 4: Use Like operator instead of equal to (=) Not Recommended: Select * from Employee where name=\u2019Amit\u2019; Recommended: Select * from Employee where name like \u2018Amit%\u2019; Tip 5: Avoid HAVING clause/GROUP BY statements HAVING clause and GROUP BY statements have high cost. So try to avoid it in sql query. Not Recommended - Select * from Employee WHERE name=\u2019Amit\u2019 GROUP BY department HAVING salary=45000; Recommended - Select * from Employee WHERE name=\u2019Amit\u2019 and salary=45000; More added: - Having clause- We use Having clause to eliminate some of the group values. Issue \u2013 Having clause restricts the rows AFTER they are read. So if no restriction in \u201cwhere clause\u201d, optimizer will use full table scan. This is really important coz all predicates in the HAVING Clause will not be used as access predicates. So they will not make optimizer use indexes, partitions etc. This is because to perform HAVING clause, it first reads all the rows and then eliminates unnecessary rows. Tip 6: Use of EXISTS and IN Operators Basically, Operator IN has lowest performance. IN operator is used when Filter criteria is in subquery, whereas EXISTS operator is used when filter criteria is in main query. Example: IN Operator Select * from Employee where Department_name IN ( Select Department_name from Department where Dno=10); Exist operator Select * from Employee where EXISTS ( Select Department_name from Department where Dno=10); More added When you run a query with IN clause, database will process it in below format \u2013 That is, in case of use of IN clause, each value of sub query is joined with outer query. Treats below Query - select * from T1 where x in (select x from T2); as - select * from t1, (select x from t2) T2 where t1.x = t2.x; But when you use EXIST clause, database will exit as soon as it gets the first match . So, in case of EXIST clause it runs executes query in below format \u2013 Treats below query - select * from T1 where exists (select x from T2 where t1.x=t2.x); as - FOR x IN (select * from t1) LOOP IF (EXISTS ( SELECT X FROM T2) ) THEN OUTPUT THE RECORD END IF; END; That is, using EXIST clause will imply database will run it like a FOR loop and as soon as match is found, it moves to next record. So which one is faster \u2013 IN or EXIST? a. This totally depends on situation. Use IN when - outer table = Big and Subquery = Small Use EXISTS when \u2013 outer table = Small and Subquery = Big b. Even above rules are not fixed. For example, if subquery has bigger table, but it has an index, in this case use of IN is suggested. c. So- EXISTS doesn\u2019t work better than IN all the times. IN is better than EXISTS if \u2013 outer table = Big and Subquery = Small outer table = Small and Subquery = Big + Indexed NOT EXISTS vs NOT IN \u2022 NOT EXISTS is not equivalent of NOT IN. \u2022 NOT EXISTS cannot be used instead of NOT IN all the times. \u2022 More specifically, if there is any NULL value in your data, they will show different result. \u2022 If your subquery returns even one NULL value, NOT IN will not match any rows. \u2022 On other hand, if you have a chance to use NOT EXISTS instead of NOT IN, you should test it. \u2022 In most database versions of oracle, EXISTS and IN are treated similarly in terms of execution plan. Tip 7: Try to use UNION ALL instead of UNION UNION scans all data first and then eliminate duplicate so it has slow performance. Not Recommended Select * from Employee where dept_no=10 UNION Select * from Employee where dept_no=20; Recommended Select * from Employee where dept_no=10 UNION ALL Select * from Employee where dept_no=20; Tip 8: Avoid use of Functions in Where condition. Not Recommended Select * from Employee where Substr(name,1,3)=\u2019Ami\u2019; Recommended Select * from Employee where name like \u2018Ami%\u2019; Tip 9: convert OR to AND If we use OR clause, it will PREVENT index usages. Instead, we should use AND where possible. Not Recommended select * from sales where prod_id = 13 or promo_id=14; Recommended select * from sales where prod_id = 13 UNION All select * from sales where promo_id=14 AND prod_id <> 13; Tip 10: Subquery Unnesting Nested queries are very costly, and so transformer tries to convert them to equivalent join statements. Not Recommended select * from sales where cust_id IN (select cust_id from customers); Recommended select sales.* from sales, customers where sales.cust_id=customers.cust_id; Tip 11: IN and BETWEEN select * from employees where emp_id in (2,3,4,5); The above is equivalent to select * from employees where emp_id = 2 OR emp_id=3 OR emp_id=4 OR emp_id=5 ---this implies full table scan. Solution - select * from employees where emp_id between 2 and 5; Tip 12: Fetching first N records Suppose we want to see only 10 records in our select statement output. There are 2 ways to do this \u2013 Using rownum (Recommended) SELECT * FROM EMPLOYEE where rownum<11; Using fetch first (not recommended) SELECT * FROM EMPLOYEE FETCH FIRST 10 ROWS ONLY; In case of rownum- Here it reads first 10 rows use count STOPKEY operator, and so faster than fetch first method. In case of fetch first \u2013 Here we read whole table, and then applied a windowing function to select 1st 10 records. Tip 13: UNION vs UNION ALL: UNION \u2013 combines data and drops duplicate rows. UNION ALL \u2013 combines data and retains duplicate rows. Suggest: Some key points- \u2022 by default, UNION ALL is less costly than UNION, as latter sorts data internally to remove duplicates. \u2022 But if your table is indexed, then sort operation in UNION wont be that costly, and so you can use UNION also. So \u2013 - Use UNION if table is indexed and you don\u2019t want duplicates in output. - Use UNION ALL if\u2013 - There is no duplicate in your data, or - You are ok with having duplicate data in output. - But overall, UNION ALL gives better performance than UNION. Tip 14: INTERSECT Vs EXISTS operator Intersect gives common rows of 2 intersection in a Sorted order. As part of intersect, 2 rows sources are first sorted, and then common records are fetched. In place of INTERSECT operator, we should try and use EXISTS clause, which is more efficient. One caveat is that, in case of EXISTS clause, output is not sorted, unlike in case of INTERSECT clause. Not Recommended SELECT employee_id FROM employees where employee_id between 145 and 179 INTERSECT SELECT employee_id FROM employees WHERE first_name LIKE 'A%'; Recommended SELECT employee_id FROM employees e where employee_id between 145 and 179 and exists (SELECT employee_id FROM employees t WHERE first_name LIKE 'A%' and e.employee_id = t.employee_id); Tip 15: MINUS Vs NOT EXISTS MINUS operator eliminates matched rows of 1st (with 2nd) and returns rest of the rows of 1st. NOT EXISTS clause can also do same work as MINUS, but has much better performance. Not Recommended SELECT employee_id FROM employees where employee_id between 145 and 179 MINUS SELECT employee_id FROM employees WHERE first_name LIKE 'A%'; Recommended SELECT employee_id FROM employees e where employee_id between 145 and 179 and not exists (SELECT employee_id FROM employees t WHERE first_name LIKE 'A%' and e.employee_id = t.employee_id); Tip 16: Using Like conditions To enable use of indexes, avoid use of wild card character at beginning of source text scan. If you are forced to use wild card character at beginning, you can create reverse index, and handle that problem using that index. In this case, when you reverse index, then source text will eliminate use of wild card at beginning. Eg \u2013 Suppose you want to find records where last name ends is \u201chhar\u201d, then create reverse() index on last_name to reverse it and then use condition where reversed last name begins with \u201crahh\u201d. Though reverse() index usage will have cost, but if your column is selective enough, it wont be much cost. Tip 17: Using Functions on Indexed Columns will suppress index usage. Use of function on indexed column will suppress index usage. So, rewrite query to avoid use of function. BAD QUERY select employee_id, first_name, last_name from employees where trunc(hire_date,'YEAR') = '01-JAN-2002'; GOOD: rewritten query select employee_id, first_name, last_name from employees where hire_date between '01-JAN-2002' and '31-DEC-2002'; Eg2 - Bad select * from mytable where substr(emp_name,1,2) = 'Po'; Good select * from mytable where emp_name like 'Po%'; Note - In Spark, we have HashAggregates and SortAggregates. Hash Aggregates are more performant, and work only on mutable data types. That is, if all elements in your Select clause (except those in Group by clause) are mutable types like INT, FLOAT, etc, then spark will use Hash Aggregates. This means, sometimes, for performance gain, we need to apply Function to transform values. See #14 at below link for details https://github.com/kushal-luthra/spark-development/blob/master/notes/spark_opimization.md Tip 18: Handling NULL Values Failing to deal with NULL values will lead to unintentional results or performance losses. Why - \u2022B-Tree indexes do not index NULL values. \u2022If there are any NULL values in your indexed columns and you need to get rows which have NULL values, optimizer will not use your index, and perform a full table scan instead. \u2022That is, having Null values in your index may sometimes suppress index usage. Ways to handle NULL value-based performance loss a. Use IS NOT NULL condition in your WHERE clause. Use IS NOT NULL condition in your WHERE clause if you don\u2019t need to have NULL values in result set. That is, even if you now your result will not be having any NULL values, you should use \u201cis not null \u201c clause to make optimizer use indexes. Eg \u2013 Query 1: select emp_name, emp_id from employee where emp_id <> 1; Query 2: select emp_name, emp_id from employee where emp_id <> 1 and emp_id is not null; In query 1, we will see FULL Table scan and in case of query 2, we see index-based scan, and lower query cost. b. Adding not null constraint to your columns and insert a specific value for NULL values like \u20180\u2019 if value in a column is null. c. If reasonable, create a BITMAP index instead of B-Tree index. BITMAP indexes store NULL values. So even if there are null values in our column, optimizer will be able to use our indexes. However, you need to take into consideration index efficiency between B-Tree and BITMAP, as former as more efficient than latter. - We use BITMAP indexes when \u2013 cardinality is LOW and index not modified often. - We use B-Tree index when \u2013 cardinality/selectivity is high. Tip 19 : Use Truncate instead of Delete Truncate is always faster than DELETE command. This is because when you run delete command, oracle database generates lots of UNDO data for deleted rows and generating UNDO data is an expensive operation. Truncate doesn\u2019t generate UNDO Data. But before using Truncate command, there are few things to note about it- \u2022 No rollback \u2013 Truncate operation cannot be rollbacked, and Flashback is also not so easy after truncate operations. You may need to use Flashback Data Archive or some other external tools in this case. \u2022 Truncate is a DDL operation \u2013 So when you run Truncate, your transaction will be committed. It performs commit before and after Truncate operation. Since it does 2 commits, and even if truncate operation fails in between, the changes you did before will be permanent in any case. \u2022 Truncate a partition - We don\u2019t need to truncate whole table all the times. You can truncate partition as well. \u2022 Truncate doesn\u2019t fire DML triggers - So you wont be able to log your truncate operation because of that. But it can fire the DDL triggers. \u2022 Truncate makes unusable indexes usable again. But delete does not. Tip 20: Data Type Mismatches If data types of column and compared value dont match, this may suppress index usage. select cust_id, cust_code from customers where cust_code = 101; Vs select cust_id, cust_code from customers where cust_code = '101'; Tip 21: Tuning Ordered queries- Order By clause Order by mostly requires SORT operations . This sort operation is done in PGA or in disk (if PGA doesn\u2019t have enough memory) This disk is shown as \u2018temporary table space\u2019 in execution plan. Issue \u2013 sorting in disk is a costly operation. Solution \u2013 \u2022 Create a B-Tree index on column used in Order by Clause, or \u2022 Modify a B-Tree index to include column used in Order by Clause. \u2022 Why\u2013> B-Tree indexes store columns in Order and using B-Tree index will eliminate sort operations. Tip 22 : Retrieving MIN and MAX Values B-Tree indexes increase the performance a lot for min and max value searches. If no B-Tree index, optimizer will need to read whole table. If our query has another column or another aggregate function in your query, it will be reading whole index or whole table. For example- When you see below, if we are looking for min() and max() values individually, output is just 2 for each. But when we want to get min() and max() together, database will read full table, and hence cost is 8 times. This is coz we have 2 aggregate functions in our query. Bad - select min(), max() from mytable; Good - select * FROM (select min() from mytable) min_cust, (select max() from mytable) max_cust; Tip 23 : Views Simple view = view created from single table. Complex view = view created by using multiple tables. Some suggestions w.r.t. views- 1. If you don\u2019t need to use all the tables in a view, then don\u2019t use the view. Instead use the actual tables. Otherwise, server will have to join all tables, do aggregation etc on them for a view. i.e. use view for the purpose for which it was created. 2. Else create another view. 3. Don\u2019t join complex views with a table or another view - This is because most of the times view is first executed completely at first, and then result is used as row source to other table or view. So, in this case you be reading lots of unnecessary data and performing unnecessary join and group by. This will increase cost a lot. 4. Avoid performing outer join to the views \u2013 because if you use equality predicate on view column, the optimizer gets wrong if the table of that column has an outer join in the view as well. Because outer join may not know performance of view and may lead to bad execution as well. E.g. \u2013 if we do outer join, optimizer may not be able to push predicate inside the view definition at times of execution plan. Materialized Views- Unlike basic and complex views, materialized views store both query and data. Materialized view data can be refreshed manually or via auto-refresh. But materialized view maintenance is a burden to database \u2013 it needs to be kept up to date for each modification on each change. As compared to normal views, materialized view will improve performance as we will select data directly from materialized view, and there will be no sorts, joins etc. We can create index, partitions etc on materialized view like in an ordinary table. Summary \u2022 If you don\u2019t need to use all the tables in a view, then don\u2019t use the view. Instead use the actual tables. \u2022 Don\u2019t join complex views with a table or another view. \u2022 Avoid performing outer join to the views. \u2022 Use Materialized View - Unlike basic and complex views, materialized views store both query and data. Materialized view data can be refreshed manually or via auto-refresh. But materialized view maintenance is a burden to database . Tip 24: Frequent commit is not desired make REDO logs bulky as we may be committing prior to period. make lock on modified rows, making them unavailable to other applications. Tip 25: Multitable DML operations (skip for big data) Sometimes we have to read same table as input to different tables in our data warehouse. So, if we have 5 different tables requiring input from 1 table, we should ideally be reading input table just once, and keep on feeding into different output tables as per requirements. For this we have 2 options \u2013 \u2022 INSERT ALL \u2022 MERGE INTO","title":"SQL Performance Tuning"},{"location":"SQL/docs/sql_performance_tuning/#sql-performance-tuning-summary","text":"","title":"SQL Performance Tuning : Summary"},{"location":"SQL/docs/sql_performance_tuning/#tip-1-never-use-star-to-fetch-all-records-from-table","text":"Sql query become fast if you use actual columns instead of * to fetch all the records from the table. Not Recommended - Select * from Employee; Recommended Select Eno,Ename,Address from Employee;","title":"Tip 1: Never use *(Star) to fetch all records from table"},{"location":"SQL/docs/sql_performance_tuning/#tip-2-try-to-avoid-distinct-keyword-from-the-query","text":"Try to avoid DISTINCT keyword from select statements. DISTINCT keyword has high cost and low performance. When anyone uses DISTINCT keyword, it first SORTS the data from column and then fetches the distinct values. Use EXISTS operator instead of DISTINCT keyword. Not Recommended: SELECT DISTINCT d.dept_no, d.department_name FROM Department d,Employee e WHERE d.dept_no= e.dept_no; Recommended: SELECT d.dept_no, d.department_name FROM Department d WHERE EXISTS ( SELECT \u2018X\u2019 FROM Employee e WHERE d.dept_no= e.dept_no);","title":"Tip 2: Try to avoid DISTINCT keyword from the query"},{"location":"SQL/docs/sql_performance_tuning/#tip-3-carefully-use-where-conditions-in-sql","text":"Try to use correct operator as per requirement given. Not Recommended: Select * from Employee WHERE salary != 65000; Recommended: Select * from Employee WHERE salary > 65000 and salary < 65000;","title":"Tip 3: Carefully use WHERE conditions in sql"},{"location":"SQL/docs/sql_performance_tuning/#tip-4-use-like-operator-instead-of-equal-to","text":"Not Recommended: Select * from Employee where name=\u2019Amit\u2019; Recommended: Select * from Employee where name like \u2018Amit%\u2019;","title":"Tip 4: Use Like operator instead of equal to (=)"},{"location":"SQL/docs/sql_performance_tuning/#tip-5-avoid-having-clausegroup-by-statements","text":"HAVING clause and GROUP BY statements have high cost. So try to avoid it in sql query. Not Recommended - Select * from Employee WHERE name=\u2019Amit\u2019 GROUP BY department HAVING salary=45000; Recommended - Select * from Employee WHERE name=\u2019Amit\u2019 and salary=45000; More added: - Having clause- We use Having clause to eliminate some of the group values. Issue \u2013 Having clause restricts the rows AFTER they are read. So if no restriction in \u201cwhere clause\u201d, optimizer will use full table scan. This is really important coz all predicates in the HAVING Clause will not be used as access predicates. So they will not make optimizer use indexes, partitions etc. This is because to perform HAVING clause, it first reads all the rows and then eliminates unnecessary rows.","title":"Tip 5: Avoid HAVING clause/GROUP BY statements"},{"location":"SQL/docs/sql_performance_tuning/#tip-6-use-of-exists-and-in-operators","text":"Basically, Operator IN has lowest performance. IN operator is used when Filter criteria is in subquery, whereas EXISTS operator is used when filter criteria is in main query. Example: IN Operator Select * from Employee where Department_name IN ( Select Department_name from Department where Dno=10); Exist operator Select * from Employee where EXISTS ( Select Department_name from Department where Dno=10); More added When you run a query with IN clause, database will process it in below format \u2013 That is, in case of use of IN clause, each value of sub query is joined with outer query. Treats below Query - select * from T1 where x in (select x from T2); as - select * from t1, (select x from t2) T2 where t1.x = t2.x; But when you use EXIST clause, database will exit as soon as it gets the first match . So, in case of EXIST clause it runs executes query in below format \u2013 Treats below query - select * from T1 where exists (select x from T2 where t1.x=t2.x); as - FOR x IN (select * from t1) LOOP IF (EXISTS ( SELECT X FROM T2) ) THEN OUTPUT THE RECORD END IF; END; That is, using EXIST clause will imply database will run it like a FOR loop and as soon as match is found, it moves to next record. So which one is faster \u2013 IN or EXIST? a. This totally depends on situation. Use IN when - outer table = Big and Subquery = Small Use EXISTS when \u2013 outer table = Small and Subquery = Big b. Even above rules are not fixed. For example, if subquery has bigger table, but it has an index, in this case use of IN is suggested. c. So- EXISTS doesn\u2019t work better than IN all the times. IN is better than EXISTS if \u2013 outer table = Big and Subquery = Small outer table = Small and Subquery = Big + Indexed NOT EXISTS vs NOT IN \u2022 NOT EXISTS is not equivalent of NOT IN. \u2022 NOT EXISTS cannot be used instead of NOT IN all the times. \u2022 More specifically, if there is any NULL value in your data, they will show different result. \u2022 If your subquery returns even one NULL value, NOT IN will not match any rows. \u2022 On other hand, if you have a chance to use NOT EXISTS instead of NOT IN, you should test it. \u2022 In most database versions of oracle, EXISTS and IN are treated similarly in terms of execution plan.","title":"Tip 6: Use of EXISTS and IN Operators"},{"location":"SQL/docs/sql_performance_tuning/#tip-7-try-to-use-union-all-instead-of-union","text":"UNION scans all data first and then eliminate duplicate so it has slow performance. Not Recommended Select * from Employee where dept_no=10 UNION Select * from Employee where dept_no=20; Recommended Select * from Employee where dept_no=10 UNION ALL Select * from Employee where dept_no=20;","title":"Tip 7: Try to use UNION ALL instead of UNION"},{"location":"SQL/docs/sql_performance_tuning/#tip-8-avoid-use-of-functions-in-where-condition","text":"Not Recommended Select * from Employee where Substr(name,1,3)=\u2019Ami\u2019; Recommended Select * from Employee where name like \u2018Ami%\u2019;","title":"Tip 8: Avoid use of Functions in Where condition."},{"location":"SQL/docs/sql_performance_tuning/#tip-9-convert-or-to-and","text":"If we use OR clause, it will PREVENT index usages. Instead, we should use AND where possible. Not Recommended select * from sales where prod_id = 13 or promo_id=14; Recommended select * from sales where prod_id = 13 UNION All select * from sales where promo_id=14 AND prod_id <> 13;","title":"Tip 9: convert OR to AND"},{"location":"SQL/docs/sql_performance_tuning/#tip-10-subquery-unnesting","text":"Nested queries are very costly, and so transformer tries to convert them to equivalent join statements. Not Recommended select * from sales where cust_id IN (select cust_id from customers); Recommended select sales.* from sales, customers where sales.cust_id=customers.cust_id;","title":"Tip 10: Subquery Unnesting"},{"location":"SQL/docs/sql_performance_tuning/#tip-11-in-and-between","text":"select * from employees where emp_id in (2,3,4,5); The above is equivalent to select * from employees where emp_id = 2 OR emp_id=3 OR emp_id=4 OR emp_id=5 ---this implies full table scan. Solution - select * from employees where emp_id between 2 and 5;","title":"Tip 11: IN and BETWEEN"},{"location":"SQL/docs/sql_performance_tuning/#tip-12-fetching-first-n-records","text":"Suppose we want to see only 10 records in our select statement output. There are 2 ways to do this \u2013 Using rownum (Recommended) SELECT * FROM EMPLOYEE where rownum<11; Using fetch first (not recommended) SELECT * FROM EMPLOYEE FETCH FIRST 10 ROWS ONLY; In case of rownum- Here it reads first 10 rows use count STOPKEY operator, and so faster than fetch first method. In case of fetch first \u2013 Here we read whole table, and then applied a windowing function to select 1st 10 records.","title":"Tip 12: Fetching first N records"},{"location":"SQL/docs/sql_performance_tuning/#tip-13-union-vs-union-all","text":"UNION \u2013 combines data and drops duplicate rows. UNION ALL \u2013 combines data and retains duplicate rows. Suggest: Some key points- \u2022 by default, UNION ALL is less costly than UNION, as latter sorts data internally to remove duplicates. \u2022 But if your table is indexed, then sort operation in UNION wont be that costly, and so you can use UNION also. So \u2013 - Use UNION if table is indexed and you don\u2019t want duplicates in output. - Use UNION ALL if\u2013 - There is no duplicate in your data, or - You are ok with having duplicate data in output. - But overall, UNION ALL gives better performance than UNION.","title":"Tip 13: UNION vs UNION ALL:"},{"location":"SQL/docs/sql_performance_tuning/#tip-14-intersect-vs-exists-operator","text":"Intersect gives common rows of 2 intersection in a Sorted order. As part of intersect, 2 rows sources are first sorted, and then common records are fetched. In place of INTERSECT operator, we should try and use EXISTS clause, which is more efficient. One caveat is that, in case of EXISTS clause, output is not sorted, unlike in case of INTERSECT clause. Not Recommended SELECT employee_id FROM employees where employee_id between 145 and 179 INTERSECT SELECT employee_id FROM employees WHERE first_name LIKE 'A%'; Recommended SELECT employee_id FROM employees e where employee_id between 145 and 179 and exists (SELECT employee_id FROM employees t WHERE first_name LIKE 'A%' and e.employee_id = t.employee_id);","title":"Tip 14: INTERSECT Vs EXISTS operator"},{"location":"SQL/docs/sql_performance_tuning/#tip-15-minus-vs-not-exists","text":"MINUS operator eliminates matched rows of 1st (with 2nd) and returns rest of the rows of 1st. NOT EXISTS clause can also do same work as MINUS, but has much better performance. Not Recommended SELECT employee_id FROM employees where employee_id between 145 and 179 MINUS SELECT employee_id FROM employees WHERE first_name LIKE 'A%'; Recommended SELECT employee_id FROM employees e where employee_id between 145 and 179 and not exists (SELECT employee_id FROM employees t WHERE first_name LIKE 'A%' and e.employee_id = t.employee_id);","title":"Tip 15: MINUS Vs NOT EXISTS"},{"location":"SQL/docs/sql_performance_tuning/#tip-16-using-like-conditions","text":"To enable use of indexes, avoid use of wild card character at beginning of source text scan. If you are forced to use wild card character at beginning, you can create reverse index, and handle that problem using that index. In this case, when you reverse index, then source text will eliminate use of wild card at beginning. Eg \u2013 Suppose you want to find records where last name ends is \u201chhar\u201d, then create reverse() index on last_name to reverse it and then use condition where reversed last name begins with \u201crahh\u201d. Though reverse() index usage will have cost, but if your column is selective enough, it wont be much cost.","title":"Tip 16: Using Like conditions"},{"location":"SQL/docs/sql_performance_tuning/#tip-17-using-functions-on-indexed-columns-will-suppress-index-usage","text":"Use of function on indexed column will suppress index usage. So, rewrite query to avoid use of function. BAD QUERY select employee_id, first_name, last_name from employees where trunc(hire_date,'YEAR') = '01-JAN-2002'; GOOD: rewritten query select employee_id, first_name, last_name from employees where hire_date between '01-JAN-2002' and '31-DEC-2002'; Eg2 - Bad select * from mytable where substr(emp_name,1,2) = 'Po'; Good select * from mytable where emp_name like 'Po%'; Note - In Spark, we have HashAggregates and SortAggregates. Hash Aggregates are more performant, and work only on mutable data types. That is, if all elements in your Select clause (except those in Group by clause) are mutable types like INT, FLOAT, etc, then spark will use Hash Aggregates. This means, sometimes, for performance gain, we need to apply Function to transform values. See #14 at below link for details https://github.com/kushal-luthra/spark-development/blob/master/notes/spark_opimization.md","title":"Tip 17: Using Functions on Indexed Columns will suppress index usage."},{"location":"SQL/docs/sql_performance_tuning/#tip-18-handling-null-values","text":"Failing to deal with NULL values will lead to unintentional results or performance losses. Why - \u2022B-Tree indexes do not index NULL values. \u2022If there are any NULL values in your indexed columns and you need to get rows which have NULL values, optimizer will not use your index, and perform a full table scan instead. \u2022That is, having Null values in your index may sometimes suppress index usage. Ways to handle NULL value-based performance loss a. Use IS NOT NULL condition in your WHERE clause. Use IS NOT NULL condition in your WHERE clause if you don\u2019t need to have NULL values in result set. That is, even if you now your result will not be having any NULL values, you should use \u201cis not null \u201c clause to make optimizer use indexes. Eg \u2013 Query 1: select emp_name, emp_id from employee where emp_id <> 1; Query 2: select emp_name, emp_id from employee where emp_id <> 1 and emp_id is not null; In query 1, we will see FULL Table scan and in case of query 2, we see index-based scan, and lower query cost. b. Adding not null constraint to your columns and insert a specific value for NULL values like \u20180\u2019 if value in a column is null. c. If reasonable, create a BITMAP index instead of B-Tree index. BITMAP indexes store NULL values. So even if there are null values in our column, optimizer will be able to use our indexes. However, you need to take into consideration index efficiency between B-Tree and BITMAP, as former as more efficient than latter. - We use BITMAP indexes when \u2013 cardinality is LOW and index not modified often. - We use B-Tree index when \u2013 cardinality/selectivity is high.","title":"Tip 18: Handling NULL Values"},{"location":"SQL/docs/sql_performance_tuning/#tip-19-use-truncate-instead-of-delete","text":"Truncate is always faster than DELETE command. This is because when you run delete command, oracle database generates lots of UNDO data for deleted rows and generating UNDO data is an expensive operation. Truncate doesn\u2019t generate UNDO Data. But before using Truncate command, there are few things to note about it- \u2022 No rollback \u2013 Truncate operation cannot be rollbacked, and Flashback is also not so easy after truncate operations. You may need to use Flashback Data Archive or some other external tools in this case. \u2022 Truncate is a DDL operation \u2013 So when you run Truncate, your transaction will be committed. It performs commit before and after Truncate operation. Since it does 2 commits, and even if truncate operation fails in between, the changes you did before will be permanent in any case. \u2022 Truncate a partition - We don\u2019t need to truncate whole table all the times. You can truncate partition as well. \u2022 Truncate doesn\u2019t fire DML triggers - So you wont be able to log your truncate operation because of that. But it can fire the DDL triggers. \u2022 Truncate makes unusable indexes usable again. But delete does not.","title":"Tip 19 : Use Truncate instead of Delete"},{"location":"SQL/docs/sql_performance_tuning/#tip-20-data-type-mismatches","text":"If data types of column and compared value dont match, this may suppress index usage. select cust_id, cust_code from customers where cust_code = 101; Vs select cust_id, cust_code from customers where cust_code = '101';","title":"Tip 20: Data Type Mismatches"},{"location":"SQL/docs/sql_performance_tuning/#tip-21-tuning-ordered-queries-order-by-clause","text":"Order by mostly requires SORT operations . This sort operation is done in PGA or in disk (if PGA doesn\u2019t have enough memory) This disk is shown as \u2018temporary table space\u2019 in execution plan. Issue \u2013 sorting in disk is a costly operation. Solution \u2013 \u2022 Create a B-Tree index on column used in Order by Clause, or \u2022 Modify a B-Tree index to include column used in Order by Clause. \u2022 Why\u2013> B-Tree indexes store columns in Order and using B-Tree index will eliminate sort operations.","title":"Tip 21: Tuning Ordered queries- Order By clause"},{"location":"SQL/docs/sql_performance_tuning/#tip-22-retrieving-min-and-max-values","text":"B-Tree indexes increase the performance a lot for min and max value searches. If no B-Tree index, optimizer will need to read whole table. If our query has another column or another aggregate function in your query, it will be reading whole index or whole table. For example- When you see below, if we are looking for min() and max() values individually, output is just 2 for each. But when we want to get min() and max() together, database will read full table, and hence cost is 8 times. This is coz we have 2 aggregate functions in our query. Bad - select min(), max() from mytable; Good - select * FROM (select min() from mytable) min_cust, (select max() from mytable) max_cust;","title":"Tip 22 : Retrieving MIN and MAX Values"},{"location":"SQL/docs/sql_performance_tuning/#tip-23-views","text":"Simple view = view created from single table. Complex view = view created by using multiple tables. Some suggestions w.r.t. views- 1. If you don\u2019t need to use all the tables in a view, then don\u2019t use the view. Instead use the actual tables. Otherwise, server will have to join all tables, do aggregation etc on them for a view. i.e. use view for the purpose for which it was created. 2. Else create another view. 3. Don\u2019t join complex views with a table or another view - This is because most of the times view is first executed completely at first, and then result is used as row source to other table or view. So, in this case you be reading lots of unnecessary data and performing unnecessary join and group by. This will increase cost a lot. 4. Avoid performing outer join to the views \u2013 because if you use equality predicate on view column, the optimizer gets wrong if the table of that column has an outer join in the view as well. Because outer join may not know performance of view and may lead to bad execution as well. E.g. \u2013 if we do outer join, optimizer may not be able to push predicate inside the view definition at times of execution plan. Materialized Views- Unlike basic and complex views, materialized views store both query and data. Materialized view data can be refreshed manually or via auto-refresh. But materialized view maintenance is a burden to database \u2013 it needs to be kept up to date for each modification on each change. As compared to normal views, materialized view will improve performance as we will select data directly from materialized view, and there will be no sorts, joins etc. We can create index, partitions etc on materialized view like in an ordinary table. Summary \u2022 If you don\u2019t need to use all the tables in a view, then don\u2019t use the view. Instead use the actual tables. \u2022 Don\u2019t join complex views with a table or another view. \u2022 Avoid performing outer join to the views. \u2022 Use Materialized View - Unlike basic and complex views, materialized views store both query and data. Materialized view data can be refreshed manually or via auto-refresh. But materialized view maintenance is a burden to database .","title":"Tip 23 : Views"},{"location":"SQL/docs/sql_performance_tuning/#tip-24-frequent-commit-is-not-desired","text":"make REDO logs bulky as we may be committing prior to period. make lock on modified rows, making them unavailable to other applications.","title":"Tip 24: Frequent commit is not desired"},{"location":"SQL/docs/sql_performance_tuning/#tip-25-multitable-dml-operations-skip-for-big-data","text":"Sometimes we have to read same table as input to different tables in our data warehouse. So, if we have 5 different tables requiring input from 1 table, we should ideally be reading input table just once, and keep on feeding into different output tables as per requirements. For this we have 2 options \u2013 \u2022 INSERT ALL \u2022 MERGE INTO","title":"Tip 25: Multitable DML operations (skip for big data)"},{"location":"SQL/docs/sql_performance_tuning_summary/","text":"SQL Performance Tuning : Summary Tip 1: Never use *(Star) to fetch all records from table Tip 2: Try to avoid DISTINCT keyword from the query Not Recommended: SELECT DISTINCT d.dept_no, d.department_name FROM Department d,Employee e WHERE d.dept_no= e.dept_no; Recommended: SELECT d.dept_no d.department_name FROM Department d WHERE EXISTS ( SELECT \u2018X\u2019 FROM Employee e WHERE d.dept_no= e.dept_no); Tip 3: Carefully use WHERE conditions in sql. Tip 4: Use Like operator instead of equal to (=) Tip 5: Avoid HAVING clause/GROUP BY statements Tip 6: Use of EXISTS and IN Operators Tip 7: Try to use UNION ALL instead of UNION as UNION scans all data first and then eliminate duplicate so it has slow performance. Tip 9: convert OR to AND Tip 10: Subquery Unnesting Tip 11: IN and BETWEEN Tip 12: Fetching first N records: SELECT * FROM EMPLOYEE where rownum<11 Tip 13: UNION vs UNION ALL: \u2022 by default, UNION ALL is less costly than UNION, as latter sorts data internally to remove duplicates. \u2022 But if your table is indexed, then sort operation in UNION wont be that costly, and so you can use UNION also. Tip 14: INTERSECT Vs EXISTS operator Tip 15: MINUS Vs NOT EXISTS Tip 16: Using Like conditions To enable use of indexes, avoid use of wild card character at beginning of source text scan. If you are forced to use wild card character at beginning, you can create reverse index, and handle that problem using that index. In this case, when you reverse index, then source text will eliminate use of wild card at beginning. Tip 17: Using Functions on Indexed Columns will suppress index usage. So, rewrite query to avoid use of function. BAD QUERY select employee_id, first_name, last_name from employees where trunc(hire_date,'YEAR') = '01-JAN-2002'; GOOD: rewritten query select employee_id, first_name, last_name from employees where hire_date between '01-JAN-2002' and '31-DEC-2002'; Eg2 - Bad select * from mytable where substr(emp_name,1,2) = 'Po'; Good select * from mytable where emp_name like 'Po%'; Tip 18: Handling NULL Values \u2022B-Tree indexes do not index NULL values. \u2022If there are any NULL values in your indexed columns and you need to get rows which have NULL values, optimizer will not use your index, and perform a full table scan instead. \u2022That is, having Null values in your index may sometimes suppress index usage. Solution - - Use IS NOT NULL condition in your WHERE clause. - Adding not null constraint to your columns and insert a specific value for NULL values like \u20180\u2019 if value in a column is null. - If reasonable, create a BITMAP index instead of B-Tree index. Tip 19 : Use Truncate instead of Delete Tip 20: Data Type Mismatches If data types of column and compared value dont match, this may suppress index usage. Tip 21: Tuning Ordered queries- Order By clause Order by mostly requires sort operations. This sort operation is done in PGA or in disk (if PGA doesn\u2019t have enough memory) This disk is shown as \u2018temporary table space\u2019 in execution plan. Issue \u2013 sorting in disk is a costly operation. Solution \u2013 \u2022 Create a B-Tree index on column used in Order by Clause, or \u2022 Modify a B-Tree index to include column used in Order by Clause. \u2022 Why \u2013 B-Tree indexes store columns in Order and using B-Tree index will eliminate sort operations. Tip 22 : Retrieving MIN and MAX Values B-Tree indexes increase the performance a lot for min and max value searches. If no B-Tree index, optimizer will need to read whole table. Bad - select min(), max() from mytable; Good - select * FROM (select min() from mytable) min_cust, (select max() from mytable) max_cust; Tip 23 : Views \u2022 If you don\u2019t need to use all the tables in a view, then don\u2019t use the view. Instead use the actual tables. \u2022 Don\u2019t join complex views with a table or another view. \u2022 Avoid performing outer join to the views. \u2022 Unlike basic and complex views, materialized views store both query and data. Materialized view data can be refreshed manually or via auto-refresh. But materialized view maintenance is a burden to database . Tip 24: Frequent commit is not desired: make REDO logs bulky as we may be committing prior to period. make lock on modified rows, making them unavailable to other applications.","title":"SQL Performance Tuning Summary"},{"location":"SQL/docs/sql_performance_tuning_summary/#sql-performance-tuning-summary","text":"","title":"SQL Performance Tuning : Summary"},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-1-never-use-star-to-fetch-all-records-from-table","text":"","title":"Tip 1: Never use *(Star) to fetch all records from table"},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-2-try-to-avoid-distinct-keyword-from-the-query","text":"Not Recommended: SELECT DISTINCT d.dept_no, d.department_name FROM Department d,Employee e WHERE d.dept_no= e.dept_no; Recommended: SELECT d.dept_no d.department_name FROM Department d WHERE EXISTS ( SELECT \u2018X\u2019 FROM Employee e WHERE d.dept_no= e.dept_no);","title":"Tip 2: Try to avoid DISTINCT keyword from the query "},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-3-carefully-use-where-conditions-in-sql","text":"","title":"Tip 3: Carefully use WHERE conditions in sql. "},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-4-use-like-operator-instead-of-equal-to","text":"","title":"Tip 4: Use Like operator instead of equal to (=)"},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-5-avoid-having-clausegroup-by-statements","text":"","title":"Tip 5: Avoid HAVING clause/GROUP BY statements"},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-6-use-of-exists-and-in-operators","text":"","title":"Tip 6: Use of EXISTS and IN Operators"},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-7-try-to-use-union-all-instead-of-union-as-union-scans-all-data-first-and-then-eliminate-duplicate-so-it-has-slow-performance","text":"","title":"Tip 7: Try to use UNION ALL instead of UNION as UNION scans all data first and then eliminate duplicate so it has slow performance."},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-9-convert-or-to-and","text":"","title":"Tip 9: convert OR to AND"},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-10-subquery-unnesting","text":"","title":"Tip 10: Subquery Unnesting"},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-11-in-and-between","text":"","title":"Tip 11: IN and BETWEEN"},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-12-fetching-first-n-records-select-from-employee-where-rownum11","text":"","title":"Tip 12: Fetching first N records:  SELECT * FROM EMPLOYEE where rownum&lt;11"},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-13-union-vs-union-all","text":"\u2022 by default, UNION ALL is less costly than UNION, as latter sorts data internally to remove duplicates. \u2022 But if your table is indexed, then sort operation in UNION wont be that costly, and so you can use UNION also.","title":"Tip 13: UNION vs UNION ALL:"},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-14-intersect-vs-exists-operator","text":"","title":"Tip 14: INTERSECT Vs EXISTS operator"},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-15-minus-vs-not-exists","text":"","title":"Tip 15: MINUS Vs NOT EXISTS"},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-16-using-like-conditions","text":"To enable use of indexes, avoid use of wild card character at beginning of source text scan. If you are forced to use wild card character at beginning, you can create reverse index, and handle that problem using that index. In this case, when you reverse index, then source text will eliminate use of wild card at beginning.","title":"Tip 16: Using Like conditions"},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-17-using-functions-on-indexed-columns-will-suppress-index-usage","text":"So, rewrite query to avoid use of function. BAD QUERY select employee_id, first_name, last_name from employees where trunc(hire_date,'YEAR') = '01-JAN-2002'; GOOD: rewritten query select employee_id, first_name, last_name from employees where hire_date between '01-JAN-2002' and '31-DEC-2002'; Eg2 - Bad select * from mytable where substr(emp_name,1,2) = 'Po'; Good select * from mytable where emp_name like 'Po%';","title":"Tip 17: Using Functions on Indexed Columns will suppress index usage."},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-18-handling-null-values","text":"\u2022B-Tree indexes do not index NULL values. \u2022If there are any NULL values in your indexed columns and you need to get rows which have NULL values, optimizer will not use your index, and perform a full table scan instead. \u2022That is, having Null values in your index may sometimes suppress index usage. Solution - - Use IS NOT NULL condition in your WHERE clause. - Adding not null constraint to your columns and insert a specific value for NULL values like \u20180\u2019 if value in a column is null. - If reasonable, create a BITMAP index instead of B-Tree index.","title":"Tip 18: Handling NULL Values"},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-19-use-truncate-instead-of-delete","text":"","title":"Tip 19 : Use Truncate instead of Delete"},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-20-data-type-mismatches","text":"If data types of column and compared value dont match, this may suppress index usage.","title":"Tip 20: Data Type Mismatches"},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-21-tuning-ordered-queries-order-by-clause","text":"Order by mostly requires sort operations. This sort operation is done in PGA or in disk (if PGA doesn\u2019t have enough memory) This disk is shown as \u2018temporary table space\u2019 in execution plan. Issue \u2013 sorting in disk is a costly operation. Solution \u2013 \u2022 Create a B-Tree index on column used in Order by Clause, or \u2022 Modify a B-Tree index to include column used in Order by Clause. \u2022 Why \u2013 B-Tree indexes store columns in Order and using B-Tree index will eliminate sort operations.","title":"Tip 21: Tuning Ordered queries- Order By clause"},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-22-retrieving-min-and-max-values","text":"B-Tree indexes increase the performance a lot for min and max value searches. If no B-Tree index, optimizer will need to read whole table. Bad - select min(), max() from mytable; Good - select * FROM (select min() from mytable) min_cust, (select max() from mytable) max_cust;","title":"Tip 22 : Retrieving MIN and MAX Values"},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-23-views","text":"\u2022 If you don\u2019t need to use all the tables in a view, then don\u2019t use the view. Instead use the actual tables. \u2022 Don\u2019t join complex views with a table or another view. \u2022 Avoid performing outer join to the views. \u2022 Unlike basic and complex views, materialized views store both query and data. Materialized view data can be refreshed manually or via auto-refresh. But materialized view maintenance is a burden to database .","title":"Tip 23 : Views"},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-24-frequent-commit-is-not-desired","text":"make REDO logs bulky as we may be committing prior to period. make lock on modified rows, making them unavailable to other applications.","title":"Tip 24: Frequent commit is not desired:"},{"location":"random/cold_mails/","text":"Cold Mails Tactics \ud83d\udfe2 Run the triple: Start every sequence with an email, social touch, and cold call. \ud83d\udfe2 5x5x5: 5 minutes of research. Find 5 insights on the person/account. Write your message in 5 minutes. \ud83d\udfe2 Subject lines shouldn\u2019t read like an advertisement. They should be 4 words max, not a complete sentence, and without punctuation. \ud83d\udfe2 Don't equate formality with professionalism. You don't need to address me as \"Dear Mr. Farrokh\"). Use conjunctions and conversational language \ud83d\udfe2 Follow the 3x3 Rule for email: 3 paragraphs, no more than 3 lines each when read on your phone. \ud83d\udfe2 Don't try to sell everything under the sun in your email. One pain point per email, or you risk losing them. Mix in multiple pain points across the sequence. \ud83d\udfe2 Avoid large images, multiple hyperlinks, and big attachments. These add-ons scream \"marketing email\" and hurt deliverability. \ud83d\udfe2 Use Interest-based CTAs. \"Open to learning more?\" is better than \u201cHow's Thursday at 4:00 for a discovery call?\u201d \ud83d\udfe2 Your emails should read like text messages. If you read it out loud and sound like a robot, you\u2019re doing it wrong. \ud83d\udfe2 Tie VMs to emails: Leave voicemails that reference your emails and vice-versa. Even if they never get answered, voicemails should boost email replies. Credits : Armand Farokh","title":"Cold Mail Tactics"},{"location":"random/cold_mails/#cold-mails-tactics","text":"\ud83d\udfe2 Run the triple: Start every sequence with an email, social touch, and cold call. \ud83d\udfe2 5x5x5: 5 minutes of research. Find 5 insights on the person/account. Write your message in 5 minutes. \ud83d\udfe2 Subject lines shouldn\u2019t read like an advertisement. They should be 4 words max, not a complete sentence, and without punctuation. \ud83d\udfe2 Don't equate formality with professionalism. You don't need to address me as \"Dear Mr. Farrokh\"). Use conjunctions and conversational language \ud83d\udfe2 Follow the 3x3 Rule for email: 3 paragraphs, no more than 3 lines each when read on your phone. \ud83d\udfe2 Don't try to sell everything under the sun in your email. One pain point per email, or you risk losing them. Mix in multiple pain points across the sequence. \ud83d\udfe2 Avoid large images, multiple hyperlinks, and big attachments. These add-ons scream \"marketing email\" and hurt deliverability. \ud83d\udfe2 Use Interest-based CTAs. \"Open to learning more?\" is better than \u201cHow's Thursday at 4:00 for a discovery call?\u201d \ud83d\udfe2 Your emails should read like text messages. If you read it out loud and sound like a robot, you\u2019re doing it wrong. \ud83d\udfe2 Tie VMs to emails: Leave voicemails that reference your emails and vice-versa. Even if they never get answered, voicemails should boost email replies. Credits : Armand Farokh","title":"Cold Mails Tactics"}]}