{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Data Engineering - 101 Index Concepts Data Warehousing Concepts ELK Database Vs Datawarehouse Vs Datalake | ETL Vs ELT OOPs concepts OOPs in Python SQL SQL Performance Tuning Page SQL Performance Tuning Summary Common Table Expressions (CTEs) SQL Analytical Functions SQL Practise Questions Storage Layer Row-based Vs Column-based File Formats Text-based File Formats Big Data File Formats File Compression Techniques in Big Data Systems PyTest overview PyTest overview SOLID Principles Introduction to SOLID Principles Single Responsibility Principle Open Closed Principle Productivity Hacks Cold Mail Tactics LinkedIn Growth Top 10 hacks to be a Bad developer Jeff Bezos on Amazon Leadership principles About Me Author : Kushal Luthra","title":"Home"},{"location":"#welcome-to-data-engineering-101","text":"","title":"Welcome to Data Engineering - 101"},{"location":"#index","text":"Concepts Data Warehousing Concepts ELK Database Vs Datawarehouse Vs Datalake | ETL Vs ELT OOPs concepts OOPs in Python SQL SQL Performance Tuning Page SQL Performance Tuning Summary Common Table Expressions (CTEs) SQL Analytical Functions SQL Practise Questions Storage Layer Row-based Vs Column-based File Formats Text-based File Formats Big Data File Formats File Compression Techniques in Big Data Systems PyTest overview PyTest overview SOLID Principles Introduction to SOLID Principles Single Responsibility Principle Open Closed Principle Productivity Hacks Cold Mail Tactics LinkedIn Growth Top 10 hacks to be a Bad developer Jeff Bezos on Amazon Leadership principles About Me Author : Kushal Luthra","title":"Index"},{"location":"aboutme/","text":"Kushal is currently Lead Engineer at Airtel Africa Digital Labs (BigData & Analytics Team). He is the Lead for Business Intelligence product, which is being built from scratch, and is aimed to be scalable (handles data that can be in Gigabytes and Terabytes) and replicable across 14 OpCos. Efficient team leader and mentor with a history of managing multiple squads. Expert in providing data driven strategic approach and creating reusable and scalable models. Building strategic assets to set up a long term sustainable operational and capability model in technology strategy arena. He has extensive experience that spans across various technologies, including Python, Spark(PySpark), SQL, Hive, Hadoop, Apache Hudi, Airflow, Sqoop, Gitlab, BItBucket, CICD. He also has experience of migrating Data Solutions on legacy system to Big Data Stack. He has built data pipelines from scratch, with focus on data frameworks. Domain : Retail and Telecom Distributed Computing: Hadoop, HDFS, Yarn, Spark Programming Languages: Python, Scala Operating System: Linux, Unix Development Tools: JIRA Databases: Postgres, MongoDB, Oracle Exadata Methodologies: Agile/Scrum Open to hearing about exciting information/opportunities in meaningful industries, more tech connections and both mentor/mentee relationships.","title":"About Me"},{"location":"OOPs/OOPs%20in%20Python/","text":"OOPS in python Key terminologies Classes are logical collection of attributes and methods; Objects are instance of class. Actions performed are covered in Methods To access attribute of your class, use self parameter Process of creating your object is called object instantiation Everything in python is an object. eg - mylist = [1,2,3] --if we do type() on this, it returns o/t as 'class list'. That means, list is a class in python, and above mylist is an object of class list. So, if you call function, say, append, then it invokes method of class list. mylist.append(4) -> [1,2,3,4] Attributes Attributes - an attribute is a property that further defines a class. What is a class attribute? a class attribute are attributes that are shared across all instances of a class. eg - no. of working hours. they are created either as a part of the class or by using className.attributeName. so if edited or modified, then reflected for all object instances. How to edit - to access them, use ClassName.ClassAttributeName and change value. eg - class Employee: HoursOfWork = 40 x=Employee() x.HoursOfWork --> output = 40 Now edit HoursOfWork --> Employee.HoursOfWork = 30 x.HoursOfWork --> output = 30 We can also edit class attribute specific for an object instead for class as a whole. eg - class Employee: HoursOfWork = 40 x=Employee() x.HoursOfWork --> output = 40 Now edit HoursOfWork --> x.HoursOfWork = 50 x.HoursOfWork --> output is 50 --> here we created another instance attribute called HoursOfWork(class attribute remains unchanged) y=Employee() y.HoursOfWork --> output = 40 What is Instance attribute? Instance attribute are attributes that are specific to each instance of a class. eg - class Employee: HoursOfWork = 40 x=Employee() x.name = \"ram\" --> creates an instance attribute for object 'x' x.name --> output = ram y=Employee() y.name --> give error coz no instance attribute associated with 'y' They are created using objectName.attributeName So, if edited or modified, then reflected only for that object instance. How self parameter is handled? The method call objectName.methodName() is interpreted as className.methodName(objectName), and this parameter is referred to as ' self ' in method definition. Methods Attributes - an attribute is a property that further defines a class. Method - a method is a function within a class which can access all the attributes of a class and perform a specific task. What is Instance Method? Methods that accept self parameter as default parameter. They are used to modify instance attributes. What is a static method? Methods that do not modify instance attributes of a class are called Static Methods. But they can be used to modify Class Attributes. What is init() method? init() method is the initializer in python. It is called when an object is instantiated. All the attributes of the class should be initialized in this method to make your object a fully initialized object. It has the default parameter as the object are static methods. self Variable When you call any class method, then by default, first argument that is passed is class object name. eg - class Employee: def employeeDetails() pass employeOne = Employee() employeOne.employeeDetails() is same as employeeOne.emplpyeDetails(employeeOne) By using self parameter, you are referring to attributes of that object, and the values persist until lifespan of program or until you manually delete the object. Eg - class Employee: def employeeDetails(self): self.name = \"kush\" print(\"name = \", self.name) age = 30 --> since we didnt use \"self\", the value age is not available to other methods print(\"age = \",age) def printEmployeeDetails(self): print(\"my name = \", self.name) print(\"my age = \",age) --> here it will throw error since \"age\" is not available to other functions. Its scope is restricted to method employeeDetails. Static Methods and Instance Methods Instance Methods - They are methods of our class that make use of self parameter and modify instance attributes of our class. Static Methods- - They are methods that do not take default self parameter. - To differentiate between static and instance method, we use decorator @staticmethod Decorators are functions that take another function as input and extend their functionality. They are denoted by starting them with '@' symbol. For example - @staticmethod is a decorator that takes the function as message, extends its functionality and ignores the binding of the object. class Employee: @staticmethod def WelcomeMessage(): print(\"welcome to our organization\") init() method - init() method is a special method in python which is used to initialize an object. eg - class Employee: def setName(self): self.name=\"ram\" def printName(self): print(self.name) x=employee() x.printName() --> it gives error coz we havent initialized name. init() method is first method to be called in a class. class Employee: def __init__(self, name): self.name = name def printName(self): print(self.name) x=employee(\"ram\") x.printName() --> output is Ram y=employee(\"shyam\") y.printName() --> output is shyam Abstraction and Encapsulation Abstraction is the process of hiding implementation details from the user. Encapsulation is done to achieve abstraction. You use method of class via abstraction. Inheritance allows code re-usability. Type 1 : single level inheritance eg 1 - dad = good baseball player. son = good baseball player. eg 2 - class Apple: manufacturer = \"Apple Inc.\" contactWebsite = \"www.apple.com/contact\" def contactDetails(self): print(\"To contact us, log on to \", self.contactWebsite ) class MacBook(Apple): def __init__(self): self.yearOfManufacture = 2017 def manufactureDetails(self): print(\"This macbook was manufactured in year {} by {}\" .format(self.yearOfManufacture, self.manufacturer)\") mymacbook = MacBook() # output = This macbook was manufactured in year 2017 by Apple Inc. mymacbook.manufactureDetails # To contact us, log on to www.apple.com/contact mymacbook.contactDetails Type2: Multiple Inheritances eg 1 - dad = good baseball player. mom = good cook. son = good baseball player and good cook eg 2 - class OperatingSystem: multitasking = True name = \"Windows OS\" Class Apple: website = \"www.apple.com\" name = \"Apple OS\" Class MacBook(OperatingSystem, Apple): def __init__(self): if self.multitasking is True: # output = this is a multitasking system. Visit www.apple.com for more details. print(\"this is a multi tasking system. Visit {} for more details\".format(self.website)) #output = Window OS --why - coz we inherited OperatingSystem class first, and so variable present in both classes will be picked up from first class. print(\"name = \".self.name) Class NewMacBook(Apple, OperatingSystem): def __init__(self): if self.multitasking is True: # output = this is a multitasking system. Visit www.apple.com for more details. print(\"this is a multi tasking system. Visit {} for more details\".format(seld.website)) # output = Apple OS --why - coz we inherited Apple class first, and so variable present in both classes will be picked up from first class. print(\"name = \".self.name) Type 3: multi level inheritance eg 1- grandfather = good baseball player father = good soccer player son = good baseball and soccer player eg 2- class MusicalInstruments: numberOfMajorKeys = 12 class StringInstruments(MusicalInstruments): typeOfWood = \"ToneWood\" class Guitar(StringInstruments): def __init__(self): self.numberOfStrings = 6 print(\"this guitar consists of {} strings. It is made of {} and it can play {} keys\" .format(self.numberOfStrings, self.typeOfWood, self.numberOfMajorKeys)) guitar = Guitar() output - This guitar consists of 6 strings. It is made of ToneWood and can play 12 keys Access Specifiers - public, private and protected public - your class, derived class, and further sub-classes derived from derived class --all can access values in main class. protected - only your class and derived class members can access data and methods of your class. private- only your class can access methods and attributes of your class. To declare attribute or method as - 1. public - MemberName 2. Protected - _memberName --> single underscore. 3. Private - __MemberName --> double underscore. class Car: numberOfWheels = 4 _color = \"Black\" __YearOfManufacture = 2018 #python saves it as _Car__yearOfManufacture i.e _ClassName__AttributeName Class BMW(Car): def __init__(self): print(\"Protected attrubte color : \", self._color) car=Car() print(\"Public Attribute numberOfWheels: \", self.car.NumberOfWheels) --> output = Public Attribute NumberOfWheels: 4 bmw = BMW() --> output = Protected attribute color : Black print(\"Private Attribute yearOfManufacture: \", car.__yearOfManufacture ) --> output = error print(\"Private Attribute yearOfManufacture: \", car._Car__yearOfManufacture) --> output = Private attribute YearOfManufacture : 2018 PolyMorphism It is characteristic of an entity to be able to be present in more than one form. eg - \"+\" operator when applied to 2 numbers returns their sum, but whe applied to 2 strings, returns their concatenation. Overriding and Super() method Overriding = redefine base class method in your derived class. super() function allows you to transfer control back to base class in a derived class. It is used in case you want to access method in base class. class Employee: def setNumberOfWorkingHours(self): self.NumberOfWorkingHours = 10 def displayNumberOfWorkingHours(self): print(self.NumberOfWorkingHours) employee = Employee() employee.setNumberOfWorkingHours() print(\"Number of working hours of emplyee = \", end = ' ') employee.displayNumberOfWorkingHours() output - Number of working hours of Employee = 10 class Trainee(Employee): def setNumberOfWorkingHours(self): self.NumberOfWorkingHours = 20 def resetNumberOfWorkingHours(self): super().setNumberOfWorkingHours() trainee = Trainee() trainee.setNumberOfWorkingHours() print(\"Number of working hours of Trainee = \", end = ' ') trainee.displayNumberOfWorkingHours() trainee.resetNumberOfWorkingHours() print(\"Number of working hours of Trainee after reset = \", end = ' ') trainee.displayNumberOfWorkingHours() output - Number of working hours of Trainee = 20 Number of working hours of Trainee after reset = 10 Diamond Problem Here we have 4 classes - A, B, C and D - A is base class - B anc C derived from A - D is derived from B and C Case 1: Method() will Not be overridden in Class B and Class C class A: def method(self): print(\"this method belongs to class A\") pass class B(A): pass class C(A): pass class D(B,C): pass d = D() d.method() output --> This method belongs to class A Case 2: Method() will be overridden in Class B but not in Class C class A: def method(self): print(\"this method belongs to class A\") pass class B(A): def method(self): print(\"this method belongs to class B\") class C(A): pass class D(B,C): pass d = D() d.method() output --> This method belongs to class B Case 3: Method() will be overridden in Class C but not in Class B class A: def method(self): print(\"this method belongs to class A\") class B(A): pass class C(A): def method(self): print(\"this method belongs to class C\") class D(B,C): pass d = D() d.method() output --> This method belongs to class C Case 4: Method() will be overridden in both Class B and Class C class A: def method(self): print(\"this method belongs to class A\") class B(A): def method(self): print(\"this method belongs to class B\") class C(A): def method(self): print(\"this method belongs to class C\") class D(B,C): pass d = D() output -> This method belongs to class B why this output -> Method Resolution Order -- in case of derived class from multiple classes, method picked from 1st class to which it is obtained; Had we defined class D as below, output would have been different. class D(C,B): pass d = D() d.method() --> output = This method belongs to class C Operator Overloading There are internal functions of python. They are overridden in following way - eg - overloading addition operator, the \"+\" operator when - applied to 2 numbers returns their sum. - applied to 2 strings, returns their concatenation. class Square: def __init__(self, side): self.side = side sq1 = Square(5) sq2 = Square(10) # output -> ERROR # Why -> sq1 + sq2 = error coz '+' operator doesn't work on object of class square print(\"the sum of sides of both the squares = \", sq1 + sq2) So, we need a mechanism to add sides of square and return value. That should allow us to add above 2 square objects as well. Solution -> overload special method add() in your class class Square: def __init__(self, side): self.side = side def __add__(square1, square2): return( (4 * square1.side) + (4 * square2.side) ) sq1 = Square(5) sq2 = Square(10) # output -> 60 print(\"the sum of sides of both the squares = \", sq1 + sq2) Abstract Base Class An Abstract Base Class is a base class which doesn't have a definition of its own. It has abstract methods which forces its implementation in derived classes To declare a class as abstract base class , it needs to be declared as instance of ABCMeta class of python . eg - # we import ABCMeta and abstractmethod from abc module of python from abc import ABCMeta, abstractmethod class shape(metaclass = ABCMeta): #decorator abstractmethod tells that given method is required to be defined in derived classes. @abstractmethod def area(self): return 0 #here we say that method area() does not have a defn of its own; but it needs to be defined in derived classes. class Square(shape): side = 4 def area(self): print(\"area of square\", self.side * self.side) class Rectangle(Shape): width = 5 breadth = 10 def area(self): print(\"area of rectangle \" , self.width * self.length) sq = Square() rc = Rectangle() sq.area() rc.area() Note - if no definition of method area() in class Square and even if we don't call sq.area() --> we get error An abstract can only be inherited in derived classes, but cannot be instantiated. If we try to create object of abstract class, we get error. sh = Shape() Summary - polymorphism = ability of an entity to be able to exist in more than one form. - overriding = form of polymorphism in which we redefine method of base class in a derived class. - By doing this we change behaviour of a base class method. - How - class BaseClass: def BaseClassMethod(): #define behaviour class DerivedClass(BaseClass): def baseClassMethod(): #redefine behaviour Operator Overloading - defining a special method for an operator within your class to handle the operation between the objects of that class is called Operator Overloading . Abstract Base Class - a base class which consists of abstract methods that should be implemented in its derived class is called abstract base class. Syntax - from abc import ABCMeta, abstractmethod class baseClass(metaclass = ABCMeta): @abstractmethod def abstractMethod(self): return","title":"OOPs in Python"},{"location":"OOPs/OOPs%20in%20Python/#oops-in-python","text":"","title":"OOPS in python"},{"location":"OOPs/OOPs%20in%20Python/#key-terminologies","text":"Classes are logical collection of attributes and methods; Objects are instance of class. Actions performed are covered in Methods To access attribute of your class, use self parameter Process of creating your object is called object instantiation Everything in python is an object. eg - mylist = [1,2,3] --if we do type() on this, it returns o/t as 'class list'. That means, list is a class in python, and above mylist is an object of class list. So, if you call function, say, append, then it invokes method of class list. mylist.append(4) -> [1,2,3,4]","title":"Key terminologies"},{"location":"OOPs/OOPs%20in%20Python/#attributes","text":"Attributes - an attribute is a property that further defines a class.","title":"Attributes"},{"location":"OOPs/OOPs%20in%20Python/#what-is-a-class-attribute","text":"a class attribute are attributes that are shared across all instances of a class. eg - no. of working hours. they are created either as a part of the class or by using className.attributeName. so if edited or modified, then reflected for all object instances. How to edit - to access them, use ClassName.ClassAttributeName and change value. eg - class Employee: HoursOfWork = 40 x=Employee() x.HoursOfWork --> output = 40 Now edit HoursOfWork --> Employee.HoursOfWork = 30 x.HoursOfWork --> output = 30 We can also edit class attribute specific for an object instead for class as a whole. eg - class Employee: HoursOfWork = 40 x=Employee() x.HoursOfWork --> output = 40 Now edit HoursOfWork --> x.HoursOfWork = 50 x.HoursOfWork --> output is 50 --> here we created another instance attribute called HoursOfWork(class attribute remains unchanged) y=Employee() y.HoursOfWork --> output = 40","title":"What is a class attribute?"},{"location":"OOPs/OOPs%20in%20Python/#what-is-instance-attribute","text":"Instance attribute are attributes that are specific to each instance of a class. eg - class Employee: HoursOfWork = 40 x=Employee() x.name = \"ram\" --> creates an instance attribute for object 'x' x.name --> output = ram y=Employee() y.name --> give error coz no instance attribute associated with 'y' They are created using objectName.attributeName So, if edited or modified, then reflected only for that object instance.","title":"What is Instance attribute?"},{"location":"OOPs/OOPs%20in%20Python/#how-self-parameter-is-handled","text":"The method call objectName.methodName() is interpreted as className.methodName(objectName), and this parameter is referred to as ' self ' in method definition.","title":"How self parameter is handled?"},{"location":"OOPs/OOPs%20in%20Python/#methods","text":"Attributes - an attribute is a property that further defines a class. Method - a method is a function within a class which can access all the attributes of a class and perform a specific task.","title":"Methods"},{"location":"OOPs/OOPs%20in%20Python/#what-is-instance-method","text":"Methods that accept self parameter as default parameter. They are used to modify instance attributes.","title":"What is Instance Method?"},{"location":"OOPs/OOPs%20in%20Python/#what-is-a-static-method","text":"Methods that do not modify instance attributes of a class are called Static Methods. But they can be used to modify Class Attributes.","title":"What is a static method?"},{"location":"OOPs/OOPs%20in%20Python/#what-is-init-method","text":"init() method is the initializer in python. It is called when an object is instantiated. All the attributes of the class should be initialized in this method to make your object a fully initialized object. It has the default parameter as the object are static methods.","title":"What is init() method?"},{"location":"OOPs/OOPs%20in%20Python/#self-variable","text":"When you call any class method, then by default, first argument that is passed is class object name. eg - class Employee: def employeeDetails() pass employeOne = Employee() employeOne.employeeDetails() is same as employeeOne.emplpyeDetails(employeeOne) By using self parameter, you are referring to attributes of that object, and the values persist until lifespan of program or until you manually delete the object. Eg - class Employee: def employeeDetails(self): self.name = \"kush\" print(\"name = \", self.name) age = 30 --> since we didnt use \"self\", the value age is not available to other methods print(\"age = \",age) def printEmployeeDetails(self): print(\"my name = \", self.name) print(\"my age = \",age) --> here it will throw error since \"age\" is not available to other functions. Its scope is restricted to method employeeDetails.","title":"self Variable"},{"location":"OOPs/OOPs%20in%20Python/#static-methods-and-instance-methods","text":"Instance Methods - They are methods of our class that make use of self parameter and modify instance attributes of our class. Static Methods- - They are methods that do not take default self parameter. - To differentiate between static and instance method, we use decorator @staticmethod Decorators are functions that take another function as input and extend their functionality. They are denoted by starting them with '@' symbol. For example - @staticmethod is a decorator that takes the function as message, extends its functionality and ignores the binding of the object. class Employee: @staticmethod def WelcomeMessage(): print(\"welcome to our organization\") init() method - init() method is a special method in python which is used to initialize an object. eg - class Employee: def setName(self): self.name=\"ram\" def printName(self): print(self.name) x=employee() x.printName() --> it gives error coz we havent initialized name. init() method is first method to be called in a class. class Employee: def __init__(self, name): self.name = name def printName(self): print(self.name) x=employee(\"ram\") x.printName() --> output is Ram y=employee(\"shyam\") y.printName() --> output is shyam","title":"Static Methods and Instance Methods"},{"location":"OOPs/OOPs%20in%20Python/#abstraction-and-encapsulation","text":"Abstraction is the process of hiding implementation details from the user. Encapsulation is done to achieve abstraction. You use method of class via abstraction.","title":"Abstraction and Encapsulation"},{"location":"OOPs/OOPs%20in%20Python/#inheritance","text":"allows code re-usability.","title":"Inheritance"},{"location":"OOPs/OOPs%20in%20Python/#type-1-single-level-inheritance","text":"eg 1 - dad = good baseball player. son = good baseball player. eg 2 - class Apple: manufacturer = \"Apple Inc.\" contactWebsite = \"www.apple.com/contact\" def contactDetails(self): print(\"To contact us, log on to \", self.contactWebsite ) class MacBook(Apple): def __init__(self): self.yearOfManufacture = 2017 def manufactureDetails(self): print(\"This macbook was manufactured in year {} by {}\" .format(self.yearOfManufacture, self.manufacturer)\") mymacbook = MacBook() # output = This macbook was manufactured in year 2017 by Apple Inc. mymacbook.manufactureDetails # To contact us, log on to www.apple.com/contact mymacbook.contactDetails","title":"Type 1 : single level inheritance"},{"location":"OOPs/OOPs%20in%20Python/#type2-multiple-inheritances","text":"eg 1 - dad = good baseball player. mom = good cook. son = good baseball player and good cook eg 2 - class OperatingSystem: multitasking = True name = \"Windows OS\" Class Apple: website = \"www.apple.com\" name = \"Apple OS\" Class MacBook(OperatingSystem, Apple): def __init__(self): if self.multitasking is True: # output = this is a multitasking system. Visit www.apple.com for more details. print(\"this is a multi tasking system. Visit {} for more details\".format(self.website)) #output = Window OS --why - coz we inherited OperatingSystem class first, and so variable present in both classes will be picked up from first class. print(\"name = \".self.name) Class NewMacBook(Apple, OperatingSystem): def __init__(self): if self.multitasking is True: # output = this is a multitasking system. Visit www.apple.com for more details. print(\"this is a multi tasking system. Visit {} for more details\".format(seld.website)) # output = Apple OS --why - coz we inherited Apple class first, and so variable present in both classes will be picked up from first class. print(\"name = \".self.name)","title":"Type2: Multiple Inheritances"},{"location":"OOPs/OOPs%20in%20Python/#type-3-multi-level-inheritance","text":"eg 1- grandfather = good baseball player father = good soccer player son = good baseball and soccer player eg 2- class MusicalInstruments: numberOfMajorKeys = 12 class StringInstruments(MusicalInstruments): typeOfWood = \"ToneWood\" class Guitar(StringInstruments): def __init__(self): self.numberOfStrings = 6 print(\"this guitar consists of {} strings. It is made of {} and it can play {} keys\" .format(self.numberOfStrings, self.typeOfWood, self.numberOfMajorKeys)) guitar = Guitar() output - This guitar consists of 6 strings. It is made of ToneWood and can play 12 keys","title":"Type 3: multi level inheritance"},{"location":"OOPs/OOPs%20in%20Python/#access-specifiers-public-private-and-protected","text":"public - your class, derived class, and further sub-classes derived from derived class --all can access values in main class. protected - only your class and derived class members can access data and methods of your class. private- only your class can access methods and attributes of your class. To declare attribute or method as - 1. public - MemberName 2. Protected - _memberName --> single underscore. 3. Private - __MemberName --> double underscore. class Car: numberOfWheels = 4 _color = \"Black\" __YearOfManufacture = 2018 #python saves it as _Car__yearOfManufacture i.e _ClassName__AttributeName Class BMW(Car): def __init__(self): print(\"Protected attrubte color : \", self._color) car=Car() print(\"Public Attribute numberOfWheels: \", self.car.NumberOfWheels) --> output = Public Attribute NumberOfWheels: 4 bmw = BMW() --> output = Protected attribute color : Black print(\"Private Attribute yearOfManufacture: \", car.__yearOfManufacture ) --> output = error print(\"Private Attribute yearOfManufacture: \", car._Car__yearOfManufacture) --> output = Private attribute YearOfManufacture : 2018","title":"Access Specifiers - public, private and protected"},{"location":"OOPs/OOPs%20in%20Python/#polymorphism","text":"It is characteristic of an entity to be able to be present in more than one form. eg - \"+\" operator when applied to 2 numbers returns their sum, but whe applied to 2 strings, returns their concatenation.","title":"PolyMorphism"},{"location":"OOPs/OOPs%20in%20Python/#overriding-and-super-method","text":"Overriding = redefine base class method in your derived class. super() function allows you to transfer control back to base class in a derived class. It is used in case you want to access method in base class. class Employee: def setNumberOfWorkingHours(self): self.NumberOfWorkingHours = 10 def displayNumberOfWorkingHours(self): print(self.NumberOfWorkingHours) employee = Employee() employee.setNumberOfWorkingHours() print(\"Number of working hours of emplyee = \", end = ' ') employee.displayNumberOfWorkingHours() output - Number of working hours of Employee = 10 class Trainee(Employee): def setNumberOfWorkingHours(self): self.NumberOfWorkingHours = 20 def resetNumberOfWorkingHours(self): super().setNumberOfWorkingHours() trainee = Trainee() trainee.setNumberOfWorkingHours() print(\"Number of working hours of Trainee = \", end = ' ') trainee.displayNumberOfWorkingHours() trainee.resetNumberOfWorkingHours() print(\"Number of working hours of Trainee after reset = \", end = ' ') trainee.displayNumberOfWorkingHours() output - Number of working hours of Trainee = 20 Number of working hours of Trainee after reset = 10","title":"Overriding and Super() method"},{"location":"OOPs/OOPs%20in%20Python/#diamond-problem","text":"Here we have 4 classes - A, B, C and D - A is base class - B anc C derived from A - D is derived from B and C","title":"Diamond Problem"},{"location":"OOPs/OOPs%20in%20Python/#case-1-method-will-not-be-overridden-in-class-b-and-class-c","text":"class A: def method(self): print(\"this method belongs to class A\") pass class B(A): pass class C(A): pass class D(B,C): pass d = D() d.method() output --> This method belongs to class A","title":"Case 1: Method() will Not be overridden in Class B and Class C"},{"location":"OOPs/OOPs%20in%20Python/#case-2-method-will-be-overridden-in-class-b-but-not-in-class-c","text":"class A: def method(self): print(\"this method belongs to class A\") pass class B(A): def method(self): print(\"this method belongs to class B\") class C(A): pass class D(B,C): pass d = D() d.method() output --> This method belongs to class B","title":"Case 2: Method() will be overridden in Class B but not in Class C"},{"location":"OOPs/OOPs%20in%20Python/#case-3-method-will-be-overridden-in-class-c-but-not-in-class-b","text":"class A: def method(self): print(\"this method belongs to class A\") class B(A): pass class C(A): def method(self): print(\"this method belongs to class C\") class D(B,C): pass d = D() d.method() output --> This method belongs to class C","title":"Case 3: Method() will be overridden in Class C but not in Class B"},{"location":"OOPs/OOPs%20in%20Python/#case-4-method-will-be-overridden-in-both-class-b-and-class-c","text":"class A: def method(self): print(\"this method belongs to class A\") class B(A): def method(self): print(\"this method belongs to class B\") class C(A): def method(self): print(\"this method belongs to class C\") class D(B,C): pass d = D() output -> This method belongs to class B why this output -> Method Resolution Order -- in case of derived class from multiple classes, method picked from 1st class to which it is obtained; Had we defined class D as below, output would have been different. class D(C,B): pass d = D() d.method() --> output = This method belongs to class C","title":"Case 4: Method() will be overridden in both Class B and Class C"},{"location":"OOPs/OOPs%20in%20Python/#operator-overloading","text":"There are internal functions of python. They are overridden in following way - eg - overloading addition operator, the \"+\" operator when - applied to 2 numbers returns their sum. - applied to 2 strings, returns their concatenation. class Square: def __init__(self, side): self.side = side sq1 = Square(5) sq2 = Square(10) # output -> ERROR # Why -> sq1 + sq2 = error coz '+' operator doesn't work on object of class square print(\"the sum of sides of both the squares = \", sq1 + sq2) So, we need a mechanism to add sides of square and return value. That should allow us to add above 2 square objects as well. Solution -> overload special method add() in your class class Square: def __init__(self, side): self.side = side def __add__(square1, square2): return( (4 * square1.side) + (4 * square2.side) ) sq1 = Square(5) sq2 = Square(10) # output -> 60 print(\"the sum of sides of both the squares = \", sq1 + sq2)","title":"Operator Overloading"},{"location":"OOPs/OOPs%20in%20Python/#abstract-base-class","text":"An Abstract Base Class is a base class which doesn't have a definition of its own. It has abstract methods which forces its implementation in derived classes To declare a class as abstract base class , it needs to be declared as instance of ABCMeta class of python . eg - # we import ABCMeta and abstractmethod from abc module of python from abc import ABCMeta, abstractmethod class shape(metaclass = ABCMeta): #decorator abstractmethod tells that given method is required to be defined in derived classes. @abstractmethod def area(self): return 0 #here we say that method area() does not have a defn of its own; but it needs to be defined in derived classes. class Square(shape): side = 4 def area(self): print(\"area of square\", self.side * self.side) class Rectangle(Shape): width = 5 breadth = 10 def area(self): print(\"area of rectangle \" , self.width * self.length) sq = Square() rc = Rectangle() sq.area() rc.area() Note - if no definition of method area() in class Square and even if we don't call sq.area() --> we get error An abstract can only be inherited in derived classes, but cannot be instantiated. If we try to create object of abstract class, we get error. sh = Shape() Summary - polymorphism = ability of an entity to be able to exist in more than one form. - overriding = form of polymorphism in which we redefine method of base class in a derived class. - By doing this we change behaviour of a base class method. - How - class BaseClass: def BaseClassMethod(): #define behaviour class DerivedClass(BaseClass): def baseClassMethod(): #redefine behaviour Operator Overloading - defining a special method for an operator within your class to handle the operation between the objects of that class is called Operator Overloading . Abstract Base Class - a base class which consists of abstract methods that should be implemented in its derived class is called abstract base class. Syntax - from abc import ABCMeta, abstractmethod class baseClass(metaclass = ABCMeta): @abstractmethod def abstractMethod(self): return","title":"Abstract Base Class"},{"location":"SOLID%20Principles/Introduction/","text":"What are SOLID Principles? Why SOLID? These are simple but cardinal design principles, that enables us to create elegant and robust software. Cost of bad software increases over time.","title":"Introduction to SOLID Principles"},{"location":"SOLID%20Principles/Introduction/#what-are-solid-principles","text":"","title":"What are SOLID Principles?"},{"location":"SOLID%20Principles/Introduction/#why-solid","text":"These are simple but cardinal design principles, that enables us to create elegant and robust software. Cost of bad software increases over time.","title":"Why SOLID?"},{"location":"SOLID%20Principles/Open%20Closed%20Principle/","text":"Introduction 'O' stands for Open Closed Principle . This is often abbreviated as OCP. As per OCP, \"Software components should be closed for modification, but open for extension\" So this presents kind of a conundrum: How can something be open and closed at the same time? To explain this, we will take the help of a real world analogy. Most of you should be familiar with this device - this is the Nintendo Wii, a very popular gaming console, though its discontinued now. So when you buy the Wii console, what you get is the console itself which is the CPU tower kind of thing, and a basic remote controller. You can see both in the picture now. Wii also manufactures a number of accessories that can go with the Wii. For instance, there's Wii Zapper, which is a good accessory for playing FPS or first person shooter games in Wii.Setting this up is pretty simple. You just place the basic remote controller into a cradle inside the zapper and you are all set. So the Zapper has now added a really useful feature to the Wii. Another accessory is the Wii steering wheel, which is a really good accessory to have, if you are into racing games. The steering wheel is also setup in the same way. You just put the remote controller into the cradle and you are all set. Now think about this: In order to add the Zapper feature and the steering wheel feature, did we do any visible change to the CPU or the basic controller? No. Nothing.It was just plug n play. Think about a situation where Wii wanted you to move a jumper on the console's motherboard, if you wanted to add n accessory. That would have been so not cool, right? So when the Wii console came out of the factory, it came out as 'Closed for modification'. The makers of Wii did not want their customers to go around opening their consoles on their own. But still they made it possible for customers to add accessories and thereby add extensions or new features to the Wii. This did NOT happen by accident. This happened because the engineers at Wii wanted this product to behave this way, and did a really good job designing it. So, to sum up, the Wii was designed in such a way, that it is closed for modification, but open for extension . That was a simple analogy that I thought of to explain the concept of open closed principle. Let's get back to software design now. So when we say software components should be closed for modification and open for extension, this is what we mean: Closed for modification means: New features getting added to the software component,should NOT have to modify existing code. Open for extension means: A software component should be extendable to add a new feature or to add a new behaviour to it. So, even though the term open-closed might sound like a conundrum, this is what it really means. Real world analogies and code snippets Lets look at a code example in this session. 'One State' is an insurance company that primarily deals with health insurance. The insurance calculations are coded as a Java library. This is one code snippet taken from the library. This piece of code shows how the premium discounts are calculated. We have an InsurancePremiumDiscountCalculator class that has a calculatePremiumDiscountPercent. This method takes in as argument a HealthInsuranceCustomerProfile object. Lets see what this class is: So HealthInsuranceCustomerProfile has a isLoyalCustomer() method which returns a true if the current customer is a loyal customer. If not, it simply returns false. So this HealthInsuranceCustomerProfile object is the input for the calculatePremiumDiscountPercent method. The calculate method invokes the isLoyalCustomer() method on the incoming HealthInsuranceCustomerProfile object and does further discount calculations based on whether the customer is loyal or not. So far, so good. Come tomorrow, One State company acquires another insurance company which is primarily into Vehicle Insurance. They change their tagline accordingly. For all your health AND vehicle insurance needs. So we now have to support vehicle insurance discounts as well. One State has decided that the discount calculation is going to be the same, that is , it is going to be based on loyalty always, regardless of whether its health, vehicle, or any other type of insurance. Okay, so to handle this, we add a new class into our design. VehicleInsuranceCustomerProfile. This is just like theHealthInsuranceCustomerProfile class.It has a isLoyal() method which returns a boolean. So, are we done? No, this is where the problems begin. We now have to modify the Calculator class, because the calculate method currently takes in a HealthInsuranceCustomerProfile object. We want it to take in a VehicleInsuranceCustomerProfile object as well. The only way out is to add a new overloaded method which takes in a VehicleInsuranceCustomerProfile object. This is just the beginning. What if we want to handle home insurance too? We will need to add code again to this Calculator class. So, why is this not good? Because in order to add a new feature, we are having to touch existing code, which goes against our Open Closed Principle. The existing code is supposed to be closed for modification. Let's refactor our design and see if we can solve this problem. We will revert our Calculator class back. We will create a new interface named CustomerProfile. The interface defines only one method: isLoyalCustomer We will make both our CustomerProfile classes implement this common interface. The beauty of this design lies in how it handles future extensions.Assume, One State enters home insurance business as well. So we will need to create a HomeInsuranceCustomerProfile object. We make it implement the common CustomerProfile interface. We do NOT need to touch the Calculator class at all. All we did is : add a new class by implementing an existing interface. Neither the calculator class, nor the interface nor any of the existing classes had to be modified. See how this makes the process of adding extensions much cleaner. What we saw now was an example of the Open Closed Principle. When we started out with this example, it was not following the open closed principle. So we picked holes in the design. The refactoring that we did, made it conform to the open closed principle. After we did it, we saw how the design became much more robust, to handle future extensions in a more elegant way.So that was an example for the Open Closed Principle. Key takeaways What are our Key takeaways? S,o what is the principal benefit that we got from the new design in the previous example? Ease of adding new features. This can translate to cost savings. Let's see how. Assume we do not follow the Open Closed Principle. Then for all additional features, we will end up having to modify the existing code. The more the number of changes we introduce to the existing code, the more time we need to spend on testing and quality assurance, to make sure we did not introduce any bug into the existing code. If we follow the open closed principle, it's not that we do not have test at all. We still need to test. But testing a new piece of code is much easier than having to run a whole regression test suite on the existing code. So to sum up , ease of adding new features , leads to minimum cost of developing and testing that is involved. Though not quite obvious is that we unknowingly did quite a bit of decoupling when we revamped our design. So after we made our design conform to the Open Closed Principle, we ended up with components that were more loosely coupled with one another. So we unknowingly followed the Single Responsibility Principle as well! So a key point here is that the SOLID principles are all intertwined and interdependent. They are most effective when they are combined together. So it is important that you get a wholesome view of all the SOLID principles. A word of caution Do not follow the open Closed Principle blindly. You will end up with a huge number of classes that can complicate your overall design. For e.g. if you were to fix a bug, and if you think existing code has to be modified in order to fix the bug effectively, then go do it. Do not think about revamping the design just to fix the bug. But if you see repeated occurrences of certain kinds of bugs, and you think that revamping your design can help reduce it, then by all means, do it. So it's a subjective, rather than an objective decision, to decide when and where to apply the Open Closed Principle to your design. Code examples Its the same example we did in previous session. Wrap up So the open closed principle says: \"Software components should be closed for modification, but open for extension\" Closed for modification means that New features getting added to the software component, should NOT have to modify existing code. And at the same time, Open for extension means: A software component should be extendable to add a new feature or to add a new behaviour to it. We saw an example where we picked holes in our design and revamped it to make it follow the open closed principle. We saw that the benefits of following this principle can lead to considerable cost savings in the long run. We also saw how the OPen closed principle and single responsibility principle can work together to achieve a better design. Finally, we also cautioned against creating a huge number of classes by applying open closed principle without applying any thought. This can be dangerous.","title":"Open Closed Principle"},{"location":"SOLID%20Principles/Open%20Closed%20Principle/#introduction","text":"'O' stands for Open Closed Principle . This is often abbreviated as OCP. As per OCP, \"Software components should be closed for modification, but open for extension\" So this presents kind of a conundrum: How can something be open and closed at the same time? To explain this, we will take the help of a real world analogy. Most of you should be familiar with this device - this is the Nintendo Wii, a very popular gaming console, though its discontinued now. So when you buy the Wii console, what you get is the console itself which is the CPU tower kind of thing, and a basic remote controller. You can see both in the picture now. Wii also manufactures a number of accessories that can go with the Wii. For instance, there's Wii Zapper, which is a good accessory for playing FPS or first person shooter games in Wii.Setting this up is pretty simple. You just place the basic remote controller into a cradle inside the zapper and you are all set. So the Zapper has now added a really useful feature to the Wii. Another accessory is the Wii steering wheel, which is a really good accessory to have, if you are into racing games. The steering wheel is also setup in the same way. You just put the remote controller into the cradle and you are all set. Now think about this: In order to add the Zapper feature and the steering wheel feature, did we do any visible change to the CPU or the basic controller? No. Nothing.It was just plug n play. Think about a situation where Wii wanted you to move a jumper on the console's motherboard, if you wanted to add n accessory. That would have been so not cool, right? So when the Wii console came out of the factory, it came out as 'Closed for modification'. The makers of Wii did not want their customers to go around opening their consoles on their own. But still they made it possible for customers to add accessories and thereby add extensions or new features to the Wii. This did NOT happen by accident. This happened because the engineers at Wii wanted this product to behave this way, and did a really good job designing it. So, to sum up, the Wii was designed in such a way, that it is closed for modification, but open for extension . That was a simple analogy that I thought of to explain the concept of open closed principle. Let's get back to software design now. So when we say software components should be closed for modification and open for extension, this is what we mean: Closed for modification means: New features getting added to the software component,should NOT have to modify existing code. Open for extension means: A software component should be extendable to add a new feature or to add a new behaviour to it. So, even though the term open-closed might sound like a conundrum, this is what it really means.","title":"Introduction"},{"location":"SOLID%20Principles/Open%20Closed%20Principle/#real-world-analogies-and-code-snippets","text":"Lets look at a code example in this session. 'One State' is an insurance company that primarily deals with health insurance. The insurance calculations are coded as a Java library. This is one code snippet taken from the library. This piece of code shows how the premium discounts are calculated. We have an InsurancePremiumDiscountCalculator class that has a calculatePremiumDiscountPercent. This method takes in as argument a HealthInsuranceCustomerProfile object. Lets see what this class is: So HealthInsuranceCustomerProfile has a isLoyalCustomer() method which returns a true if the current customer is a loyal customer. If not, it simply returns false. So this HealthInsuranceCustomerProfile object is the input for the calculatePremiumDiscountPercent method. The calculate method invokes the isLoyalCustomer() method on the incoming HealthInsuranceCustomerProfile object and does further discount calculations based on whether the customer is loyal or not. So far, so good. Come tomorrow, One State company acquires another insurance company which is primarily into Vehicle Insurance. They change their tagline accordingly. For all your health AND vehicle insurance needs. So we now have to support vehicle insurance discounts as well. One State has decided that the discount calculation is going to be the same, that is , it is going to be based on loyalty always, regardless of whether its health, vehicle, or any other type of insurance. Okay, so to handle this, we add a new class into our design. VehicleInsuranceCustomerProfile. This is just like theHealthInsuranceCustomerProfile class.It has a isLoyal() method which returns a boolean. So, are we done? No, this is where the problems begin. We now have to modify the Calculator class, because the calculate method currently takes in a HealthInsuranceCustomerProfile object. We want it to take in a VehicleInsuranceCustomerProfile object as well. The only way out is to add a new overloaded method which takes in a VehicleInsuranceCustomerProfile object. This is just the beginning. What if we want to handle home insurance too? We will need to add code again to this Calculator class. So, why is this not good? Because in order to add a new feature, we are having to touch existing code, which goes against our Open Closed Principle. The existing code is supposed to be closed for modification. Let's refactor our design and see if we can solve this problem. We will revert our Calculator class back. We will create a new interface named CustomerProfile. The interface defines only one method: isLoyalCustomer We will make both our CustomerProfile classes implement this common interface. The beauty of this design lies in how it handles future extensions.Assume, One State enters home insurance business as well. So we will need to create a HomeInsuranceCustomerProfile object. We make it implement the common CustomerProfile interface. We do NOT need to touch the Calculator class at all. All we did is : add a new class by implementing an existing interface. Neither the calculator class, nor the interface nor any of the existing classes had to be modified. See how this makes the process of adding extensions much cleaner. What we saw now was an example of the Open Closed Principle. When we started out with this example, it was not following the open closed principle. So we picked holes in the design. The refactoring that we did, made it conform to the open closed principle. After we did it, we saw how the design became much more robust, to handle future extensions in a more elegant way.So that was an example for the Open Closed Principle.","title":"Real world analogies and code snippets"},{"location":"SOLID%20Principles/Open%20Closed%20Principle/#key-takeaways","text":"","title":"Key takeaways"},{"location":"SOLID%20Principles/Open%20Closed%20Principle/#what-are-our-key-takeaways","text":"S,o what is the principal benefit that we got from the new design in the previous example? Ease of adding new features. This can translate to cost savings. Let's see how. Assume we do not follow the Open Closed Principle. Then for all additional features, we will end up having to modify the existing code. The more the number of changes we introduce to the existing code, the more time we need to spend on testing and quality assurance, to make sure we did not introduce any bug into the existing code. If we follow the open closed principle, it's not that we do not have test at all. We still need to test. But testing a new piece of code is much easier than having to run a whole regression test suite on the existing code. So to sum up , ease of adding new features , leads to minimum cost of developing and testing that is involved. Though not quite obvious is that we unknowingly did quite a bit of decoupling when we revamped our design. So after we made our design conform to the Open Closed Principle, we ended up with components that were more loosely coupled with one another. So we unknowingly followed the Single Responsibility Principle as well! So a key point here is that the SOLID principles are all intertwined and interdependent. They are most effective when they are combined together. So it is important that you get a wholesome view of all the SOLID principles.","title":"What are our Key takeaways?"},{"location":"SOLID%20Principles/Open%20Closed%20Principle/#a-word-of-caution","text":"Do not follow the open Closed Principle blindly. You will end up with a huge number of classes that can complicate your overall design. For e.g. if you were to fix a bug, and if you think existing code has to be modified in order to fix the bug effectively, then go do it. Do not think about revamping the design just to fix the bug. But if you see repeated occurrences of certain kinds of bugs, and you think that revamping your design can help reduce it, then by all means, do it. So it's a subjective, rather than an objective decision, to decide when and where to apply the Open Closed Principle to your design.","title":"A word of caution"},{"location":"SOLID%20Principles/Open%20Closed%20Principle/#code-examples","text":"Its the same example we did in previous session.","title":"Code examples"},{"location":"SOLID%20Principles/Open%20Closed%20Principle/#wrap-up","text":"So the open closed principle says: \"Software components should be closed for modification, but open for extension\" Closed for modification means that New features getting added to the software component, should NOT have to modify existing code. And at the same time, Open for extension means: A software component should be extendable to add a new feature or to add a new behaviour to it. We saw an example where we picked holes in our design and revamped it to make it follow the open closed principle. We saw that the benefits of following this principle can lead to considerable cost savings in the long run. We also saw how the OPen closed principle and single responsibility principle can work together to achieve a better design. Finally, we also cautioned against creating a huge number of classes by applying open closed principle without applying any thought. This can be dangerous.","title":"Wrap up"},{"location":"SOLID%20Principles/Single%20Responsibility%20Principle/","text":"Introduction S stands for Single Responsibility Principle, often abbreviated and referred to as SRP. So what is this principle? The Single Responsibility Principle says that 'Every software component should have one and only one responsibility' When we say software component, if we are talking in the context of an object oriented programming language like Java, the first thing that comes to our mind is a Java class. But it is to be noted that the term software component could also refer to a method or a function or even a module. So the rule states that a software component should have only one responsibility. Let me bring up a picture to explain this. This is Swiss Army Knife. As you know,a swiss army knife is a combination of a number of useful tools, each one with a distinct purpose. You can have anything from a can opener to a pair of mini scissors or even a screw-driver inside a Swiss Army knife. Although a Swiss Army knife is a versatile tool and much sought after and appreciated, when it comes to software, the rules change. If you think of the Swiss Army Knife as a software component, it violates the Single Responsibility Principle, for the reason that it has multiple responsibilities. So then what does SRP recommend? If I can put a visual to it, this is it. So this is a knife that has a single responsibility. It can only be used to cut. You cannot use it as a can opener, you cannot use it as a screw-driver, you can only use it to cut. This is a very high level introduction of the Single Responsibility Principle. Cohesion Take a look at this class. public class Square { private boolean highResolutionMonitor = true; private int side = 5; public int calculateArea() { return side * side; // side\u00b2 - side ^ 2; } public int calculatePerimeter() { return side * 4; } public void draw() { if (highResolutionMonitor) { // Render a high resolution image of a square } else { // Render a high normal image of a square } } public void rotate() { // Rotate the image of the square clockwise to // the required degree and re-render } } The name of the class is Square. It has 4 methods: - calculateArea(), - calculatePerimeter() - draw() - rotate() The calculateArea and calculatePerimeter functions do exactly what they are supposed to do, they calculate the area and perimeter of a square given the length of its side. The draw() function renders the image of the square on the display. It has multiple code flow, depending on what type of display is being used. The rotate() function rotates the image of the square and re-renders it on the display. In the context of this code snippet, we will learn about a new concept termed Cohesion . Cohesion , in the software world is defined as the degree to which the various parts of a software component are related. Here, you see garbage that looks unsegregated. You cannot really come up with a relation between all the contents of this garbage can. It has a wide variety of items like plastic cans, beer bottles, paper waste etc. But let's look at how it looks, after it gets segregated. Very nice. Take a look at the yellow bin for plastics. There are a number of plastic bottles inside the bin. The bottles are NOT alike. But if you look at all the contents of the yellow bin, they have a common relation. They are all made up of plastic. If we apply the definition of cohesion here, which says that cohesion is the degree of relation, we could say that the contents of the unsegregated waste bin have a low cohesion, and the contents of each of the segregated waste bins have a high cohesion. Let's get back to our code snippet and apply the same principle here. What do you make of the methods inside the Square class? The methods calculateArea and calculatePerimeter are closely related, in that they deal with the measurements of a square. So there is a high level of cohesion between these two methods. The draw() method and the rotate() method deal with rendering the image of the square in a certain way on the display. So there is a high level of cohesion between these two methods as well. But if you take all of the methods as a whole, the level of cohesion is low. For instance , the calculatePerimeter() method is not closely related to the draw() method as they deal with entirely different responsibilities. So we are going to do some shuffling so as to increase the level of cohesion. We take these draw and rotate methods and move them to a different class - SquareUI. By doing this, even though I have split the methods into two classes, I have increased the level of cohesion in each of the classes. All the two methods inside the Square class are now closely related, as both of them deal with the measurements of the square. All the two methods inside the SquareUI class are now closely related, as both of them deal with the graphic rendering of the square. So one aspect of the Single Responsibility Principle is that, we should always aim for high cohesion within a component. Component means class in this case. If there is high cohesion between all methods of a class, we can assign a single responsibility to all the methods as a whole. For e.g. for the first class Square, we can safely say that the responsibility of the Square class as a whole is to deal with the measurements related to a square. Similarly , for the second class SquareUI, we can safely say that the responsibility of the SquareUI class as a whole is to deal with rendering the image of a square. So we just saw how aiming for higher cohesion can help us move towards conforming to the single responsibility principle. Coupling Like cohesion, there is another concept that we need to look at - Coupling. Coupling is defined as the level of inter-dependency between various software components. Take a look at these two trains. What do you notice about the width of the tracks? One is Standard Gauge Rail, and other is Broad Gauge Rail. - Standard Gauge is 1.4 m wide. - Broad Gauge is 1.6m wide. The difference in width makes it two entirely different railway systems. For e.g. the width of the train for standard gauge is different from that of broad gauge. One train cannot move on a track of a different gauge. In other words, a train is tightly coupled to its track. Tight coupling may be a necessity in railways, but in software, tight coupling is an undesirable feature. Why? See below example - So Coupling is defined as the level of inter-dependency between various software components. Let's see a code snippet now. Here's a class Student. One of the methods inside the Student class is the save() method. The 'save' method will convert the student class into a serialized form and persist it into a Database. You can see that this method deals with a lot of low level details related to handling record insertion into a database. Let's assume the database you are using now is MySQL. Sometime in the future, if you decide to go with a NoSQL database like,say, MongoDB, most of this code will need to change. So you can see that the Student class is tightly coupled with the database layer we use at the back end, just like the train is tightly coupled to the track. The Student class should ideally deal with only basic student related functionalities like getting student id, date of birth, address etc. The Student class should NOT be made cognizant of the low level details related to dealing with the back end database. So tight coupling is bad in software. So, How do we fix this? We'll take the database related code, and we'll move it into a new Repository class. Then we'll refer to this Repository method from inside the Student class. By doing so, we have removed the tight coupling and made it loose. So now if we change the underlying database, the Student class does NOT need to get changed and recompiled. You only need to change the Repository class. If you look at this in terms of responsibilities, the Student class has the responsibility of dealing with core student related data. And the Repository class has a single responsibility of dealing with database operations. So by removing tight coupling, and making the coupling loose, we are again abiding by the Single Responsibility principle. So to sum up - - we looked at two concepts - Cohesion and Coupling. - We saw how low cohesion is bad. - Single Responsibility Principle always advocates higher cohesion. - We also saw how tight coupling is bad. Single Responsibility Principle always recommends loose coupling. - So always aim for Higher Cohesion and Loose Coupling. Reason for change : Single Responsibility Principle revisited The principle ,that we studied so far, states that 'Every software component should have one and only one responsibility' In place of the term responsibility, we are going to put a new phrase 'reason to change'. So now it reads 'Every software component should have one and only one reason to change'. So what's this new phrase 'reason to change'? In the words of the Greek Philosopher - Heraclitus. \"The only thing that is constant is change\". In other words, change is inevitable. This quote applies to the software world as well. Software is never dormant. It always keeps changing. Let us explain this with an example. We'll start with the same Student class that we used for our previous session, the original version of it. Assume that this Student class is part of a software module which is already in production. There could be multiple reasons for a software component to change in the future - - A change in the student id format, as suggested by school management. - A change in the student name format, as necessitated by a state law. - A change in the database back end, as advised by the technical team. Another one. So, we have 3 reasons to change. What does the Single Responsibility Principle say? As per the new definition, 'Every software component should have one and only one ~~responsibility~~ reason to change' Why? What's the problem if our class has more reasons to change? If a software component has multiple reasons to change, then the frequency of changes to it will increase. Every change to a software component opens up the possibility of introducing bugs into the software. So if there are frequent changes to a software component, the probability of introducing a bug goes up. This would require more time and effort to be spent on re-testing the software after the changes are made, because we need to make sure we catch all the bugs before we release the modified version of the software. More time and effort, means more money. So the difference between following and not following the Single Responsibility Principle could be a considerable financial impact! And this applies, not just to the Single Responsibility Principle, but to all other SOLID principles as well. So, back to our code snippet... How do we fix this? What can we do about the multiple reasons to change? We'll repeat the same move we did in our last session. We'll take the database operations out and move it to a separate Repository class. Because we split the classes, lets split the 'reasons to change' as well. So the Student class is left with 2 reasons to change. And the Repository class has one reason to change. 2 is still a problem, isn't it. We are supposed to have only one reason to change, right? Technically, yes. But if the reasons are closely related to one another, you can go ahead and combine them. So if we examine the 2 reasons to change for the Student class closely, one is related to student id, and another is related to student name. We could combine both these and say 'changes to student profile'. So now both our classes follow the new definition of the Single Responsibility Principle which says 'Every software component should have one and only one reason to change' So this new definition which replaces Responsibility with Reason to Change is just a new perspective. At the end of the day, the action which we took to fix is the same, in the previous session and the current session. So, these two definitions are pretty much the same. Code exercise Given below code, we find 3 reasons to change -> We take out 2 methods -> employeeRepository and TaxCalculation. Now our final code looks like this -> So we have applied the single responsibility principle here. But a word of caution is that, do not keep creating a huge number of classes just like that. For instance, it is a bad idea to create separate classes to handle employee id, separate class to handle employee name etc. If you can group the responsibilities together in a sensible way,then do it. Else, you will end up having a huge number of classes thereby adding unnecessary complexity to your code. So follow the principle, but use your discretion and make an informed judgment.","title":"Single Responsibility Principle"},{"location":"SOLID%20Principles/Single%20Responsibility%20Principle/#introduction","text":"S stands for Single Responsibility Principle, often abbreviated and referred to as SRP. So what is this principle? The Single Responsibility Principle says that 'Every software component should have one and only one responsibility' When we say software component, if we are talking in the context of an object oriented programming language like Java, the first thing that comes to our mind is a Java class. But it is to be noted that the term software component could also refer to a method or a function or even a module. So the rule states that a software component should have only one responsibility. Let me bring up a picture to explain this. This is Swiss Army Knife. As you know,a swiss army knife is a combination of a number of useful tools, each one with a distinct purpose. You can have anything from a can opener to a pair of mini scissors or even a screw-driver inside a Swiss Army knife. Although a Swiss Army knife is a versatile tool and much sought after and appreciated, when it comes to software, the rules change. If you think of the Swiss Army Knife as a software component, it violates the Single Responsibility Principle, for the reason that it has multiple responsibilities. So then what does SRP recommend? If I can put a visual to it, this is it. So this is a knife that has a single responsibility. It can only be used to cut. You cannot use it as a can opener, you cannot use it as a screw-driver, you can only use it to cut. This is a very high level introduction of the Single Responsibility Principle.","title":"Introduction"},{"location":"SOLID%20Principles/Single%20Responsibility%20Principle/#cohesion","text":"Take a look at this class. public class Square { private boolean highResolutionMonitor = true; private int side = 5; public int calculateArea() { return side * side; // side\u00b2 - side ^ 2; } public int calculatePerimeter() { return side * 4; } public void draw() { if (highResolutionMonitor) { // Render a high resolution image of a square } else { // Render a high normal image of a square } } public void rotate() { // Rotate the image of the square clockwise to // the required degree and re-render } } The name of the class is Square. It has 4 methods: - calculateArea(), - calculatePerimeter() - draw() - rotate() The calculateArea and calculatePerimeter functions do exactly what they are supposed to do, they calculate the area and perimeter of a square given the length of its side. The draw() function renders the image of the square on the display. It has multiple code flow, depending on what type of display is being used. The rotate() function rotates the image of the square and re-renders it on the display. In the context of this code snippet, we will learn about a new concept termed Cohesion . Cohesion , in the software world is defined as the degree to which the various parts of a software component are related. Here, you see garbage that looks unsegregated. You cannot really come up with a relation between all the contents of this garbage can. It has a wide variety of items like plastic cans, beer bottles, paper waste etc. But let's look at how it looks, after it gets segregated. Very nice. Take a look at the yellow bin for plastics. There are a number of plastic bottles inside the bin. The bottles are NOT alike. But if you look at all the contents of the yellow bin, they have a common relation. They are all made up of plastic. If we apply the definition of cohesion here, which says that cohesion is the degree of relation, we could say that the contents of the unsegregated waste bin have a low cohesion, and the contents of each of the segregated waste bins have a high cohesion. Let's get back to our code snippet and apply the same principle here. What do you make of the methods inside the Square class? The methods calculateArea and calculatePerimeter are closely related, in that they deal with the measurements of a square. So there is a high level of cohesion between these two methods. The draw() method and the rotate() method deal with rendering the image of the square in a certain way on the display. So there is a high level of cohesion between these two methods as well. But if you take all of the methods as a whole, the level of cohesion is low. For instance , the calculatePerimeter() method is not closely related to the draw() method as they deal with entirely different responsibilities. So we are going to do some shuffling so as to increase the level of cohesion. We take these draw and rotate methods and move them to a different class - SquareUI. By doing this, even though I have split the methods into two classes, I have increased the level of cohesion in each of the classes. All the two methods inside the Square class are now closely related, as both of them deal with the measurements of the square. All the two methods inside the SquareUI class are now closely related, as both of them deal with the graphic rendering of the square. So one aspect of the Single Responsibility Principle is that, we should always aim for high cohesion within a component. Component means class in this case. If there is high cohesion between all methods of a class, we can assign a single responsibility to all the methods as a whole. For e.g. for the first class Square, we can safely say that the responsibility of the Square class as a whole is to deal with the measurements related to a square. Similarly , for the second class SquareUI, we can safely say that the responsibility of the SquareUI class as a whole is to deal with rendering the image of a square. So we just saw how aiming for higher cohesion can help us move towards conforming to the single responsibility principle.","title":"Cohesion"},{"location":"SOLID%20Principles/Single%20Responsibility%20Principle/#coupling","text":"Like cohesion, there is another concept that we need to look at - Coupling. Coupling is defined as the level of inter-dependency between various software components. Take a look at these two trains. What do you notice about the width of the tracks? One is Standard Gauge Rail, and other is Broad Gauge Rail. - Standard Gauge is 1.4 m wide. - Broad Gauge is 1.6m wide. The difference in width makes it two entirely different railway systems. For e.g. the width of the train for standard gauge is different from that of broad gauge. One train cannot move on a track of a different gauge. In other words, a train is tightly coupled to its track. Tight coupling may be a necessity in railways, but in software, tight coupling is an undesirable feature. Why? See below example - So Coupling is defined as the level of inter-dependency between various software components. Let's see a code snippet now. Here's a class Student. One of the methods inside the Student class is the save() method. The 'save' method will convert the student class into a serialized form and persist it into a Database. You can see that this method deals with a lot of low level details related to handling record insertion into a database. Let's assume the database you are using now is MySQL. Sometime in the future, if you decide to go with a NoSQL database like,say, MongoDB, most of this code will need to change. So you can see that the Student class is tightly coupled with the database layer we use at the back end, just like the train is tightly coupled to the track. The Student class should ideally deal with only basic student related functionalities like getting student id, date of birth, address etc. The Student class should NOT be made cognizant of the low level details related to dealing with the back end database. So tight coupling is bad in software. So, How do we fix this? We'll take the database related code, and we'll move it into a new Repository class. Then we'll refer to this Repository method from inside the Student class. By doing so, we have removed the tight coupling and made it loose. So now if we change the underlying database, the Student class does NOT need to get changed and recompiled. You only need to change the Repository class. If you look at this in terms of responsibilities, the Student class has the responsibility of dealing with core student related data. And the Repository class has a single responsibility of dealing with database operations. So by removing tight coupling, and making the coupling loose, we are again abiding by the Single Responsibility principle. So to sum up - - we looked at two concepts - Cohesion and Coupling. - We saw how low cohesion is bad. - Single Responsibility Principle always advocates higher cohesion. - We also saw how tight coupling is bad. Single Responsibility Principle always recommends loose coupling. - So always aim for Higher Cohesion and Loose Coupling.","title":"Coupling"},{"location":"SOLID%20Principles/Single%20Responsibility%20Principle/#reason-for-change-single-responsibility-principle-revisited","text":"The principle ,that we studied so far, states that 'Every software component should have one and only one responsibility' In place of the term responsibility, we are going to put a new phrase 'reason to change'. So now it reads 'Every software component should have one and only one reason to change'. So what's this new phrase 'reason to change'? In the words of the Greek Philosopher - Heraclitus. \"The only thing that is constant is change\". In other words, change is inevitable. This quote applies to the software world as well. Software is never dormant. It always keeps changing. Let us explain this with an example. We'll start with the same Student class that we used for our previous session, the original version of it. Assume that this Student class is part of a software module which is already in production. There could be multiple reasons for a software component to change in the future - - A change in the student id format, as suggested by school management. - A change in the student name format, as necessitated by a state law. - A change in the database back end, as advised by the technical team. Another one. So, we have 3 reasons to change. What does the Single Responsibility Principle say? As per the new definition, 'Every software component should have one and only one ~~responsibility~~ reason to change' Why? What's the problem if our class has more reasons to change? If a software component has multiple reasons to change, then the frequency of changes to it will increase. Every change to a software component opens up the possibility of introducing bugs into the software. So if there are frequent changes to a software component, the probability of introducing a bug goes up. This would require more time and effort to be spent on re-testing the software after the changes are made, because we need to make sure we catch all the bugs before we release the modified version of the software. More time and effort, means more money. So the difference between following and not following the Single Responsibility Principle could be a considerable financial impact! And this applies, not just to the Single Responsibility Principle, but to all other SOLID principles as well. So, back to our code snippet... How do we fix this? What can we do about the multiple reasons to change? We'll repeat the same move we did in our last session. We'll take the database operations out and move it to a separate Repository class. Because we split the classes, lets split the 'reasons to change' as well. So the Student class is left with 2 reasons to change. And the Repository class has one reason to change. 2 is still a problem, isn't it. We are supposed to have only one reason to change, right? Technically, yes. But if the reasons are closely related to one another, you can go ahead and combine them. So if we examine the 2 reasons to change for the Student class closely, one is related to student id, and another is related to student name. We could combine both these and say 'changes to student profile'. So now both our classes follow the new definition of the Single Responsibility Principle which says 'Every software component should have one and only one reason to change' So this new definition which replaces Responsibility with Reason to Change is just a new perspective. At the end of the day, the action which we took to fix is the same, in the previous session and the current session. So, these two definitions are pretty much the same.","title":"Reason for change : Single Responsibility Principle revisited"},{"location":"SOLID%20Principles/Single%20Responsibility%20Principle/#code-exercise","text":"Given below code, we find 3 reasons to change -> We take out 2 methods -> employeeRepository and TaxCalculation. Now our final code looks like this -> So we have applied the single responsibility principle here. But a word of caution is that, do not keep creating a huge number of classes just like that. For instance, it is a bad idea to create separate classes to handle employee id, separate class to handle employee name etc. If you can group the responsibilities together in a sensible way,then do it. Else, you will end up having a huge number of classes thereby adding unnecessary complexity to your code. So follow the principle, but use your discretion and make an informed judgment.","title":"Code exercise"},{"location":"SQL/docs/Common%20Table%20Expressions/","text":"Common Table Expressions (CTEs) What are Common Table Expressions (CTEs)? A Common Table Expression (CTE) is the result set of a query which exists temporarily and for use only within the context of a larger query. Much like a derived table, the result of a CTE is not stored and exists only for the duration of the query. Common Table expression (CTE) is a temporary named result set that you can reference within a SELECT, INSERT, UPDATE, or DELETE statement. You can also use a CTE in a CREATE a view, as part of the view\u2019s SELECT query. How are CTEs helpful? CTEs, like database views and derived tables, enable users to more easily write and maintain complex queries via increased readability and simplification. This reduction in complexity is achieved by deconstructing ordinarily complex queries into simple blocks to be used, and reused if necessary, in rewriting the query. Example use cases include: - Needing to reference a derived table multiple times in a single query - An alternative to creating a view in the database - Performing the same calculation multiple times over across multiple query components How to create CTE? We can define CTEs by adding a WITH clause directly before SELECT, INSERT, UPDATE, DELETE, or MERGE statement. The WITH clause can include one or more CTEs separated by commas. The following syntax can be followed: [WITH [, ...]] ::= cte_name [(column_name [, ...])] AS (cte_query) After you define your WITH clause with the CTEs, you can then reference the CTEs as you would refer any other table. However, you can refer a CTE only within the execution scope of the statement that immediately follows the WITH clause. After you\u2019ve run your statement, the CTE result set is not available to other statements.","title":"Common Table Expressions (CTEs)"},{"location":"SQL/docs/Common%20Table%20Expressions/#common-table-expressions-ctes","text":"","title":"Common Table Expressions (CTEs)"},{"location":"SQL/docs/Common%20Table%20Expressions/#what-are-common-table-expressions-ctes","text":"A Common Table Expression (CTE) is the result set of a query which exists temporarily and for use only within the context of a larger query. Much like a derived table, the result of a CTE is not stored and exists only for the duration of the query. Common Table expression (CTE) is a temporary named result set that you can reference within a SELECT, INSERT, UPDATE, or DELETE statement. You can also use a CTE in a CREATE a view, as part of the view\u2019s SELECT query.","title":"What are Common Table Expressions (CTEs)?"},{"location":"SQL/docs/Common%20Table%20Expressions/#how-are-ctes-helpful","text":"CTEs, like database views and derived tables, enable users to more easily write and maintain complex queries via increased readability and simplification. This reduction in complexity is achieved by deconstructing ordinarily complex queries into simple blocks to be used, and reused if necessary, in rewriting the query. Example use cases include: - Needing to reference a derived table multiple times in a single query - An alternative to creating a view in the database - Performing the same calculation multiple times over across multiple query components","title":"How are CTEs helpful?"},{"location":"SQL/docs/Common%20Table%20Expressions/#how-to-create-cte","text":"We can define CTEs by adding a WITH clause directly before SELECT, INSERT, UPDATE, DELETE, or MERGE statement. The WITH clause can include one or more CTEs separated by commas. The following syntax can be followed: [WITH [, ...]] ::= cte_name [(column_name [, ...])] AS (cte_query) After you define your WITH clause with the CTEs, you can then reference the CTEs as you would refer any other table. However, you can refer a CTE only within the execution scope of the statement that immediately follows the WITH clause. After you\u2019ve run your statement, the CTE result set is not available to other statements.","title":"How to create CTE?"},{"location":"SQL/docs/Data-Warehousing-basics/","text":"Data Warehouse What is Data Warehouse? Put simply, it\u2019s a central place where data is stored for the purpose of analysis and reporting. The data may be collected from a variety of sources. It\u2019s organised to provide complete data for users that can be easily understood in a business context. Different from databases in that it\u2019s purpose is for analysis -In companies, we use dimensional modelling to design our data warehouses. Dimensional modelling always uses two types of tables \u2013 facts and dimensions. a. Fact Tables- Contain the measurements, metrics or facts of a business process i.e. Transactions (items & baskets) Each fact will include measures (e.g. spend) and context data. Describe the measurements/facts in a transaction table \u2013 what was bought/how much it cost, etc. b. Dimension Tables- Stores attributes that describe the objects in a fact table i.e. Stores, Products, Customers They are linked together \u2013 this is a relational model - a. Primary key - Is the attribute or a set of attributes in an entity whose value(s) guarantee only one tuple (row) exists for each value. The fact table will also include foreign keys that relate each fact to our dimension tables. b. Foreign key The primary key of another table referenced here. Each entity in a dimension table will contain an attributes that describe that entity. There will also be a key that is used to join the dimension table to the fact table. Characteristics of Data warehouse Data warehouse is a database which is separate from operational database which stores historical information also. Data warehouse database contains transaction(OLTP) as well as analytical data(OLAP). Data warehouse helps higher management to take strategic as well as tactical decisions using historical or current data. Data warehouse helps consolidated historical data analysis. Data warehouse helps business user to see the current trends to run the business. Data warehouse is used for reporting and data analysis purpose. Types of Data warehouse systems a. Data Mart Data Mart is a simplest set of Data warehouse which is used to focus on single functional area of the business. b. Online Analytical Processing (OLAP) - Refer OLAP is used at strategic level and contains aggregated data, covering number of years. The key purpose to use OLAP system is to reduce the query response time and increase the effectiveness of reporting. So, data is denormalized. OLAP uses Star-Schema,Snowflakes schema or Fact-Dimensions. OLAP database stores aggregated historical data in multidimensional schema. eg - summaries data. c. Online Transactional Processing (OLTP) - Refer It is operational database, maintaining large number of small daily transactions like insert,update and delete. Data is normailzed. OLTP uses Entity Relations. OLTP system maintains concurrency and it avoids the centralization so as to avoid the single point of failures. Data concurrency and integrity = focus. d. Predictive Analysis Difference between Data Warehouse and Data Mart a. Definition - The Data Warehouse is a large repository of data collected from different organizations or departments within a corporation. The data mart is an only sub-type of a Data Warehouse. It is designed to meet the need of a certain user group. b. Focus - Data warehouse focuses on multiple business areas. Data mart focuses only on single subject area. c. Usage - DW - It helps to take a strategic decision. DM - The data mart is used to take tactical decisions for growth of business. d. Type of system - DW - This is centralized system where one fact is at center surrounded by dimension tables. DM - Data mart system is de-centralized system e. data model - DW = top down DM - bottom up f. source - Data warehouse data comes from multiple heterogeneous data sources. Data mart data is data of only one business area.Many times it will come from only one data source. g. Implementation Time - Data warehouse contains all data which will come from multiple data sources. It will take time to build data warehouse. The Time to build data warehouse is months to years. Data mart is small data warehouse which will contain the data of only a single business area. The implementation time to build data mart is in months. Data Warehousing -It\u2019s the process of TRANSFORMING data into information and making it available to users in a TIMELY enough manner to make a difference. Different use cases of ETL a. Data Warehousing - User needs to fetch the historical data as well as current data for developing data warehouse. The Data warehouse data is nothing but combination of historical data as well as transactional data. Its data sources might be different.User needs to fetch the data from multiple heterogeneous systems and load it in to single target system which is also called as data warehouse. b. Data Migration ETL tools are widely used in data migration projects. If the organization is managing the data in oracle 10 g previously and now organization wants to go for SQL server cloud database then there is need to migrate the data from Source to Target. To do this kind of migration the ETL tools are very useful. If user wants to write the code of ETL it is very time consuming process. To make this simple the ETL tools are very useful in which the coding is simple as compare to PL SQL or T-SQL code. So ETL process i very useful in Data migration projects. c. Data Integration Now a days big organizations are acquiring small firms. Obviously the data source for the different organizations may be different.We need to integrate the data from one organization to other organization. These kind of integration projects need the ETL process to extract the data,transform the data and load the data. d. Third Party data management- many a times company outsources process to different vendors. eg - telecom - billing managed by one and CRM by other vendor. If CRM company needs some data from the company who is managing the Billing. That company will receive a data feed from the other company. To load the data from the feed ETL process is used. ETL Extract Extract source data from our client- Once the data we will receive has been agreed, it is transferred from the client to us via our secure FTP system called Axway. This data is known as source data. Once the data has been received, it is validated according to the retailer-specific rules which are outlined in the DIS. If the data does not reflect what is outlined in the DIS changes may need to be made \u2013 either by updating the DIS or requesting a resupply. Once we agree the data is in the correct format it can be read in. Data Extraction can be - Full or Partial (Delta). DIS - The data we receive is mapped in a document known as Data Interface Specification (DIS). QA check in Source / RAW layer- A key section of RAW is quality assurance (QA). We carry out standard checks to ensure data is \u201chealthy\u201d and without errors Main focus of the checks is the fact tables, and relation on the fact data with the key dimension tables. Checks include: * Number of baskets in the basket and item tables * Levels of spend * Missing foreign/primary keys Any issues found are recorded and can either be resolved with the solution DSG or may require input from the client Once these checks are successfully completed the build moves forward into the PREP stage. Transform Transform this client data to meet the operational and business needs of our internal database. Within prep we transform the client data into a standardized format within the guidelines of Marketplace What types of transformations do we perform? i. Reject bad data - Bad data i.e. record where key info is missing. What is rejected depends on the business rules. We keep a record of rejected data by extracting them to a separate table, mark the missing field. ii. Remove duplicates Duplicate data can have negative impact on results. Important to understand if it is really a duplicate before removing. iii. Convert fields Convert from character to date and numeric fields where relevant e.g. spends/quantities. iv. Text manipulations Changes format e.g. change lookup value to descriptive form. v. Merges with other tables Merge lookups/useful fields that should be on specific table. vi. Aggregate data Sometimes need to roll up products in same basket or even create basket table - involves summations. vii. Rename fields Rename to make more meaningful \u2013 esp if in a foreign language. viii. Create standard fields Essential to marketplace, same naming conventions e.g. dib_bask_code. Load Load into our analytical data mart within Marketplace The load is automated, so you will not be expected to know exactly what occurs. Here is an overview: Inbound Outbound -extracts and updates required data into standard structure Staging - manage slowly changing dimensions, generate surrogate keys and *create skeleton records SCD - a dimension is considered a SCD when its attributes remain almost constant over time, requiring relatively minor alterations to represent the evolved state. Surrogate Keys - system-generated and non-persistent integer keys which replace foreign keys. Skeleton records - Generated when a foreign key in a fact table does not have a match in the dimension table. A dummy or \u2018skeleton\u2019 record is created in the dimension table. There are following 3 Types of Data Loading Strategies : i. Initial load : Populating all the data tables from source system and loads it in to data warehouse table. ii. Incremental Load : Applying the ongoing changes as necessary in periodic manner. iii. Full Refresh : Completely erases the data from one or more tables and reload the fresh data. Star and Snowflake schema Star Schema In the star schema design, a single object (the fact table) sits in the middle and is radically connected to other surrounding objects (dimension lookup tables) like a star. Each dimension is represented as a single table. The primary key in each dimension table is related to a foreign key in the fact table. All measures in the fact table are related to all the dimensions that fact table is related to. In other words, they all have the same level of granularity. A star schema can be simple or complex. A simple star consists of one fact table; a complex star can have more than one fact table. Snowflake Schema It is an extension of star schema. In a star schema, each dimension is represented by a single dimensional table, whereas in a snowflake schema, that dimensional table is normalized into multiple lookup tables, each representing a level in the dimensional hierarchy. Adv - improvement in query performance due to minimized disk storage requirements and joining smaller lookup tables. Disadvantage- additional maintenance efforts needed due to the increase number of lookup tables. Fact Table Granularity The first step in designing a fact table is to determine the granularity of the fact table. By granularity, we mean the lowest level of information that will be stored in the fact table. This constitutes two steps: i. Determine which dimensions will be included - this depends on business process being targetted. ii. Determine where along the hierarchy of each dimension the information will be kept - This depends on requirements. Eg - if client wants hourly reports, then fact table will keep hour as lowest level of granularity. If daily reports are fine, then date_id is lowest level of granularity. The determining factors usually goes back to the requirements. Fact And Fact Table Types There are three types of facts: i. Additive: Additive facts are facts that can be summed up through all of the dimensions in the fact table. ii. Semi-Additive: Semi-additive facts are facts that can be summed up for some of the dimensions in the fact table, but not the others. iii. Non-Additive: Non-additive facts are facts that cannot be summed up for any of the dimensions present in the fact table. eg1 - Additive Fact - Consider a retailer fact table with following columns - - Date - Store - Product - Sales_Amount The purpose of this table is to record the sales amount for each product in each store on a daily basis. Sales_Amount is an additive fact, because you can sum up this fact along any of the three dimensions present in the fact table -- date, store, and product. eg2A - Semi-Additive Fact and Non-Additive Fact - Say we are a bank with the following fact table: - Date - Account - Current_Balance - Profit_Margin The purpose of this table is to record the current balance for each account at the end of each day, as well as the profit margin for each account for each day. Current_Balance and Profit_Margin are the facts. Current_Balance is a semi-additive fact, as it makes sense to add them up for all accounts (what's the total current balance for all accounts in the bank?), but it does not make sense to add them up through time (adding up all current balances for a given account for each day of the month does not give us any useful information). Profit_Margin is a non-additive fact, for it does not make sense to add them up for the account level or the day level. eg 2B - semi -additive - distinct customers who shopped in a day = semi additive. Across all stores, this number can be aggregated. For example, store A has 300 customers and store B has 200 customers. So total 500 customers. But cant add across date dimension. So no summation possible across days in a week. non-additive = %age loyalty transaction in a day. For example, store A has 30% sales as loyalty count, and store B has 40%. But we cant add these two figures to find overall loyalty sales. Based on the above classifications, there are two types of Fact TABLES: * Cumulative: This type of fact table describes what has happened over a period of time. For example, this fact table may describe the total sales by product by store by day. The facts for this type of fact tables are mostly additive facts. The first example presented here is a cumulative fact table. * Snapshot: This type of fact table describes the state of things in a particular instance of time, and usually includes more semi-additive and non-additive facts. The second example presented here is a snapshot fact table. Slowly Changing Dimensions The \"Slowly Changing Dimension\" problem is a common one particular to data warehousing. In a nutshell, this applies to cases where the attribute for a record varies over time. There are in general three ways to solve this type of problem, and they are categorized as follows: Type 1 The new record replaces the original record. No trace of the old record exists. In other words, no history is kept. Advantage - * easiest to handle as no need to maintain history. Disadvantage- *History is lost. Cant track past behavior. So, Type 1 slowly changing dimension should be used when it is not necessary for the data warehouse to keep track of historical changes. Type 2 A new record is added into the customer dimension table. Therefore, the customer is treated essentially as two people. Both the original and the new record will be present. The new record gets its own primary key. Advantages: - This allows us to accurately keep all historical information. Disadvantages: - This will cause the size of the table to grow fast. In cases where the number of rows for the table is very high to start with, storage and performance can become a concern. - This necessarily complicates the ETL process. Type 3 The original record is modified to reflect the change. We add more column to track change. But this is feasible only if changes to be tracked are finite. For example, phone or address changes more than once will complicate things. Data Integrity Data integrity refers to the validity of data, meaning data is consistent and correct. In the data warehousing field, we frequently hear the term, \"Garbage In, Garbage Out.\" If there is no data integrity in the data warehouse, any resulting report and analysis will not be useful. In a data warehouse or a data mart, there are 3 areas of where data integrity needs to be enforced: a. Database level We can enforce data integrity at the database level. Common ways of enforcing data integrity include: i. Referential integrity The relationship between the primary key of one table and the foreign key of another table must always be maintained. For example, a primary key cannot be deleted if there is still a foreign key that refers to this primary key. ii. Primary key / Unique constraint Primary keys and the UNIQUE constraint are used to make sure every row in a table can be uniquely identified. iii. Not NULL vs. NULL-able For columns identified as NOT NULL, they may not have a NULL value. iv. Valid Values Only allowed values are permitted in the database. For example, if a column can only have positive integers, a value of '-1' cannot be allowed. b. ETL process For each step of the ETL process, data integrity checks should be put in place to ensure that source data is the same as the data in the destination. Most common checks include record counts or record sums. c. Access level We need to ensure that data is not altered by any unauthorized means either during the ETL process or in the data warehouse. To do this, there needs to be safeguards against unauthorized access to data (including physical access to the servers), as well as logging of all data access history. Data integrity can only ensured if there is no unauthorized access to the data. 4F. Factless Fact Table A factless fact table is a fact table that does not have any measures. It is essentially an intersection of dimensions. On the surface, a factless fact table does not make sense, since a fact table is, after all, about facts. However, there are situations where having this kind of relationship makes sense in data warehousing. eg1 - student class attendance record. In this case, the fact table would consist of 3 dimensions: the student dimension, the time dimension, and the class dimension. This factless fact table would look like the following: Fact Table \"school_attendance\" date_id classId student_id 02-02-2020 1 101 02-02-2020 1 102 02-02-2020 1 103 ---------------------------------- The only measure that you can possibly attach to each combination is \"1\" to show the presence of that particular combination. However, adding a fact that always shows 1 is redundant because we can simply use the COUNT function in SQL to answer the same questions. eg 2 - online sales in CRV. columns - date_id, store_id, till_id, pos_id In essence it contains only 1 column = basket_key. If a basket is in this table, it means its online sale, else offline sale. eg3 - Promotion data. Table structure could be - date_id | store_id| promo_type| promo_id| basket_key promo_type = Promotion can be online, in-store, flat discount, coupon, voucher, etc. Above table contains info of promotion applied on a basket. No measurable fact exists here. But why needed ? Transaction data contains info of what item was sold on promotion. But promotion data contains information of all the promotion during the purchase period. That is, all products having promotion applied on them, including those which were not sold in spite of promotion. And so, this promotion table becomes pivotal even though it contains no measurable fact. Why need factless facts? Factless fact tables offer the most flexibility in data warehouse design. For example, one can easily answer the following questions with this factless fact table: * How many students attended a particular class on a particular day? * How many classes on average does a student attend on a given day? Without using a factless fact table, we will need two separate fact tables to answer the above two questions. With the above factless fact table, it becomes the only fact table that's needed. Junk Dimension There are columns in Fact table which can have only a few or 2 kind of values - true or false, 1 or 0, etc. eg = bulk Vs non-bulk online vs offline promo vs non-promo vs hybrid sale etc. From business point of view, capturing above info in Fact table is very important. Issue -having these info will only make our fact table bulky and eventually unmanageable. Soln - junk dimension. eg - CRV basket channel seg - shop_channel_code in 0,1,2 or 3 - covers both bulk/non-bulk and online/offline. this would reduce 2 columns in fact table to 1. we can expand scope of above column to include promo info, and in that way we replace 3 fact columns by 1. This will result in a data warehousing environment that offer better performance as well as being easier to manage. [reference] (https://www.1keydata.com/datawarehousing/junk-dimension.html)","title":"Data Warehousing Concepts"},{"location":"SQL/docs/Data-Warehousing-basics/#data-warehouse","text":"","title":"Data Warehouse"},{"location":"SQL/docs/Data-Warehousing-basics/#what-is-data-warehouse","text":"Put simply, it\u2019s a central place where data is stored for the purpose of analysis and reporting. The data may be collected from a variety of sources. It\u2019s organised to provide complete data for users that can be easily understood in a business context. Different from databases in that it\u2019s purpose is for analysis -In companies, we use dimensional modelling to design our data warehouses. Dimensional modelling always uses two types of tables \u2013 facts and dimensions. a. Fact Tables- Contain the measurements, metrics or facts of a business process i.e. Transactions (items & baskets) Each fact will include measures (e.g. spend) and context data. Describe the measurements/facts in a transaction table \u2013 what was bought/how much it cost, etc. b. Dimension Tables- Stores attributes that describe the objects in a fact table i.e. Stores, Products, Customers They are linked together \u2013 this is a relational model - a. Primary key - Is the attribute or a set of attributes in an entity whose value(s) guarantee only one tuple (row) exists for each value. The fact table will also include foreign keys that relate each fact to our dimension tables. b. Foreign key The primary key of another table referenced here. Each entity in a dimension table will contain an attributes that describe that entity. There will also be a key that is used to join the dimension table to the fact table.","title":"What is Data Warehouse?"},{"location":"SQL/docs/Data-Warehousing-basics/#characteristics-of-data-warehouse","text":"Data warehouse is a database which is separate from operational database which stores historical information also. Data warehouse database contains transaction(OLTP) as well as analytical data(OLAP). Data warehouse helps higher management to take strategic as well as tactical decisions using historical or current data. Data warehouse helps consolidated historical data analysis. Data warehouse helps business user to see the current trends to run the business. Data warehouse is used for reporting and data analysis purpose.","title":"Characteristics of Data warehouse"},{"location":"SQL/docs/Data-Warehousing-basics/#types-of-data-warehouse-systems","text":"a. Data Mart Data Mart is a simplest set of Data warehouse which is used to focus on single functional area of the business. b. Online Analytical Processing (OLAP) - Refer OLAP is used at strategic level and contains aggregated data, covering number of years. The key purpose to use OLAP system is to reduce the query response time and increase the effectiveness of reporting. So, data is denormalized. OLAP uses Star-Schema,Snowflakes schema or Fact-Dimensions. OLAP database stores aggregated historical data in multidimensional schema. eg - summaries data. c. Online Transactional Processing (OLTP) - Refer It is operational database, maintaining large number of small daily transactions like insert,update and delete. Data is normailzed. OLTP uses Entity Relations. OLTP system maintains concurrency and it avoids the centralization so as to avoid the single point of failures. Data concurrency and integrity = focus. d. Predictive Analysis","title":"Types of Data warehouse systems"},{"location":"SQL/docs/Data-Warehousing-basics/#difference-between-data-warehouse-and-data-mart","text":"a. Definition - The Data Warehouse is a large repository of data collected from different organizations or departments within a corporation. The data mart is an only sub-type of a Data Warehouse. It is designed to meet the need of a certain user group. b. Focus - Data warehouse focuses on multiple business areas. Data mart focuses only on single subject area. c. Usage - DW - It helps to take a strategic decision. DM - The data mart is used to take tactical decisions for growth of business. d. Type of system - DW - This is centralized system where one fact is at center surrounded by dimension tables. DM - Data mart system is de-centralized system e. data model - DW = top down DM - bottom up f. source - Data warehouse data comes from multiple heterogeneous data sources. Data mart data is data of only one business area.Many times it will come from only one data source. g. Implementation Time - Data warehouse contains all data which will come from multiple data sources. It will take time to build data warehouse. The Time to build data warehouse is months to years. Data mart is small data warehouse which will contain the data of only a single business area. The implementation time to build data mart is in months.","title":"Difference between Data Warehouse and Data Mart"},{"location":"SQL/docs/Data-Warehousing-basics/#data-warehousing","text":"-It\u2019s the process of TRANSFORMING data into information and making it available to users in a TIMELY enough manner to make a difference.","title":"Data Warehousing"},{"location":"SQL/docs/Data-Warehousing-basics/#different-use-cases-of-etl","text":"a. Data Warehousing - User needs to fetch the historical data as well as current data for developing data warehouse. The Data warehouse data is nothing but combination of historical data as well as transactional data. Its data sources might be different.User needs to fetch the data from multiple heterogeneous systems and load it in to single target system which is also called as data warehouse. b. Data Migration ETL tools are widely used in data migration projects. If the organization is managing the data in oracle 10 g previously and now organization wants to go for SQL server cloud database then there is need to migrate the data from Source to Target. To do this kind of migration the ETL tools are very useful. If user wants to write the code of ETL it is very time consuming process. To make this simple the ETL tools are very useful in which the coding is simple as compare to PL SQL or T-SQL code. So ETL process i very useful in Data migration projects. c. Data Integration Now a days big organizations are acquiring small firms. Obviously the data source for the different organizations may be different.We need to integrate the data from one organization to other organization. These kind of integration projects need the ETL process to extract the data,transform the data and load the data. d. Third Party data management- many a times company outsources process to different vendors. eg - telecom - billing managed by one and CRM by other vendor. If CRM company needs some data from the company who is managing the Billing. That company will receive a data feed from the other company. To load the data from the feed ETL process is used.","title":"Different use cases of ETL"},{"location":"SQL/docs/Data-Warehousing-basics/#etl","text":"","title":"ETL"},{"location":"SQL/docs/Data-Warehousing-basics/#extract","text":"Extract source data from our client- Once the data we will receive has been agreed, it is transferred from the client to us via our secure FTP system called Axway. This data is known as source data. Once the data has been received, it is validated according to the retailer-specific rules which are outlined in the DIS. If the data does not reflect what is outlined in the DIS changes may need to be made \u2013 either by updating the DIS or requesting a resupply. Once we agree the data is in the correct format it can be read in. Data Extraction can be - Full or Partial (Delta). DIS - The data we receive is mapped in a document known as Data Interface Specification (DIS). QA check in Source / RAW layer- A key section of RAW is quality assurance (QA). We carry out standard checks to ensure data is \u201chealthy\u201d and without errors Main focus of the checks is the fact tables, and relation on the fact data with the key dimension tables. Checks include: * Number of baskets in the basket and item tables * Levels of spend * Missing foreign/primary keys Any issues found are recorded and can either be resolved with the solution DSG or may require input from the client Once these checks are successfully completed the build moves forward into the PREP stage.","title":"Extract"},{"location":"SQL/docs/Data-Warehousing-basics/#transform","text":"Transform this client data to meet the operational and business needs of our internal database. Within prep we transform the client data into a standardized format within the guidelines of Marketplace What types of transformations do we perform? i. Reject bad data - Bad data i.e. record where key info is missing. What is rejected depends on the business rules. We keep a record of rejected data by extracting them to a separate table, mark the missing field. ii. Remove duplicates Duplicate data can have negative impact on results. Important to understand if it is really a duplicate before removing. iii. Convert fields Convert from character to date and numeric fields where relevant e.g. spends/quantities. iv. Text manipulations Changes format e.g. change lookup value to descriptive form. v. Merges with other tables Merge lookups/useful fields that should be on specific table. vi. Aggregate data Sometimes need to roll up products in same basket or even create basket table - involves summations. vii. Rename fields Rename to make more meaningful \u2013 esp if in a foreign language. viii. Create standard fields Essential to marketplace, same naming conventions e.g. dib_bask_code.","title":"Transform"},{"location":"SQL/docs/Data-Warehousing-basics/#load","text":"Load into our analytical data mart within Marketplace The load is automated, so you will not be expected to know exactly what occurs. Here is an overview: Inbound Outbound -extracts and updates required data into standard structure Staging - manage slowly changing dimensions, generate surrogate keys and *create skeleton records SCD - a dimension is considered a SCD when its attributes remain almost constant over time, requiring relatively minor alterations to represent the evolved state. Surrogate Keys - system-generated and non-persistent integer keys which replace foreign keys. Skeleton records - Generated when a foreign key in a fact table does not have a match in the dimension table. A dummy or \u2018skeleton\u2019 record is created in the dimension table. There are following 3 Types of Data Loading Strategies : i. Initial load : Populating all the data tables from source system and loads it in to data warehouse table. ii. Incremental Load : Applying the ongoing changes as necessary in periodic manner. iii. Full Refresh : Completely erases the data from one or more tables and reload the fresh data.","title":"Load"},{"location":"SQL/docs/Data-Warehousing-basics/#star-and-snowflake-schema","text":"","title":"Star and Snowflake schema"},{"location":"SQL/docs/Data-Warehousing-basics/#star-schema","text":"In the star schema design, a single object (the fact table) sits in the middle and is radically connected to other surrounding objects (dimension lookup tables) like a star. Each dimension is represented as a single table. The primary key in each dimension table is related to a foreign key in the fact table. All measures in the fact table are related to all the dimensions that fact table is related to. In other words, they all have the same level of granularity. A star schema can be simple or complex. A simple star consists of one fact table; a complex star can have more than one fact table.","title":"Star Schema"},{"location":"SQL/docs/Data-Warehousing-basics/#snowflake-schema","text":"It is an extension of star schema. In a star schema, each dimension is represented by a single dimensional table, whereas in a snowflake schema, that dimensional table is normalized into multiple lookup tables, each representing a level in the dimensional hierarchy. Adv - improvement in query performance due to minimized disk storage requirements and joining smaller lookup tables. Disadvantage- additional maintenance efforts needed due to the increase number of lookup tables.","title":"Snowflake Schema"},{"location":"SQL/docs/Data-Warehousing-basics/#fact-table-granularity","text":"The first step in designing a fact table is to determine the granularity of the fact table. By granularity, we mean the lowest level of information that will be stored in the fact table. This constitutes two steps: i. Determine which dimensions will be included - this depends on business process being targetted. ii. Determine where along the hierarchy of each dimension the information will be kept - This depends on requirements. Eg - if client wants hourly reports, then fact table will keep hour as lowest level of granularity. If daily reports are fine, then date_id is lowest level of granularity. The determining factors usually goes back to the requirements.","title":"Fact Table Granularity"},{"location":"SQL/docs/Data-Warehousing-basics/#fact-and-fact-table-types","text":"There are three types of facts: i. Additive: Additive facts are facts that can be summed up through all of the dimensions in the fact table. ii. Semi-Additive: Semi-additive facts are facts that can be summed up for some of the dimensions in the fact table, but not the others. iii. Non-Additive: Non-additive facts are facts that cannot be summed up for any of the dimensions present in the fact table. eg1 - Additive Fact - Consider a retailer fact table with following columns - - Date - Store - Product - Sales_Amount The purpose of this table is to record the sales amount for each product in each store on a daily basis. Sales_Amount is an additive fact, because you can sum up this fact along any of the three dimensions present in the fact table -- date, store, and product. eg2A - Semi-Additive Fact and Non-Additive Fact - Say we are a bank with the following fact table: - Date - Account - Current_Balance - Profit_Margin The purpose of this table is to record the current balance for each account at the end of each day, as well as the profit margin for each account for each day. Current_Balance and Profit_Margin are the facts. Current_Balance is a semi-additive fact, as it makes sense to add them up for all accounts (what's the total current balance for all accounts in the bank?), but it does not make sense to add them up through time (adding up all current balances for a given account for each day of the month does not give us any useful information). Profit_Margin is a non-additive fact, for it does not make sense to add them up for the account level or the day level. eg 2B - semi -additive - distinct customers who shopped in a day = semi additive. Across all stores, this number can be aggregated. For example, store A has 300 customers and store B has 200 customers. So total 500 customers. But cant add across date dimension. So no summation possible across days in a week. non-additive = %age loyalty transaction in a day. For example, store A has 30% sales as loyalty count, and store B has 40%. But we cant add these two figures to find overall loyalty sales. Based on the above classifications, there are two types of Fact TABLES: * Cumulative: This type of fact table describes what has happened over a period of time. For example, this fact table may describe the total sales by product by store by day. The facts for this type of fact tables are mostly additive facts. The first example presented here is a cumulative fact table. * Snapshot: This type of fact table describes the state of things in a particular instance of time, and usually includes more semi-additive and non-additive facts. The second example presented here is a snapshot fact table.","title":"Fact And Fact Table Types "},{"location":"SQL/docs/Data-Warehousing-basics/#slowly-changing-dimensions","text":"The \"Slowly Changing Dimension\" problem is a common one particular to data warehousing. In a nutshell, this applies to cases where the attribute for a record varies over time. There are in general three ways to solve this type of problem, and they are categorized as follows:","title":"Slowly Changing Dimensions"},{"location":"SQL/docs/Data-Warehousing-basics/#type-1","text":"The new record replaces the original record. No trace of the old record exists. In other words, no history is kept. Advantage - * easiest to handle as no need to maintain history. Disadvantage- *History is lost. Cant track past behavior. So, Type 1 slowly changing dimension should be used when it is not necessary for the data warehouse to keep track of historical changes.","title":"Type 1"},{"location":"SQL/docs/Data-Warehousing-basics/#type-2","text":"A new record is added into the customer dimension table. Therefore, the customer is treated essentially as two people. Both the original and the new record will be present. The new record gets its own primary key. Advantages: - This allows us to accurately keep all historical information. Disadvantages: - This will cause the size of the table to grow fast. In cases where the number of rows for the table is very high to start with, storage and performance can become a concern. - This necessarily complicates the ETL process.","title":"Type 2"},{"location":"SQL/docs/Data-Warehousing-basics/#type-3","text":"The original record is modified to reflect the change. We add more column to track change. But this is feasible only if changes to be tracked are finite. For example, phone or address changes more than once will complicate things.","title":"Type 3"},{"location":"SQL/docs/Data-Warehousing-basics/#data-integrity","text":"Data integrity refers to the validity of data, meaning data is consistent and correct. In the data warehousing field, we frequently hear the term, \"Garbage In, Garbage Out.\" If there is no data integrity in the data warehouse, any resulting report and analysis will not be useful. In a data warehouse or a data mart, there are 3 areas of where data integrity needs to be enforced: a. Database level We can enforce data integrity at the database level. Common ways of enforcing data integrity include: i. Referential integrity The relationship between the primary key of one table and the foreign key of another table must always be maintained. For example, a primary key cannot be deleted if there is still a foreign key that refers to this primary key. ii. Primary key / Unique constraint Primary keys and the UNIQUE constraint are used to make sure every row in a table can be uniquely identified. iii. Not NULL vs. NULL-able For columns identified as NOT NULL, they may not have a NULL value. iv. Valid Values Only allowed values are permitted in the database. For example, if a column can only have positive integers, a value of '-1' cannot be allowed. b. ETL process For each step of the ETL process, data integrity checks should be put in place to ensure that source data is the same as the data in the destination. Most common checks include record counts or record sums. c. Access level We need to ensure that data is not altered by any unauthorized means either during the ETL process or in the data warehouse. To do this, there needs to be safeguards against unauthorized access to data (including physical access to the servers), as well as logging of all data access history. Data integrity can only ensured if there is no unauthorized access to the data. 4F. Factless Fact Table A factless fact table is a fact table that does not have any measures. It is essentially an intersection of dimensions. On the surface, a factless fact table does not make sense, since a fact table is, after all, about facts. However, there are situations where having this kind of relationship makes sense in data warehousing. eg1 - student class attendance record. In this case, the fact table would consist of 3 dimensions: the student dimension, the time dimension, and the class dimension. This factless fact table would look like the following: Fact Table \"school_attendance\" date_id classId student_id 02-02-2020 1 101 02-02-2020 1 102 02-02-2020 1 103 ---------------------------------- The only measure that you can possibly attach to each combination is \"1\" to show the presence of that particular combination. However, adding a fact that always shows 1 is redundant because we can simply use the COUNT function in SQL to answer the same questions. eg 2 - online sales in CRV. columns - date_id, store_id, till_id, pos_id In essence it contains only 1 column = basket_key. If a basket is in this table, it means its online sale, else offline sale. eg3 - Promotion data. Table structure could be - date_id | store_id| promo_type| promo_id| basket_key promo_type = Promotion can be online, in-store, flat discount, coupon, voucher, etc. Above table contains info of promotion applied on a basket. No measurable fact exists here. But why needed ? Transaction data contains info of what item was sold on promotion. But promotion data contains information of all the promotion during the purchase period. That is, all products having promotion applied on them, including those which were not sold in spite of promotion. And so, this promotion table becomes pivotal even though it contains no measurable fact. Why need factless facts? Factless fact tables offer the most flexibility in data warehouse design. For example, one can easily answer the following questions with this factless fact table: * How many students attended a particular class on a particular day? * How many classes on average does a student attend on a given day? Without using a factless fact table, we will need two separate fact tables to answer the above two questions. With the above factless fact table, it becomes the only fact table that's needed.","title":"Data Integrity"},{"location":"SQL/docs/Data-Warehousing-basics/#junk-dimension","text":"There are columns in Fact table which can have only a few or 2 kind of values - true or false, 1 or 0, etc. eg = bulk Vs non-bulk online vs offline promo vs non-promo vs hybrid sale etc. From business point of view, capturing above info in Fact table is very important. Issue -having these info will only make our fact table bulky and eventually unmanageable. Soln - junk dimension. eg - CRV basket channel seg - shop_channel_code in 0,1,2 or 3 - covers both bulk/non-bulk and online/offline. this would reduce 2 columns in fact table to 1. we can expand scope of above column to include promo info, and in that way we replace 3 fact columns by 1. This will result in a data warehousing environment that offer better performance as well as being easier to manage. [reference] (https://www.1keydata.com/datawarehousing/junk-dimension.html)","title":"Junk Dimension"},{"location":"SQL/docs/SQL%20leetcode/","text":"Second Highest Salary Link : https://leetcode.com/problems/second-highest-salary/ Table: Employee +-------------+------+ | Column Name | Type | +-------------+------+ | id | int | | salary | int | +-------------+------+ id is the primary key column for this table. Each row of this table contains information about the salary of an employee. Write an SQL query to report the second highest salary from the Employee table. If there is no second highest salary, the query should report null. The query result format is in the following example. Example 1: Input: Employee table: +----+--------+ | id | salary | +----+--------+ | 1 | 100 | | 2 | 200 | | 3 | 300 | +----+--------+ Output: +---------------------+ | SecondHighestSalary | +---------------------+ | 200 | +---------------------+ Example 2: Input: Employee table: +----+--------+ | id | salary | +----+--------+ | 1 | 100 | +----+--------+ Output: +---------------------+ | SecondHighestSalary | +---------------------+ | null | +---------------------+ Ans select max(salary) as SecondHighestSalary from (select salary, dense_rank() over (order by salary desc) as sal_rank from employee ) where sal_rank=2; Note -> if Nth highest salary --> Link : https://leetcode.com/problems/nth-highest-salary/ CREATE FUNCTION getNthHighestSalary(N IN NUMBER) RETURN NUMBER IS result NUMBER; BEGIN /* Write your PL/SQL query statement below */ select max(salary) into result from ( select salary, dense_rank() over (order by salary desc) as sal_rank from employee ) where sal_rank=N; RETURN result; END; Find Median Given Frequency of Numbers Link : https://leetcode.com/problems/find-median-given-frequency-of-numbers/ Table: Numbers +-------------+------+ | Column Name | Type | +-------------+------+ | num | int | | frequency | int | +-------------+------+ num is the primary key for this table. Each row of this table shows the frequency of a number in the database. The median is the value separating the higher half from the lower half of a data sample. Write an SQL query to report the median of all the numbers in the database after decompressing the Numbers table. Round the median to one decimal point. The query result format is in the following example. Example 1: Input: Numbers table: +-----+-----------+ | num | frequency | +-----+-----------+ | 0 | 7 | | 1 | 1 | | 2 | 3 | | 3 | 1 | +-----+-----------+ Output: +--------+ | median | +--------+ | 0.0 | +--------+ Explanation: If we decompress the Numbers table, we will get [0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 3], so the median is (0 + 0) / 2 = 0. Ans select round(avg(num),1) as median from ( select num, frequency, lag(cum_sum,1,0) over (order by num) as prev_sum, cum_sum, medium_num from (select num, frequency, sum(frequency) over (order by num) as cum_sum, sum(frequency) over () / 2 as medium_num from numbers) a ) b where medium_num between prev_sum and cum_sum; Find Cumulative Salary of an Employee Table: Employee +-------------+------+ | Column Name | Type | +-------------+------+ | id | int | | month | int | | salary | int | +-------------+------+ (id, month) is the primary key for this table. Each row in the table indicates the salary of an employee in one month during the year 2020. Write an SQL query to calculate the cumulative salary summary for every employee in a single unified table. The cumulative salary summary for an employee can be calculated as follows: - For each month that the employee worked, sum up the salaries in that month and the previous two months. This is their 3-month sum for that month. If an employee did not work for the company in previous months, their effective salary for those months is 0. - Do not include the 3-month sum for the most recent month that the employee worked for in the summary. - Do not include the 3-month sum for any month the employee did not work. - Return the result table ordered by id in ascending order. In case of a tie, order it by month in descending order. The query result format is in the following example. Example 1: Input: Employee table: +----+-------+--------+ | id | month | salary | +----+-------+--------+ | 1 | 1 | 20 | | 2 | 1 | 20 | | 1 | 2 | 30 | | 2 | 2 | 30 | | 3 | 2 | 40 | | 1 | 3 | 40 | | 3 | 3 | 60 | | 1 | 4 | 60 | | 3 | 4 | 70 | | 1 | 7 | 90 | | 1 | 8 | 90 | +----+-------+--------+ Output: +----+-------+--------+ | id | month | Salary | +----+-------+--------+ | 1 | 7 | 90 | | 1 | 4 | 130 | | 1 | 3 | 90 | | 1 | 2 | 50 | | 1 | 1 | 20 | | 2 | 1 | 20 | | 3 | 3 | 100 | | 3 | 2 | 40 | +----+-------+--------+ Explanation: Employee '1' has five salary records excluding their most recent month '8': - 90 for month '7'. - 60 for month '4'. - 40 for month '3'. - 30 for month '2'. - 20 for month '1'. So the cumulative salary summary for this employee is: +----+-------+--------+ | id | month | salary | +----+-------+--------+ | 1 | 7 | 90 | (90 + 0 + 0) | 1 | 4 | 130 | (60 + 40 + 30) | 1 | 3 | 90 | (40 + 30 + 20) | 1 | 2 | 50 | (30 + 20 + 0) | 1 | 1 | 20 | (20 + 0 + 0) +----+-------+--------+ Note that the 3-month sum for month '7' is 90 because they did not work during month '6' or month '5'. Employee '2' only has one salary record (month '1') excluding their most recent month '2'. +----+-------+--------+ | id | month | salary | +----+-------+--------+ | 2 | 1 | 20 | (20 + 0 + 0) +----+-------+--------+ Employee '3' has two salary records excluding their most recent month '4': - 60 for month '3'. - 40 for month '2'. So the cumulative salary summary for this employee is: +----+-------+--------+ | id | month | salary | +----+-------+--------+ | 3 | 3 | 100 | (60 + 40 + 0) | 3 | 2 | 40 | (40 + 0 + 0) +----+-------+--------+ Ans select id, month, sum(salary) over (partition by id order by month range between 2 PRECEDING and current row) as salary from ( select id, month, salary, max(month) over (partition by id order by month rows between unbounded preceding and unbounded following) as max_month_for_employee from employee ) where month!=max_month_for_employee order by id, month desc; Average Salary: Departments VS Company Link : https://leetcode.com/problems/average-salary-departments-vs-company/ Table: Salary +-------------+------+ | Column Name | Type | +-------------+------+ | id | int | | employee_id | int | | amount | int | | pay_date | date | +-------------+------+ id is the primary key column for this table. Each row of this table indicates the salary of an employee in one month. employee_id is a foreign key from the Employee table. Table: Employee +---------------+------+ | Column Name | Type | +---------------+------+ | employee_id | int | | department_id | int | +---------------+------+ employee_id is the primary key column for this table. Each row of this table indicates the department of an employee. Write an SQL query to report the comparison result (higher/lower/same) of the average salary of employees in a department to the company's average salary. Return the result table in any order. The query result format is in the following example. Example 1: Input: Salary table: +----+-------------+--------+------------+ | id | employee_id | amount | pay_date | +----+-------------+--------+------------+ | 1 | 1 | 9000 | 2017/03/31 | | 2 | 2 | 6000 | 2017/03/31 | | 3 | 3 | 10000 | 2017/03/31 | | 4 | 1 | 7000 | 2017/02/28 | | 5 | 2 | 6000 | 2017/02/28 | | 6 | 3 | 8000 | 2017/02/28 | +----+-------------+--------+------------+ Employee table: +-------------+---------------+ | employee_id | department_id | +-------------+---------------+ | 1 | 1 | | 2 | 2 | | 3 | 2 | +-------------+---------------+ Output: +-----------+---------------+------------+ | pay_month | department_id | comparison | +-----------+---------------+------------+ | 2017-02 | 1 | same | | 2017-03 | 1 | higher | | 2017-02 | 2 | same | | 2017-03 | 2 | lower | +-----------+---------------+------------+ Explanation: In March, the company's average salary is (9000+6000+10000)/3 = 8333.33... The average salary for department '1' is 9000, which is the salary of employee_id '1' since there is only one employee in this department. So the comparison result is 'higher' since 9000 > 8333.33 obviously. The average salary of department '2' is (6000 + 10000)/2 = 8000, which is the average of employee_id '2' and '3'. So the comparison result is 'lower' since 8000 < 8333.33. With he same formula for the average salary comparison in February, the result is 'same' since both the department '1' and '2' have the same average salary with the company, which is 7000. Ans select distinct pay_month, department_id, case when avg_dept<avg_company then 'lower' when avg_dept=avg_company then 'same' when avg_dept>avg_company then 'higher' end as comparison from ( select department_id, date_format(pay_date, '%Y-%m') as pay_month, avg(employee_salary) over (partition by pay_date ) as avg_company, avg(employee_salary) over (partition by pay_date, department_id ) as avg_dept from (select s.employee_id, s.amount as employee_salary, s.pay_date, e.department_id from salary s inner join employee e on s.employee_id = e.employee_id) a ) b Sales by Day of the Week https://leetcode.com/problems/sales-by-day-of-the-week/ Table: Orders +---------------+---------+ | Column Name | Type | +---------------+---------+ | order_id | int | | customer_id | int | | order_date | date | | item_id | varchar | | quantity | int | +---------------+---------+ (ordered_id, item_id) is the primary key for this table. This table contains information on the orders placed. order_date is the date item_id was ordered by the customer with id customer_id. Table: Items +---------------------+---------+ | Column Name | Type | +---------------------+---------+ | item_id | varchar | | item_name | varchar | | item_category | varchar | +---------------------+---------+ item_id is the primary key for this table. item_name is the name of the item. item_category is the category of the item. You are the business owner and would like to obtain a sales report for category items and the day of the week. Write an SQL query to report how many units in each category have been ordered on each day of the week. Return the result table ordered by category. The query result format is in the following example. Example 1: Input: Orders table: +------------+--------------+-------------+--------------+-------------+ | order_id | customer_id | order_date | item_id | quantity | +------------+--------------+-------------+--------------+-------------+ | 1 | 1 | 2020-06-01 | 1 | 10 | | 2 | 1 | 2020-06-08 | 2 | 10 | | 3 | 2 | 2020-06-02 | 1 | 5 | | 4 | 3 | 2020-06-03 | 3 | 5 | | 5 | 4 | 2020-06-04 | 4 | 1 | | 6 | 4 | 2020-06-05 | 5 | 5 | | 7 | 5 | 2020-06-05 | 1 | 10 | | 8 | 5 | 2020-06-14 | 4 | 5 | | 9 | 5 | 2020-06-21 | 3 | 5 | +------------+--------------+-------------+--------------+-------------+ Items table: +------------+----------------+---------------+ | item_id | item_name | item_category | +------------+----------------+---------------+ | 1 | LC Alg. Book | Book | | 2 | LC DB. Book | Book | | 3 | LC SmarthPhone | Phone | | 4 | LC Phone 2020 | Phone | | 5 | LC SmartGlass | Glasses | | 6 | LC T-Shirt XL | T-Shirt | +------------+----------------+---------------+ Output: +------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+ | Category | Monday | Tuesday | Wednesday | Thursday | Friday | Saturday | Sunday | +------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+ | Book | 20 | 5 | 0 | 0 | 10 | 0 | 0 | | Glasses | 0 | 0 | 0 | 0 | 5 | 0 | 0 | | Phone | 0 | 0 | 5 | 1 | 0 | 0 | 10 | | T-Shirt | 0 | 0 | 0 | 0 | 0 | 0 | 0 | +------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+ Explanation: On Monday (2020-06-01, 2020-06-08) were sold a total of 20 units (10 + 10) in the category Book (ids: 1, 2). On Tuesday (2020-06-02) were sold a total of 5 units in the category Book (ids: 1, 2). On Wednesday (2020-06-03) were sold a total of 5 units in the category Phone (ids: 3, 4). On Thursday (2020-06-04) were sold a total of 1 unit in the category Phone (ids: 3, 4). On Friday (2020-06-05) were sold 10 units in the category Book (ids: 1, 2) and 5 units in Glasses (ids: 5). On Saturday there are no items sold. On Sunday (2020-06-14, 2020-06-21) were sold a total of 10 units (5 +5) in the category Phone (ids: 3, 4). There are no sales of T-shirts. Ans with cte_table as (select b.item_category, dayname(a.order_date) as day_of_week, COALESCE(sum(a.quantity),0) as qty from orders a RIGHT join items b on a.item_id = b.item_id group by b.item_category, dayname(a.order_date) ) select item_category as \"CATEGORY\", sum(case when upper(day_of_week)='MONDAY' then qty else 0 end) as 'MONDAY', sum(case when upper(day_of_week)='TUESDAY' then qty else 0 end) as 'TUESDAY', sum(case when upper(day_of_week)='WEDNESDAY' then qty else 0 end) as 'WEDNESDAY', sum(case when upper(day_of_week)='THURSDAY' then qty else 0 end) as 'THURSDAY', sum(case when upper(day_of_week)='FRIDAY' then qty else 0 end) as 'FRIDAY', sum(case when upper(day_of_week)='SATURDAY' then qty else 0 end) as 'SATURDAY', sum(case when upper(day_of_week)='SUNDAY' then qty else 0 end) as 'SUNDAY' from cte_table group by item_category order by item_category; Report Contiguous Dates https://leetcode.com/problems/report-contiguous-dates/ Table: Failed +--------------+---------+ | Column Name | Type | +--------------+---------+ | fail_date | date | +--------------+---------+ fail_date is the primary key for this table. This table contains the days of failed tasks. Table: Succeeded +--------------+---------+ | Column Name | Type | +--------------+---------+ | success_date | date | +--------------+---------+ success_date is the primary key for this table. This table contains the days of succeeded tasks. A system is running one task every day. Every task is independent of the previous tasks. The tasks can fail or succeed. Write an SQL query to generate a report of period_state for each continuous interval of days in the period from 2019-01-01 to 2019-12-31. period_state is 'failed' if tasks in this interval failed or 'succeeded' if tasks in this interval succeeded. Interval of days are retrieved as start_date and end_date. Return the result table ordered by start_date. The query result format is in the following example. Example 1: Input: Failed table: +-------------------+ | fail_date | +-------------------+ | 2018-12-28 | | 2018-12-29 | | 2019-01-04 | | 2019-01-05 | +-------------------+ Succeeded table: +-------------------+ | success_date | +-------------------+ | 2018-12-30 | | 2018-12-31 | | 2019-01-01 | | 2019-01-02 | | 2019-01-03 | | 2019-01-06 | +-------------------+ Output: +--------------+--------------+--------------+ | period_state | start_date | end_date | +--------------+--------------+--------------+ | succeeded | 2019-01-01 | 2019-01-03 | | failed | 2019-01-04 | 2019-01-05 | | succeeded | 2019-01-06 | 2019-01-06 | +--------------+--------------+--------------+ Explanation: The report ignored the system state in 2018 as we care about the system in the period 2019-01-01 to 2019-12-31. From 2019-01-01 to 2019-01-03 all tasks succeeded and the system state was \"succeeded\". From 2019-01-04 to 2019-01-05 all tasks failed and the system state was \"failed\". From 2019-01-06 to 2019-01-06 all tasks succeeded and the system state was \"succeeded\". Ans with cte1 as ( select 'failed' as status, date_format(fail_date,'%Y-%m-%d') as mydate from Failed where fail_date between '2019-01-01' and '2019-12-31' union select 'succeeded' as status, date_format(success_date,'%Y-%m-%d') as mydate from Succeeded where success_date between '2019-01-01' and '2019-12-31' ), cte2 as (select mydate, status, row_number() over (order by mydate) - dense_rank() over (partition by status order by mydate) as grp from cte1 ) select status as period_state, min(mydate) as start_date, max(mydate) as end_date from cte2 group by grp , status order by min(mydate); User Purchase Platform Table: Spending +-------------+---------+ | Column Name | Type | +-------------+---------+ | user_id | int | | spend_date | date | | platform | enum | | amount | int | +-------------+---------+ The table logs the history of the spending of users that make purchases from an online shopping website that has a desktop and a mobile application. (user_id, spend_date, platform) is the primary key of this table. The platform column is an ENUM type of ('desktop', 'mobile'). Write an SQL query to find the total number of users and the total amount spent using the mobile only, the desktop only, and both mobile and desktop together for each date. Return the result table in any order. The query result format is in the following example. Example 1: Input: Spending table: +---------+------------+----------+--------+ | user_id | spend_date | platform | amount | +---------+------------+----------+--------+ | 1 | 2019-07-01 | mobile | 100 | | 1 | 2019-07-01 | desktop | 100 | | 2 | 2019-07-01 | mobile | 100 | | 2 | 2019-07-02 | mobile | 100 | | 3 | 2019-07-01 | desktop | 100 | | 3 | 2019-07-02 | desktop | 100 | +---------+------------+----------+--------+ Output: +------------+----------+--------------+-------------+ | spend_date | platform | total_amount | total_users | +------------+----------+--------------+-------------+ | 2019-07-01 | desktop | 100 | 1 | | 2019-07-01 | mobile | 100 | 1 | | 2019-07-01 | both | 200 | 1 | | 2019-07-02 | desktop | 100 | 1 | | 2019-07-02 | mobile | 100 | 1 | | 2019-07-02 | both | 0 | 0 | +------------+----------+--------------+-------------+ Explanation: On 2019-07-01, user 1 purchased using both desktop and mobile, user 2 purchased using mobile only and user 3 purchased using desktop only. On 2019-07-02, user 2 purchased using mobile only, user 3 purchased using desktop only and no one purchased using both platforms. Ans with cte as ( select spend_date, user_id, sum(case when platform='mobile' then amount else 0 end) as mobile_amount, sum(case when platform='desktop' then amount else 0 end) as desktop_amount from spending group by spend_date,user_id ), cte2 as ( select spend_date, user_id, case when mobile_amount > 0 and desktop_amount>0 then 'both' when mobile_amount > 0 and desktop_amount=0 then 'mobile' when mobile_amount = 0 and desktop_amount>0 then 'desktop' end as platform, (mobile_amount+desktop_amount) as total_amount from cte group by spend_date , user_id ), cte3 as ( SELECT distinct(spend_date), 'desktop' platform FROM Spending UNION SELECT distinct(spend_date), 'mobile' platform FROM Spending UNION SELECT distinct(spend_date), 'both' platform FROM Spending ) select cte3.spend_date, cte3.platform, coalesce(sum(cte2.total_amount),0) as total_amount, count(cte2.user_id) as total_users from cte3 left join cte2 on cte3.spend_date=cte2.spend_date and cte3.platform=cte2.platform group by spend_date, platform;","title":"SQL leetcode"},{"location":"SQL/docs/SQL%20leetcode/#second-highest-salary","text":"Link : https://leetcode.com/problems/second-highest-salary/ Table: Employee +-------------+------+ | Column Name | Type | +-------------+------+ | id | int | | salary | int | +-------------+------+ id is the primary key column for this table. Each row of this table contains information about the salary of an employee. Write an SQL query to report the second highest salary from the Employee table. If there is no second highest salary, the query should report null. The query result format is in the following example. Example 1: Input: Employee table: +----+--------+ | id | salary | +----+--------+ | 1 | 100 | | 2 | 200 | | 3 | 300 | +----+--------+ Output: +---------------------+ | SecondHighestSalary | +---------------------+ | 200 | +---------------------+ Example 2: Input: Employee table: +----+--------+ | id | salary | +----+--------+ | 1 | 100 | +----+--------+ Output: +---------------------+ | SecondHighestSalary | +---------------------+ | null | +---------------------+ Ans select max(salary) as SecondHighestSalary from (select salary, dense_rank() over (order by salary desc) as sal_rank from employee ) where sal_rank=2; Note -> if Nth highest salary --> Link : https://leetcode.com/problems/nth-highest-salary/ CREATE FUNCTION getNthHighestSalary(N IN NUMBER) RETURN NUMBER IS result NUMBER; BEGIN /* Write your PL/SQL query statement below */ select max(salary) into result from ( select salary, dense_rank() over (order by salary desc) as sal_rank from employee ) where sal_rank=N; RETURN result; END;","title":"Second Highest Salary"},{"location":"SQL/docs/SQL%20leetcode/#find-median-given-frequency-of-numbers","text":"Link : https://leetcode.com/problems/find-median-given-frequency-of-numbers/ Table: Numbers +-------------+------+ | Column Name | Type | +-------------+------+ | num | int | | frequency | int | +-------------+------+ num is the primary key for this table. Each row of this table shows the frequency of a number in the database. The median is the value separating the higher half from the lower half of a data sample. Write an SQL query to report the median of all the numbers in the database after decompressing the Numbers table. Round the median to one decimal point. The query result format is in the following example. Example 1: Input: Numbers table: +-----+-----------+ | num | frequency | +-----+-----------+ | 0 | 7 | | 1 | 1 | | 2 | 3 | | 3 | 1 | +-----+-----------+ Output: +--------+ | median | +--------+ | 0.0 | +--------+ Explanation: If we decompress the Numbers table, we will get [0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 3], so the median is (0 + 0) / 2 = 0. Ans select round(avg(num),1) as median from ( select num, frequency, lag(cum_sum,1,0) over (order by num) as prev_sum, cum_sum, medium_num from (select num, frequency, sum(frequency) over (order by num) as cum_sum, sum(frequency) over () / 2 as medium_num from numbers) a ) b where medium_num between prev_sum and cum_sum;","title":"Find Median Given Frequency of Numbers"},{"location":"SQL/docs/SQL%20leetcode/#find-cumulative-salary-of-an-employee","text":"Table: Employee +-------------+------+ | Column Name | Type | +-------------+------+ | id | int | | month | int | | salary | int | +-------------+------+ (id, month) is the primary key for this table. Each row in the table indicates the salary of an employee in one month during the year 2020. Write an SQL query to calculate the cumulative salary summary for every employee in a single unified table. The cumulative salary summary for an employee can be calculated as follows: - For each month that the employee worked, sum up the salaries in that month and the previous two months. This is their 3-month sum for that month. If an employee did not work for the company in previous months, their effective salary for those months is 0. - Do not include the 3-month sum for the most recent month that the employee worked for in the summary. - Do not include the 3-month sum for any month the employee did not work. - Return the result table ordered by id in ascending order. In case of a tie, order it by month in descending order. The query result format is in the following example. Example 1: Input: Employee table: +----+-------+--------+ | id | month | salary | +----+-------+--------+ | 1 | 1 | 20 | | 2 | 1 | 20 | | 1 | 2 | 30 | | 2 | 2 | 30 | | 3 | 2 | 40 | | 1 | 3 | 40 | | 3 | 3 | 60 | | 1 | 4 | 60 | | 3 | 4 | 70 | | 1 | 7 | 90 | | 1 | 8 | 90 | +----+-------+--------+ Output: +----+-------+--------+ | id | month | Salary | +----+-------+--------+ | 1 | 7 | 90 | | 1 | 4 | 130 | | 1 | 3 | 90 | | 1 | 2 | 50 | | 1 | 1 | 20 | | 2 | 1 | 20 | | 3 | 3 | 100 | | 3 | 2 | 40 | +----+-------+--------+ Explanation: Employee '1' has five salary records excluding their most recent month '8': - 90 for month '7'. - 60 for month '4'. - 40 for month '3'. - 30 for month '2'. - 20 for month '1'. So the cumulative salary summary for this employee is: +----+-------+--------+ | id | month | salary | +----+-------+--------+ | 1 | 7 | 90 | (90 + 0 + 0) | 1 | 4 | 130 | (60 + 40 + 30) | 1 | 3 | 90 | (40 + 30 + 20) | 1 | 2 | 50 | (30 + 20 + 0) | 1 | 1 | 20 | (20 + 0 + 0) +----+-------+--------+ Note that the 3-month sum for month '7' is 90 because they did not work during month '6' or month '5'. Employee '2' only has one salary record (month '1') excluding their most recent month '2'. +----+-------+--------+ | id | month | salary | +----+-------+--------+ | 2 | 1 | 20 | (20 + 0 + 0) +----+-------+--------+ Employee '3' has two salary records excluding their most recent month '4': - 60 for month '3'. - 40 for month '2'. So the cumulative salary summary for this employee is: +----+-------+--------+ | id | month | salary | +----+-------+--------+ | 3 | 3 | 100 | (60 + 40 + 0) | 3 | 2 | 40 | (40 + 0 + 0) +----+-------+--------+ Ans select id, month, sum(salary) over (partition by id order by month range between 2 PRECEDING and current row) as salary from ( select id, month, salary, max(month) over (partition by id order by month rows between unbounded preceding and unbounded following) as max_month_for_employee from employee ) where month!=max_month_for_employee order by id, month desc;","title":"Find Cumulative Salary of an Employee"},{"location":"SQL/docs/SQL%20leetcode/#average-salary-departments-vs-company","text":"Link : https://leetcode.com/problems/average-salary-departments-vs-company/ Table: Salary +-------------+------+ | Column Name | Type | +-------------+------+ | id | int | | employee_id | int | | amount | int | | pay_date | date | +-------------+------+ id is the primary key column for this table. Each row of this table indicates the salary of an employee in one month. employee_id is a foreign key from the Employee table. Table: Employee +---------------+------+ | Column Name | Type | +---------------+------+ | employee_id | int | | department_id | int | +---------------+------+ employee_id is the primary key column for this table. Each row of this table indicates the department of an employee. Write an SQL query to report the comparison result (higher/lower/same) of the average salary of employees in a department to the company's average salary. Return the result table in any order. The query result format is in the following example. Example 1: Input: Salary table: +----+-------------+--------+------------+ | id | employee_id | amount | pay_date | +----+-------------+--------+------------+ | 1 | 1 | 9000 | 2017/03/31 | | 2 | 2 | 6000 | 2017/03/31 | | 3 | 3 | 10000 | 2017/03/31 | | 4 | 1 | 7000 | 2017/02/28 | | 5 | 2 | 6000 | 2017/02/28 | | 6 | 3 | 8000 | 2017/02/28 | +----+-------------+--------+------------+ Employee table: +-------------+---------------+ | employee_id | department_id | +-------------+---------------+ | 1 | 1 | | 2 | 2 | | 3 | 2 | +-------------+---------------+ Output: +-----------+---------------+------------+ | pay_month | department_id | comparison | +-----------+---------------+------------+ | 2017-02 | 1 | same | | 2017-03 | 1 | higher | | 2017-02 | 2 | same | | 2017-03 | 2 | lower | +-----------+---------------+------------+ Explanation: In March, the company's average salary is (9000+6000+10000)/3 = 8333.33... The average salary for department '1' is 9000, which is the salary of employee_id '1' since there is only one employee in this department. So the comparison result is 'higher' since 9000 > 8333.33 obviously. The average salary of department '2' is (6000 + 10000)/2 = 8000, which is the average of employee_id '2' and '3'. So the comparison result is 'lower' since 8000 < 8333.33. With he same formula for the average salary comparison in February, the result is 'same' since both the department '1' and '2' have the same average salary with the company, which is 7000. Ans select distinct pay_month, department_id, case when avg_dept<avg_company then 'lower' when avg_dept=avg_company then 'same' when avg_dept>avg_company then 'higher' end as comparison from ( select department_id, date_format(pay_date, '%Y-%m') as pay_month, avg(employee_salary) over (partition by pay_date ) as avg_company, avg(employee_salary) over (partition by pay_date, department_id ) as avg_dept from (select s.employee_id, s.amount as employee_salary, s.pay_date, e.department_id from salary s inner join employee e on s.employee_id = e.employee_id) a ) b","title":"Average Salary: Departments VS Company"},{"location":"SQL/docs/SQL%20leetcode/#sales-by-day-of-the-week","text":"https://leetcode.com/problems/sales-by-day-of-the-week/ Table: Orders +---------------+---------+ | Column Name | Type | +---------------+---------+ | order_id | int | | customer_id | int | | order_date | date | | item_id | varchar | | quantity | int | +---------------+---------+ (ordered_id, item_id) is the primary key for this table. This table contains information on the orders placed. order_date is the date item_id was ordered by the customer with id customer_id. Table: Items +---------------------+---------+ | Column Name | Type | +---------------------+---------+ | item_id | varchar | | item_name | varchar | | item_category | varchar | +---------------------+---------+ item_id is the primary key for this table. item_name is the name of the item. item_category is the category of the item. You are the business owner and would like to obtain a sales report for category items and the day of the week. Write an SQL query to report how many units in each category have been ordered on each day of the week. Return the result table ordered by category. The query result format is in the following example. Example 1: Input: Orders table: +------------+--------------+-------------+--------------+-------------+ | order_id | customer_id | order_date | item_id | quantity | +------------+--------------+-------------+--------------+-------------+ | 1 | 1 | 2020-06-01 | 1 | 10 | | 2 | 1 | 2020-06-08 | 2 | 10 | | 3 | 2 | 2020-06-02 | 1 | 5 | | 4 | 3 | 2020-06-03 | 3 | 5 | | 5 | 4 | 2020-06-04 | 4 | 1 | | 6 | 4 | 2020-06-05 | 5 | 5 | | 7 | 5 | 2020-06-05 | 1 | 10 | | 8 | 5 | 2020-06-14 | 4 | 5 | | 9 | 5 | 2020-06-21 | 3 | 5 | +------------+--------------+-------------+--------------+-------------+ Items table: +------------+----------------+---------------+ | item_id | item_name | item_category | +------------+----------------+---------------+ | 1 | LC Alg. Book | Book | | 2 | LC DB. Book | Book | | 3 | LC SmarthPhone | Phone | | 4 | LC Phone 2020 | Phone | | 5 | LC SmartGlass | Glasses | | 6 | LC T-Shirt XL | T-Shirt | +------------+----------------+---------------+ Output: +------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+ | Category | Monday | Tuesday | Wednesday | Thursday | Friday | Saturday | Sunday | +------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+ | Book | 20 | 5 | 0 | 0 | 10 | 0 | 0 | | Glasses | 0 | 0 | 0 | 0 | 5 | 0 | 0 | | Phone | 0 | 0 | 5 | 1 | 0 | 0 | 10 | | T-Shirt | 0 | 0 | 0 | 0 | 0 | 0 | 0 | +------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+ Explanation: On Monday (2020-06-01, 2020-06-08) were sold a total of 20 units (10 + 10) in the category Book (ids: 1, 2). On Tuesday (2020-06-02) were sold a total of 5 units in the category Book (ids: 1, 2). On Wednesday (2020-06-03) were sold a total of 5 units in the category Phone (ids: 3, 4). On Thursday (2020-06-04) were sold a total of 1 unit in the category Phone (ids: 3, 4). On Friday (2020-06-05) were sold 10 units in the category Book (ids: 1, 2) and 5 units in Glasses (ids: 5). On Saturday there are no items sold. On Sunday (2020-06-14, 2020-06-21) were sold a total of 10 units (5 +5) in the category Phone (ids: 3, 4). There are no sales of T-shirts. Ans with cte_table as (select b.item_category, dayname(a.order_date) as day_of_week, COALESCE(sum(a.quantity),0) as qty from orders a RIGHT join items b on a.item_id = b.item_id group by b.item_category, dayname(a.order_date) ) select item_category as \"CATEGORY\", sum(case when upper(day_of_week)='MONDAY' then qty else 0 end) as 'MONDAY', sum(case when upper(day_of_week)='TUESDAY' then qty else 0 end) as 'TUESDAY', sum(case when upper(day_of_week)='WEDNESDAY' then qty else 0 end) as 'WEDNESDAY', sum(case when upper(day_of_week)='THURSDAY' then qty else 0 end) as 'THURSDAY', sum(case when upper(day_of_week)='FRIDAY' then qty else 0 end) as 'FRIDAY', sum(case when upper(day_of_week)='SATURDAY' then qty else 0 end) as 'SATURDAY', sum(case when upper(day_of_week)='SUNDAY' then qty else 0 end) as 'SUNDAY' from cte_table group by item_category order by item_category;","title":"Sales by Day of the Week"},{"location":"SQL/docs/SQL%20leetcode/#report-contiguous-dates","text":"https://leetcode.com/problems/report-contiguous-dates/ Table: Failed +--------------+---------+ | Column Name | Type | +--------------+---------+ | fail_date | date | +--------------+---------+ fail_date is the primary key for this table. This table contains the days of failed tasks. Table: Succeeded +--------------+---------+ | Column Name | Type | +--------------+---------+ | success_date | date | +--------------+---------+ success_date is the primary key for this table. This table contains the days of succeeded tasks. A system is running one task every day. Every task is independent of the previous tasks. The tasks can fail or succeed. Write an SQL query to generate a report of period_state for each continuous interval of days in the period from 2019-01-01 to 2019-12-31. period_state is 'failed' if tasks in this interval failed or 'succeeded' if tasks in this interval succeeded. Interval of days are retrieved as start_date and end_date. Return the result table ordered by start_date. The query result format is in the following example. Example 1: Input: Failed table: +-------------------+ | fail_date | +-------------------+ | 2018-12-28 | | 2018-12-29 | | 2019-01-04 | | 2019-01-05 | +-------------------+ Succeeded table: +-------------------+ | success_date | +-------------------+ | 2018-12-30 | | 2018-12-31 | | 2019-01-01 | | 2019-01-02 | | 2019-01-03 | | 2019-01-06 | +-------------------+ Output: +--------------+--------------+--------------+ | period_state | start_date | end_date | +--------------+--------------+--------------+ | succeeded | 2019-01-01 | 2019-01-03 | | failed | 2019-01-04 | 2019-01-05 | | succeeded | 2019-01-06 | 2019-01-06 | +--------------+--------------+--------------+ Explanation: The report ignored the system state in 2018 as we care about the system in the period 2019-01-01 to 2019-12-31. From 2019-01-01 to 2019-01-03 all tasks succeeded and the system state was \"succeeded\". From 2019-01-04 to 2019-01-05 all tasks failed and the system state was \"failed\". From 2019-01-06 to 2019-01-06 all tasks succeeded and the system state was \"succeeded\". Ans with cte1 as ( select 'failed' as status, date_format(fail_date,'%Y-%m-%d') as mydate from Failed where fail_date between '2019-01-01' and '2019-12-31' union select 'succeeded' as status, date_format(success_date,'%Y-%m-%d') as mydate from Succeeded where success_date between '2019-01-01' and '2019-12-31' ), cte2 as (select mydate, status, row_number() over (order by mydate) - dense_rank() over (partition by status order by mydate) as grp from cte1 ) select status as period_state, min(mydate) as start_date, max(mydate) as end_date from cte2 group by grp , status order by min(mydate);","title":"Report Contiguous Dates"},{"location":"SQL/docs/SQL%20leetcode/#user-purchase-platform","text":"Table: Spending +-------------+---------+ | Column Name | Type | +-------------+---------+ | user_id | int | | spend_date | date | | platform | enum | | amount | int | +-------------+---------+ The table logs the history of the spending of users that make purchases from an online shopping website that has a desktop and a mobile application. (user_id, spend_date, platform) is the primary key of this table. The platform column is an ENUM type of ('desktop', 'mobile'). Write an SQL query to find the total number of users and the total amount spent using the mobile only, the desktop only, and both mobile and desktop together for each date. Return the result table in any order. The query result format is in the following example. Example 1: Input: Spending table: +---------+------------+----------+--------+ | user_id | spend_date | platform | amount | +---------+------------+----------+--------+ | 1 | 2019-07-01 | mobile | 100 | | 1 | 2019-07-01 | desktop | 100 | | 2 | 2019-07-01 | mobile | 100 | | 2 | 2019-07-02 | mobile | 100 | | 3 | 2019-07-01 | desktop | 100 | | 3 | 2019-07-02 | desktop | 100 | +---------+------------+----------+--------+ Output: +------------+----------+--------------+-------------+ | spend_date | platform | total_amount | total_users | +------------+----------+--------------+-------------+ | 2019-07-01 | desktop | 100 | 1 | | 2019-07-01 | mobile | 100 | 1 | | 2019-07-01 | both | 200 | 1 | | 2019-07-02 | desktop | 100 | 1 | | 2019-07-02 | mobile | 100 | 1 | | 2019-07-02 | both | 0 | 0 | +------------+----------+--------------+-------------+ Explanation: On 2019-07-01, user 1 purchased using both desktop and mobile, user 2 purchased using mobile only and user 3 purchased using desktop only. On 2019-07-02, user 2 purchased using mobile only, user 3 purchased using desktop only and no one purchased using both platforms. Ans with cte as ( select spend_date, user_id, sum(case when platform='mobile' then amount else 0 end) as mobile_amount, sum(case when platform='desktop' then amount else 0 end) as desktop_amount from spending group by spend_date,user_id ), cte2 as ( select spend_date, user_id, case when mobile_amount > 0 and desktop_amount>0 then 'both' when mobile_amount > 0 and desktop_amount=0 then 'mobile' when mobile_amount = 0 and desktop_amount>0 then 'desktop' end as platform, (mobile_amount+desktop_amount) as total_amount from cte group by spend_date , user_id ), cte3 as ( SELECT distinct(spend_date), 'desktop' platform FROM Spending UNION SELECT distinct(spend_date), 'mobile' platform FROM Spending UNION SELECT distinct(spend_date), 'both' platform FROM Spending ) select cte3.spend_date, cte3.platform, coalesce(sum(cte2.total_amount),0) as total_amount, count(cte2.user_id) as total_users from cte3 left join cte2 on cte3.spend_date=cte2.spend_date and cte3.platform=cte2.platform group by spend_date, platform;","title":"User Purchase Platform"},{"location":"SQL/docs/sql-analytical-functions/","text":"SQL Analytical Functions Source over (partition by / order by ) eg - SELECT o.region_id region_id, o.cust_nbr cust_nbr, SUM(o.tot_sales) tot_sales, SUM(SUM(o.tot_sales)) OVER (PARTITION BY o.region_id) region_sales FROM orders o WHERE o.year = 2001 GROUP BY o.region_id, o.cust_nbr; Ranking Functions source ROW_NUMBER, RANK and DENSE_RANK ROW_NUMBER - Returns a unique number for each row starting with 1. For rows that have duplicate values, numbers are arbitrarily assigned. DENSE_RANK - Assigns a unique number for each row starting with 1, except for rows that have duplicate values, in which case the same ranking is assigned. RANK - Assigns a unique number for each row starting with 1, except for rows that have duplicate values, in which case the same ranking is assigned and a gap appears in the sequence for each duplicate ranking. eg 1 - SELECT region_id, cust_nbr, SUM(tot_sales) cust_sales, RANK( ) OVER (ORDER BY SUM(tot_sales) DESC) sales_rank, DENSE_RANK( ) OVER (ORDER BY SUM(tot_sales) DESC) sales_dense_rank, ROW_NUMBER( ) OVER (ORDER BY SUM(tot_sales) DESC) sales_number FROM orders WHERE year = 2001 GROUP BY region_id, cust_nbr ORDER BY sales_number; REGION_ID CUST_NBR CUST_SALES SALES_RANK SALES_DENSE_RANK SALES_NUMBER ---------- ---------- ---------- ---------- ---------------- ------------ 8 18 1253840 11 11 11 5 2 1224992 12 12 12 9 23 1224992 12 12 13 9 24 1224992 12 12 14 10 30 1216858 15 13 15 eg 2 - The following query generates rankings for customer sales within each region rather than across all regions. Note the addition of the PARTITION BY clause: SELECT region_id, cust_nbr, SUM(tot_sales) cust_sales, RANK( ) OVER (PARTITION BY region_id ORDER BY SUM(tot_sales) DESC) sales_rank, DENSE_RANK( ) OVER (PARTITION BY region_id ORDER BY SUM(tot_sales) DESC) sales_dense_rank, ROW_NUMBER( ) OVER (PARTITION BY region_id ORDER BY SUM(tot_sales) DESC) sales_number FROM orders WHERE year = 2001 GROUP BY region_id, cust_nbr ORDER BY region_id, sales_number; REGION_ID CUST_NBR CUST_SALES SALES_RANK SALES_DENSE_RANK SALES_NUMBER ---------- ---------- ---------- ---------- ---------------- ------------ 5 4 1878275 1 1 1 5 2 1224992 2 2 2 5 5 1169926 3 3 3 5 3 1161286 4 4 4 5 1 1151162 5 5 5 6 6 1788836 1 1 1 6 9 1208959 2 2 2 Handling NULLs All ranking functions allow you to specify where in the ranking order NULL values should appear. This is accomplished by appending either NULLS FIRST or NULLS LAST after the ORDER BY clause of the function, as in: SELECT region_id, cust_nbr, SUM(tot_sales) cust_sales, RANK( ) OVER (ORDER BY SUM(tot_sales) DESC NULLS LAST) sales_rank FROM orders WHERE year = 2001 GROUP BY region_id, cust_nbr; NTILE Another way rankings are commonly used is to generate buckets into which sets of rankings are grouped. For example, you may want to find those customers whose total sales ranked in the top 25%. The following query uses the NTILE function to group the customers into four buckets (or quartiles): SELECT region_id, cust_nbr, SUM(tot_sales) cust_sales, NTILE(4) OVER (ORDER BY SUM(tot_sales) DESC) sales_quartile FROM orders WHERE year = 2001 GROUP BY region_id, cust_nbr ORDER BY sales_quartile, cust_sales DESC; REGION_ID CUST_NBR CUST_SALES SALES_QUARTILE ---------- ---------- ---------- -------------- 9 25 2232703 1 8 17 1944281 1 7 14 1929774 1 CUME_DIST and PERCENT_RANK CUME_DIST function (short for Cumulative Distribution) calculates the ratio of the number of rows that have a lesser or equal ranking to the total number of rows in the partition. PERCENT_RANK function calculates the ratio of a row's ranking to the number of rows in the partition using the formula: (RRP -- 1) / (NRP -- 1) where RRP is the \"rank of row in partition,\" and NRP is the \"number of rows in partition.\" Windowing Functions ROWS BETWEEN <> AND <> Some of the sample values can be - ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING eg - SELECT month, SUM(tot_sales) monthly_sales, SUM(SUM(tot_sales)) OVER ( ORDER BY month ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING ) total_sales FROM orders WHERE year = 2001 AND region_id = 6 GROUP BY month ORDER BY month; MONTH MONTHLY_SALES TOTAL_SALES 1 610697 6307766 2 428676 6307766 3 637031 6307766 4 541146 6307766 ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW eg 1 - max current value- SELECT month, SUM(tot_sales) monthly_sales, MAX(SUM(tot_sales)) OVER (ORDER BY month ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) max_preceeding FROM orders WHERE year = 2001 AND region_id = 6 GROUP BY month ORDER BY month; MONTH MONTHLY_SALES MAX_PRECEEDING 1 610697 610697 2 428676 610697 3 637031 637031 4 541146 637031 5 592935 637031 6 501485 637031 eg 2 - cumulative SUM - SELECT month, SUM(tot_sales) monthly_sales, SUM(SUM(tot_sales)) OVER (ORDER BY month ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) max_preceeding FROM orders WHERE year = 2001 AND region_id = 6 GROUP BY month ORDER BY month; MONTH MONTHLY_SALES RUNNING_TOTAL 1 610697 610697 2 428676 1039373 3 637031 1676404 4 541146 2217550 5 592935 2810485 ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING - eg -calculate avg of rolling 3 values (current row, rev row and next row); SELECT month, SUM(tot_sales) monthly_sales, AVG(SUM(tot_sales)) OVER (ORDER BY month ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING) rolling_avg FROM orders WHERE year = 2001 AND region_id = 6 GROUP BY month ORDER BY month; MONTH MONTHLY_SALES ROLLING_AVG 1 610697 519686.5 2 428676 558801.333 3 637031 535617.667 4 541146 590370.667 5 592935 545188.667 RANGE BETWEEN ROWS BETWEEN - based on row ordered as per condition of ordering inside over() clause. RANGE BETWEEN - based on value ranges specified under over() clause. eg1 - to generate a three-month rolling average (similar to above ROWS BETWEEN question). In our table, month is numeric integer value, and so RANGE works perfectly fine here. This substitution works because the month column contains integer values, so adding and subtracting 1 from the current month yields a three-month range. But if its character, then below wont be suited. SELECT month, SUM(tot_sales) monthly_sales, AVG(SUM(tot_sales)) OVER (ORDER BY month RANGE BETWEEN 1 PRECEDING AND 1 FOLLOWING) rolling_avg FROM orders WHERE year = 2001 AND region_id = 6 GROUP BY month ORDER BY month; ---------- ------------- ----------- MONTH MONTHLY_SALES ROLLING_AVG ---------- ------------- ----------- 1 610697 519686.5 2 428676 558801.333 3 637031 535617.667 eg2 - if we do a range of +/- 1.999, then also we get same values: SELECT month, SUM(tot_sales) monthly_sales, AVG(SUM(tot_sales)) OVER (ORDER BY month RANGE BETWEEN 1.99 PRECEDING AND 1.99 FOLLOWING) rolling_avg FROM orders WHERE year = 2001 AND region_id = 6 GROUP BY month ORDER BY month; ---------- ------------- ----------- MONTH MONTHLY_SALES ROLLING_AVG ---------- ------------- ----------- 1 610697 519686.5 2 428676 558801.333 3 637031 535617.667 eg3 - working with DATE Range . ROWS wont be of much use if we are working on Date Range. If you are generating a window based on a DATE column, you can specify a range in increments of days, months, or years. Since the orders table has no DATE columns, the next example shows how a date range can be specified against the order_dt column of the cust_order table: SELECT TRUNC(order_dt) day, SUM(sale_price) daily_sales, AVG(SUM(sale_price)) OVER ( ORDER BY TRUNC(order_dt) RANGE BETWEEN INTERVAL '2' DAY PRECEDING AND INTERVAL '2' DAY FOLLOWING ) five_day_avg FROM cust_order WHERE sale_price IS NOT NULL AND order_dt BETWEEN TO_DATE('01-JUL-2001','DD-MON-YYYY') AND TO_DATE('31-JUL-2001','DD-MON-YYYY') GROUP BY TRUNC(order_dt); --------- ----------- ------------ DAY DAILY_SALES FIVE_DAY_AVG --------- ----------- ------------ 16-JUL-01 112 146 18-JUL-01 180 114 20-JUL-01 50 169 21-JUL-01 50 165.333333 22-JUL-01 396 165.333333 FIRST_VALUE and LAST_VALUE They are used with windowing functions to identify the values of the first and last values in the window. sample que :\"How did each month's sales compare to the first month?\" eg - In the case of the three-month rolling average query shown previously, you could display the values of all three months along with the average of the three, as in: SELECT month, FIRST_VALUE(SUM(tot_sales)) OVER (ORDER BY month ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING) prev_month, SUM(tot_sales) monthly_sales, LAST_VALUE(SUM(tot_sales)) OVER (ORDER BY month ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING) next_month, AVG(SUM(tot_sales)) OVER (ORDER BY month ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING) rolling_avg FROM orders WHERE year = 2001 AND region_id = 6 GROUP BY month ORDER BY month; MONTH PREV_MONTH MONTHLY_SALES NEXT_MONTH ROLLING_AVG 1 610697 610697 428676 519686.5 2 610697 428676 637031 558801.333 3 428676 637031 541146 535617.667 4 637031 541146 592935 590370.667 5 541146 592935 501485 545188.667 LAG/LEAD Functions \"LAG(VAL, N, <default value>) OVER () \" -- N=1 by default. \"LEAD(VAL, N, <default value>) OVER () \" -- N=1 by default. Query - \"Compute the total sales per month for the Mid-Atlantic region, including the percent change from the previous month\" requires data from both the current and preceding rows to calculate the answer. Step 1 - get prev month's data using LAG function. SELECT month, SUM(tot_sales) monthly_sales, LAG(SUM(tot_sales), 1) OVER (ORDER BY month) prev_month_sales FROM orders WHERE year = 2001 AND region_id = 6 GROUP BY month ORDER BY month; ---------- ------------- ---------------- MONTH MONTHLY_SALES PREV_MONTH_SALES ---------- ------------- ---------------- 1 610697 2 428676 610697 3 637031 428676 Step 2 - handle NULL values. If you see above, for month 1, PREV_MONTH_SALES is NULL. So, can calculate % change in sales. Here we keep current month sales as default value, and % sales in this case is 0%. SELECT months.month month, months.monthly_sales monthly_sales, ROUND((months.monthly_sales - months.prev_month_sales)/ months.prev_month_sales, 3) * 100 percent_change FROM ( SELECT month, SUM(tot_sales) monthly_sales, LAG(SUM(tot_sales), 1, SUM(tot_sales)) OVER (ORDER BY month) prev_month_sales FROM orders WHERE year = 2001 AND region_id = 6 GROUP BY month) months ORDER BY month; ---------- ------------- -------------- MONTH MONTHLY_SALES PERCENT_CHANGE ---------- ------------- -------------- 1 610697 0 2 428676 -29.8 3 637031 48.6","title":"SQL Analytical Functions"},{"location":"SQL/docs/sql-analytical-functions/#sql-analytical-functions","text":"Source","title":"SQL Analytical Functions"},{"location":"SQL/docs/sql-analytical-functions/#over-partition-by-order-by","text":"eg - SELECT o.region_id region_id, o.cust_nbr cust_nbr, SUM(o.tot_sales) tot_sales, SUM(SUM(o.tot_sales)) OVER (PARTITION BY o.region_id) region_sales FROM orders o WHERE o.year = 2001 GROUP BY o.region_id, o.cust_nbr;","title":"over (partition by / order by )"},{"location":"SQL/docs/sql-analytical-functions/#ranking-functions","text":"source","title":"Ranking Functions"},{"location":"SQL/docs/sql-analytical-functions/#row_number-rank-and-dense_rank","text":"ROW_NUMBER - Returns a unique number for each row starting with 1. For rows that have duplicate values, numbers are arbitrarily assigned. DENSE_RANK - Assigns a unique number for each row starting with 1, except for rows that have duplicate values, in which case the same ranking is assigned. RANK - Assigns a unique number for each row starting with 1, except for rows that have duplicate values, in which case the same ranking is assigned and a gap appears in the sequence for each duplicate ranking. eg 1 - SELECT region_id, cust_nbr, SUM(tot_sales) cust_sales, RANK( ) OVER (ORDER BY SUM(tot_sales) DESC) sales_rank, DENSE_RANK( ) OVER (ORDER BY SUM(tot_sales) DESC) sales_dense_rank, ROW_NUMBER( ) OVER (ORDER BY SUM(tot_sales) DESC) sales_number FROM orders WHERE year = 2001 GROUP BY region_id, cust_nbr ORDER BY sales_number; REGION_ID CUST_NBR CUST_SALES SALES_RANK SALES_DENSE_RANK SALES_NUMBER ---------- ---------- ---------- ---------- ---------------- ------------ 8 18 1253840 11 11 11 5 2 1224992 12 12 12 9 23 1224992 12 12 13 9 24 1224992 12 12 14 10 30 1216858 15 13 15 eg 2 - The following query generates rankings for customer sales within each region rather than across all regions. Note the addition of the PARTITION BY clause: SELECT region_id, cust_nbr, SUM(tot_sales) cust_sales, RANK( ) OVER (PARTITION BY region_id ORDER BY SUM(tot_sales) DESC) sales_rank, DENSE_RANK( ) OVER (PARTITION BY region_id ORDER BY SUM(tot_sales) DESC) sales_dense_rank, ROW_NUMBER( ) OVER (PARTITION BY region_id ORDER BY SUM(tot_sales) DESC) sales_number FROM orders WHERE year = 2001 GROUP BY region_id, cust_nbr ORDER BY region_id, sales_number; REGION_ID CUST_NBR CUST_SALES SALES_RANK SALES_DENSE_RANK SALES_NUMBER ---------- ---------- ---------- ---------- ---------------- ------------ 5 4 1878275 1 1 1 5 2 1224992 2 2 2 5 5 1169926 3 3 3 5 3 1161286 4 4 4 5 1 1151162 5 5 5 6 6 1788836 1 1 1 6 9 1208959 2 2 2","title":"ROW_NUMBER, RANK and DENSE_RANK"},{"location":"SQL/docs/sql-analytical-functions/#handling-nulls","text":"All ranking functions allow you to specify where in the ranking order NULL values should appear. This is accomplished by appending either NULLS FIRST or NULLS LAST after the ORDER BY clause of the function, as in: SELECT region_id, cust_nbr, SUM(tot_sales) cust_sales, RANK( ) OVER (ORDER BY SUM(tot_sales) DESC NULLS LAST) sales_rank FROM orders WHERE year = 2001 GROUP BY region_id, cust_nbr;","title":"Handling NULLs"},{"location":"SQL/docs/sql-analytical-functions/#ntile","text":"Another way rankings are commonly used is to generate buckets into which sets of rankings are grouped. For example, you may want to find those customers whose total sales ranked in the top 25%. The following query uses the NTILE function to group the customers into four buckets (or quartiles): SELECT region_id, cust_nbr, SUM(tot_sales) cust_sales, NTILE(4) OVER (ORDER BY SUM(tot_sales) DESC) sales_quartile FROM orders WHERE year = 2001 GROUP BY region_id, cust_nbr ORDER BY sales_quartile, cust_sales DESC; REGION_ID CUST_NBR CUST_SALES SALES_QUARTILE ---------- ---------- ---------- -------------- 9 25 2232703 1 8 17 1944281 1 7 14 1929774 1","title":"NTILE"},{"location":"SQL/docs/sql-analytical-functions/#cume_dist-and-percent_rank","text":"CUME_DIST function (short for Cumulative Distribution) calculates the ratio of the number of rows that have a lesser or equal ranking to the total number of rows in the partition. PERCENT_RANK function calculates the ratio of a row's ranking to the number of rows in the partition using the formula: (RRP -- 1) / (NRP -- 1) where RRP is the \"rank of row in partition,\" and NRP is the \"number of rows in partition.\"","title":"CUME_DIST and PERCENT_RANK"},{"location":"SQL/docs/sql-analytical-functions/#windowing-functions","text":"","title":"Windowing Functions"},{"location":"SQL/docs/sql-analytical-functions/#rows-between-and","text":"Some of the sample values can be -","title":"ROWS BETWEEN &lt;&gt; AND &lt;&gt;"},{"location":"SQL/docs/sql-analytical-functions/#rows-between-unbounded-preceding-and-unbounded-following","text":"eg - SELECT month, SUM(tot_sales) monthly_sales, SUM(SUM(tot_sales)) OVER ( ORDER BY month ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING ) total_sales FROM orders WHERE year = 2001 AND region_id = 6 GROUP BY month ORDER BY month; MONTH MONTHLY_SALES TOTAL_SALES 1 610697 6307766 2 428676 6307766 3 637031 6307766 4 541146 6307766","title":"ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING"},{"location":"SQL/docs/sql-analytical-functions/#rows-between-unbounded-preceding-and-current-row","text":"eg 1 - max current value- SELECT month, SUM(tot_sales) monthly_sales, MAX(SUM(tot_sales)) OVER (ORDER BY month ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) max_preceeding FROM orders WHERE year = 2001 AND region_id = 6 GROUP BY month ORDER BY month; MONTH MONTHLY_SALES MAX_PRECEEDING 1 610697 610697 2 428676 610697 3 637031 637031 4 541146 637031 5 592935 637031 6 501485 637031 eg 2 - cumulative SUM - SELECT month, SUM(tot_sales) monthly_sales, SUM(SUM(tot_sales)) OVER (ORDER BY month ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) max_preceeding FROM orders WHERE year = 2001 AND region_id = 6 GROUP BY month ORDER BY month; MONTH MONTHLY_SALES RUNNING_TOTAL 1 610697 610697 2 428676 1039373 3 637031 1676404 4 541146 2217550 5 592935 2810485","title":"ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW"},{"location":"SQL/docs/sql-analytical-functions/#rows-between-1-preceding-and-1-following-","text":"eg -calculate avg of rolling 3 values (current row, rev row and next row); SELECT month, SUM(tot_sales) monthly_sales, AVG(SUM(tot_sales)) OVER (ORDER BY month ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING) rolling_avg FROM orders WHERE year = 2001 AND region_id = 6 GROUP BY month ORDER BY month; MONTH MONTHLY_SALES ROLLING_AVG 1 610697 519686.5 2 428676 558801.333 3 637031 535617.667 4 541146 590370.667 5 592935 545188.667","title":"ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING -"},{"location":"SQL/docs/sql-analytical-functions/#range-between","text":"ROWS BETWEEN - based on row ordered as per condition of ordering inside over() clause. RANGE BETWEEN - based on value ranges specified under over() clause. eg1 - to generate a three-month rolling average (similar to above ROWS BETWEEN question). In our table, month is numeric integer value, and so RANGE works perfectly fine here. This substitution works because the month column contains integer values, so adding and subtracting 1 from the current month yields a three-month range. But if its character, then below wont be suited. SELECT month, SUM(tot_sales) monthly_sales, AVG(SUM(tot_sales)) OVER (ORDER BY month RANGE BETWEEN 1 PRECEDING AND 1 FOLLOWING) rolling_avg FROM orders WHERE year = 2001 AND region_id = 6 GROUP BY month ORDER BY month; ---------- ------------- ----------- MONTH MONTHLY_SALES ROLLING_AVG ---------- ------------- ----------- 1 610697 519686.5 2 428676 558801.333 3 637031 535617.667 eg2 - if we do a range of +/- 1.999, then also we get same values: SELECT month, SUM(tot_sales) monthly_sales, AVG(SUM(tot_sales)) OVER (ORDER BY month RANGE BETWEEN 1.99 PRECEDING AND 1.99 FOLLOWING) rolling_avg FROM orders WHERE year = 2001 AND region_id = 6 GROUP BY month ORDER BY month; ---------- ------------- ----------- MONTH MONTHLY_SALES ROLLING_AVG ---------- ------------- ----------- 1 610697 519686.5 2 428676 558801.333 3 637031 535617.667 eg3 - working with DATE Range . ROWS wont be of much use if we are working on Date Range. If you are generating a window based on a DATE column, you can specify a range in increments of days, months, or years. Since the orders table has no DATE columns, the next example shows how a date range can be specified against the order_dt column of the cust_order table: SELECT TRUNC(order_dt) day, SUM(sale_price) daily_sales, AVG(SUM(sale_price)) OVER ( ORDER BY TRUNC(order_dt) RANGE BETWEEN INTERVAL '2' DAY PRECEDING AND INTERVAL '2' DAY FOLLOWING ) five_day_avg FROM cust_order WHERE sale_price IS NOT NULL AND order_dt BETWEEN TO_DATE('01-JUL-2001','DD-MON-YYYY') AND TO_DATE('31-JUL-2001','DD-MON-YYYY') GROUP BY TRUNC(order_dt); --------- ----------- ------------ DAY DAILY_SALES FIVE_DAY_AVG --------- ----------- ------------ 16-JUL-01 112 146 18-JUL-01 180 114 20-JUL-01 50 169 21-JUL-01 50 165.333333 22-JUL-01 396 165.333333","title":"RANGE BETWEEN"},{"location":"SQL/docs/sql-analytical-functions/#first_value-and-last_value","text":"They are used with windowing functions to identify the values of the first and last values in the window. sample que :\"How did each month's sales compare to the first month?\" eg - In the case of the three-month rolling average query shown previously, you could display the values of all three months along with the average of the three, as in: SELECT month, FIRST_VALUE(SUM(tot_sales)) OVER (ORDER BY month ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING) prev_month, SUM(tot_sales) monthly_sales, LAST_VALUE(SUM(tot_sales)) OVER (ORDER BY month ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING) next_month, AVG(SUM(tot_sales)) OVER (ORDER BY month ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING) rolling_avg FROM orders WHERE year = 2001 AND region_id = 6 GROUP BY month ORDER BY month; MONTH PREV_MONTH MONTHLY_SALES NEXT_MONTH ROLLING_AVG 1 610697 610697 428676 519686.5 2 610697 428676 637031 558801.333 3 428676 637031 541146 535617.667 4 637031 541146 592935 590370.667 5 541146 592935 501485 545188.667","title":"FIRST_VALUE and LAST_VALUE"},{"location":"SQL/docs/sql-analytical-functions/#laglead-functions","text":"\"LAG(VAL, N, <default value>) OVER () \" -- N=1 by default. \"LEAD(VAL, N, <default value>) OVER () \" -- N=1 by default. Query - \"Compute the total sales per month for the Mid-Atlantic region, including the percent change from the previous month\" requires data from both the current and preceding rows to calculate the answer. Step 1 - get prev month's data using LAG function. SELECT month, SUM(tot_sales) monthly_sales, LAG(SUM(tot_sales), 1) OVER (ORDER BY month) prev_month_sales FROM orders WHERE year = 2001 AND region_id = 6 GROUP BY month ORDER BY month; ---------- ------------- ---------------- MONTH MONTHLY_SALES PREV_MONTH_SALES ---------- ------------- ---------------- 1 610697 2 428676 610697 3 637031 428676 Step 2 - handle NULL values. If you see above, for month 1, PREV_MONTH_SALES is NULL. So, can calculate % change in sales. Here we keep current month sales as default value, and % sales in this case is 0%. SELECT months.month month, months.monthly_sales monthly_sales, ROUND((months.monthly_sales - months.prev_month_sales)/ months.prev_month_sales, 3) * 100 percent_change FROM ( SELECT month, SUM(tot_sales) monthly_sales, LAG(SUM(tot_sales), 1, SUM(tot_sales)) OVER (ORDER BY month) prev_month_sales FROM orders WHERE year = 2001 AND region_id = 6 GROUP BY month) months ORDER BY month; ---------- ------------- -------------- MONTH MONTHLY_SALES PERCENT_CHANGE ---------- ------------- -------------- 1 610697 0 2 428676 -29.8 3 637031 48.6","title":"LAG/LEAD Functions"},{"location":"SQL/docs/sql-practise-questions/","text":"Leetcode SQL The Most Recent Three Orders Table: Customers +---------------+---------+ | Column Name | Type | +---------------+---------+ | customer_id | int | | name | varchar | +---------------+---------+ customer_id is the primary key for this table. This table contains information about customers. Table: Orders +---------------+---------+ | Column Name | Type | +---------------+---------+ | order_id | int | | order_date | date | | customer_id | int | | cost | int | +---------------+---------+ order_id is the primary key for this table. This table contains information about the orders made customer_id. Each customer has one order per day. Write an SQL query to find the most recent 3 orders of each user. If a user ordered less than 3 orders return all of their orders. Return the result table sorted by customer_name in ascending order and in case of a tie by the customer_id in ascending order. If there still a tie, order them by the order_date in descending order. The query result format is in the following example: Customers +-------------+-----------+ | customer_id | name | +-------------+-----------+ | 1 | Winston | | 2 | Jonathan | | 3 | Annabelle | | 4 | Marwan | | 5 | Khaled | +-------------+-----------+ Orders +----------+------------+-------------+------+ | order_id | order_date | customer_id | cost | +----------+------------+-------------+------+ | 1 | 2020-07-31 | 1 | 30 | | 2 | 2020-07-30 | 2 | 40 | | 3 | 2020-07-31 | 3 | 70 | | 4 | 2020-07-29 | 4 | 100 | | 5 | 2020-06-10 | 1 | 1010 | | 6 | 2020-08-01 | 2 | 102 | | 7 | 2020-08-01 | 3 | 111 | | 8 | 2020-08-03 | 1 | 99 | | 9 | 2020-08-07 | 2 | 32 | | 10 | 2020-07-15 | 1 | 2 | +----------+------------+-------------+------+ Result table: +---------------+-------------+----------+------------+ | customer_name | customer_id | order_id | order_date | +---------------+-------------+----------+------------+ | Annabelle | 3 | 7 | 2020-08-01 | | Annabelle | 3 | 3 | 2020-07-31 | | Jonathan | 2 | 9 | 2020-08-07 | | Jonathan | 2 | 6 | 2020-08-01 | | Jonathan | 2 | 2 | 2020-07-30 | | Marwan | 4 | 4 | 2020-07-29 | | Winston | 1 | 8 | 2020-08-03 | | Winston | 1 | 1 | 2020-07-31 | | Winston | 1 | 10 | 2020-07-15 | +---------------+-------------+----------+------------+ Winston has 4 orders, we discard the order of \"2020-06-10\" because it is the oldest order. Annabelle has only 2 orders, we return them. Jonathan has exactly 3 orders. Marwan ordered only one time. We sort the result table by customer_name in ascending order, by customer_id in ascending order and by order_date in descending order in case of a tie. Ans. select customer_name, customer_id, order_id, order_date from (select a.name as customer_name, a.customer_id, b.order_id, to_char(b.order_date, 'YYYY-MM-DD') as order_date, rank() over ( partition by a.customer_id order by b.order_date desc) date_rank from customers a inner join orders b on a.customer_id = b.customer_id order by a.customer_id, b.order_date ) where date_rank<=3 order by customer_name, customer_id, order_date desc ; 2A. Shortest Distance in a Plane https://leetcode.com/problems/shortest-distance-in-a-plane/ Table point_2d holds the coordinates (x,y) of some unique points (more than two) in a plane. Write a query to find the shortest distance between these points rounded to 2 decimals. x y -1 -1 0 0 -1 -2 The shortest distance is 1.00 from point (-1,-1) to (-1,2). So the output should be: shortest 1.00 Note: The longest distance among all the points are less than 10000. Ans. select min(dist) as shortest from ( select a.x as x1, a.y as y1, b.x as x2, b.y as y2, round(sqrt((a.x - b.x)*(a.x - b.x) + (a.y - b.y)*(a.y - b.y) ),2) as dist from point_2d a, point_2d b where concat(a.x,a.y)!=concat(b.x,b.y) -- this bit of matching coordinates is IMP to ensure same coordinates are not being captured. ); Investments in 2016 https://leetcode.com/problems/investments-in-2016/ Write a query to print the sum of all total investment values in 2016 (TIV_2016), to a scale of 2 decimal places, for all policy-holders who meet the following criteria: - Have the same TIV_2015 value as one or more other policyholders. - Are not located in the same city as any other policyholder (i.e.: the (latitude, longitude) attribute pairs must be unique). Input Format: The insurance table is described as follows: Column Name Type PID INTEGER(11) TIV_2015 NUMERIC(15,2) TIV_2016 NUMERIC(15,2) LAT NUMERIC(5,2) LON NUMERIC(5,2) where PID is the policyholder's policy ID, TIV_2015 is the total investment value in 2015, TIV_2016 is the total investment value in 2016, LAT is the latitude of the policy holder's city, and LON is the longitude of the policy holder's city. Sample Input PID TIV_2015 TIV_2016 LAT LON 1 10 5 10 10 2 20 20 20 20 3 10 30 20 20 4 10 40 40 40 Sample Output | TIV_2016 | |----------| | 45.00 | Explanation The first record in the table, like the last record, meets both of the two criteria. The TIV_2015 value '10' is as the same as the third and forth record, and its location unique. The second record does not meet any of the two criteria. Its TIV_2015 is not like any other policyholders. And its location is the same with the third record, which makes the third record fail, too. So, the result is the sum of TIV_2016 of the first and last record, which is 45. Ans. select round(sum(tiv_2016),2) as \"TIV_2016\" from ( select pid, count(pid) over (partition by tiv_2015) as tiv_2015_count, tiv_2016, count(*) over (partition by concat(concat(lat,'_'),lon)) as locn_count from insurance order by lat,lon) where locn_count=1 and tiv_2015_count>1; Calculate Salaries https://leetcode.com/problems/calculate-salaries/ Table Salaries: Column Name Type company_id int employee_id int employee_name varchar salary int (company_id, employee_id) is the primary key for this table. This table contains the company id, the id, the name and the salary for an employee. Write an SQL query to find the salaries of the employees after applying taxes. The tax rate is calculated for each company based on the following criteria: 0% If the max salary of any employee in the company is less than 1000$. 24% If the max salary of any employee in the company is in the range [1000, 10000] inclusive. 49% If the max salary of any employee in the company is greater than 10000$. Return the result table in any order. Round the salary to the nearest integer. The query result format is in the following example: Salaries table: +------------+-------------+---------------+--------+ | company_id | employee_id | employee_name | salary | +------------+-------------+---------------+--------+ | 1 | 1 | Tony | 2000 | | 1 | 2 | Pronub | 21300 | | 1 | 3 | Tyrrox | 10800 | | 2 | 1 | Pam | 300 | | 2 | 7 | Bassem | 450 | | 2 | 9 | Hermione | 700 | | 3 | 7 | Bocaben | 100 | | 3 | 2 | Ognjen | 2200 | | 3 | 13 | Nyancat | 3300 | | 3 | 15 | Morninngcat | 1866 | +------------+-------------+---------------+--------+ Result table: +------------+-------------+---------------+--------+ | company_id | employee_id | employee_name | salary | +------------+-------------+---------------+--------+ | 1 | 1 | Tony | 1020 | | 1 | 2 | Pronub | 10863 | | 1 | 3 | Tyrrox | 5508 | | 2 | 1 | Pam | 300 | | 2 | 7 | Bassem | 450 | | 2 | 9 | Hermione | 700 | | 3 | 7 | Bocaben | 76 | | 3 | 2 | Ognjen | 1672 | | 3 | 13 | Nyancat | 2508 | | 3 | 15 | Morninngcat | 5911 | +------------+-------------+---------------+--------+ For company 1, Max salary is 21300. Employees in company 1 have taxes = 49% For company 2, Max salary is 700. Employees in company 2 have taxes = 0% For company 3, Max salary is 7777. Employees in company 3 have taxes = 24% The salary after taxes = salary - (taxes percentage / 100) * salary For example, Salary for Morninngcat (3, 15) after taxes = 7777 - 7777 * (24 / 100) = 7777 - 1866.48 = 5910.52, which is rounded to 5911. Ans. select company_id, employee_id, employee_name, round( salary*(case when max_sal_per_company<1000 then 1 when max_sal_per_company>=1000 and max_sal_per_company<=10000 then 0.76 else 0.51 end ),0) as salary from ( select company_id , employee_id , employee_name , salary, max(salary) over (partition by company_id ) as max_sal_per_company from salaries); Countries You Can Safely Invest In https://leetcode.com/problems/countries-you-can-safely-invest-in/ Table Person: Column Name Type id int name varchar phone_number varchar id is the primary key for this table. Each row of this table contains the name of a person and their phone number. Phone number will be in the form 'xxx-yyyyyyy' where xxx is the country code (3 characters) and yyyyyyy is the phone number (7 characters) where x and y are digits. Both can contain leading zeros. Table Country: Column Name Type name varchar country_code varchar country_code is the primary key for this table. Each row of this table contains the country name and its code. country_code will be in the form 'xxx' where x is digits. Table Calls: Column Name Type caller_id int callee_id int duration int There is no primary key for this table, it may contain duplicates. Each row of this table contains the caller id, callee id and the duration of the call in minutes. caller_id != callee_id A telecommunications company wants to invest in new countries. The company intends to invest in the countries where the average call duration of the calls in this country is strictly greater than the global average call duration. Write an SQL query to find the countries where this company can invest. Return the result table in any order. The query result format is in the following example. Person table: +----+----------+--------------+ | id | name | phone_number | +----+----------+--------------+ | 3 | Jonathan | 051-1234567 | | 12 | Elvis | 051-7654321 | | 1 | Moncef | 212-1234567 | | 2 | Maroua | 212-6523651 | | 7 | Meir | 972-1234567 | | 9 | Rachel | 972-0011100 | +----+----------+--------------+ Country table: +----------+--------------+ | name | country_code | +----------+--------------+ | Peru | 051 | | Israel | 972 | | Morocco | 212 | | Germany | 049 | | Ethiopia | 251 | +----------+--------------+ Calls table: +-----------+-----------+----------+ | caller_id | callee_id | duration | +-----------+-----------+----------+ | 1 | 9 | 33 | | 2 | 9 | 4 | | 1 | 2 | 59 | | 3 | 12 | 102 | | 3 | 12 | 330 | | 12 | 3 | 5 | | 7 | 9 | 13 | | 7 | 1 | 3 | | 9 | 7 | 1 | | 1 | 7 | 7 | +-----------+-----------+----------+ Result table: +----------+ | country | +----------+ | Peru | +----------+ The average call duration for Peru is (102 + 102 + 330 + 330 + 5 + 5) / 6 = 145.666667 The average call duration for Israel is (33 + 4 + 13 + 13 + 3 + 1 + 1 + 7) / 8 = 9.37500 The average call duration for Morocco is (33 + 4 + 59 + 59 + 3 + 7) / 6 = 27.5000 Global call duration average = (2 * (33 + 3 + 59 + 102 + 330 + 5 + 13 + 3 + 1 + 7)) / 20 = 55.70000 Since Peru is the only country where average call duration is greater than the global average, it's the only recommended country. Ans. with call_table as (select distinct caller_id, callee_id, duration from calls ) select distinct country from ( select t.name as country, c.country_code, avg(c.duration) over (partition by c.country_code) as country_avg, avg(c.duration) over () as overall_avg from ( select substr(b.phone_number,1,3) as country_code, a.duration from call_table a left join Person b on a.caller_id = b.id union all select substr(b.phone_number,1,3) as country_code, a.duration from call_table a left join Person b on a.callee_id = b.id ) c left join Country t on trim(c.country_code) = trim(t.country_code) ) where country_avg>overall_avg; Rectangles Area https://leetcode.com/problems/rectangles-area/ Table: Points +---------------+---------+ | Column Name | Type | +---------------+---------+ | id | int | | x_value | int | | y_value | int | +---------------+---------+ id is the primary key for this table. Each point is represented as a 2D Dimensional (x_value, y_value). Write an SQL query to report of all possible rectangles which can be formed by any two points of the table. Each row in the result contains three columns (p1, p2, area) where: p1 and p2 are the id of two opposite corners of a rectangle and p1 < p2. Area of this rectangle is represented by the column area. Report the query in descending order by area in case of tie in ascending order by p1 and p2. Points table: +----------+-------------+-------------+ | id | x_value | y_value | +----------+-------------+-------------+ | 1 | 2 | 8 | | 2 | 4 | 7 | | 3 | 2 | 10 | +----------+-------------+-------------+ Result table: +----------+-------------+-------------+ | p1 | p2 | area | +----------+-------------+-------------+ | 2 | 3 | 6 | | 1 | 2 | 2 | +----------+-------------+-------------+ p1 should be less than p2 and area greater than 0. p1 = 1 and p2 = 2, has an area equal to |2-4| * |8-7| = 2. p1 = 2 and p2 = 3, has an area equal to |4-2| * |7-10| = 6. p1 = 1 and p2 = 3 It's not possible because the rectangle has an area equal to 0. Ans. -- Approach 1: select a.id as P1, b.id as P2, abs(a.x_value - b.x_value)*abs(a.y_value - b.y_value) as area from points a, points b where a.id < b.id and abs(a.x_value - b.x_value)*abs(a.y_value - b.y_value)>0 order by abs(a.x_value - b.x_value)*abs(a.y_value - b.y_value) desc, a.id, b.id; -- Approach 2: select a.p1, b.p2, abs(a.x1-b.x2)*abs(a.y1-b.y2) as area from (select id as P1, x_value as x1, y_value as y1, concat(x_value, y_value) as p1_coordinate from points) a, (select id as P2, x_value as x2, y_value as y2, concat(x_value, y_value) as p2_coordinate from points ) b where p1<p2 and p1_coordinate!=p2_coordinate --no duplicate/same point and abs(x1-x2)*abs(y1-y2)>0 order by abs(a.x1-b.x2)*abs(a.y1-b.y2) desc, a.p1, b.p2 Active users : Imp -- date diff between consecutive days. https://leetcode.com/problems/active-users/ Table Accounts: Column Name Type id int name varchar the id is the primary key for this table. This table contains the account id and the user name of each account. Table Logins: Column Name Type id int login_date date +---------------+---------+ There is no primary key for this table, it may contain duplicates. This table contains the account id of the user who logged in and the login date. A user may log in multiple times in the day. Write an SQL query to find the id and the name of active users. Active users are those who logged in to their accounts for 5 or more consecutive days. Return the result table ordered by the id. The query result format is in the following example: Accounts table: id name 1 Winston 7 Jonathan Logins table: id login_date 7 2020-05-30 1 2020-05-30 7 2020-05-31 7 2020-06-01 7 2020-06-02 7 2020-06-02 7 2020-06-03 1 2020-06-07 7 2020-06-10 Result table: id name 7 Jonathan User Winston with id = 1 logged in 2 times only in 2 different days, so, Winston is not an active user. User Jonathan with id = 7 logged in 7 times in 6 different days, five of them were consecutive days, so, Jonathan is an active user. Ans. select distinct d.id, d.name from ( select b.id, c.name, b.login_date, b.found_recs_old_login_date, to_date(b.login_date,'YYYY-MM-DD') - to_date(b.found_recs_old_login_date,'YYYY-MM-DD') as date_diff from ( select a.id, a.login_date, lag( a.login_date,4,'1990-01-01') over (partition by a.id order by a.login_date) as found_recs_old_login_date from (select distinct id, to_char(login_date,'YYYY-MM-DD') as login_date from Logins ) a ) b left join Accounts c on b.id = c.id ) d where d.date_diff=4 order by d.id; Apples & Oranges https://leetcode.com/problems/apples-oranges/ Table: Sales Column Name Type sale_date date fruit enum sold_num int (sale_date,fruit) is the primary key for this table. This table contains the sales of \"apples\" and \"oranges\" sold each day. Write an SQL query to report the difference between number of apples and oranges sold each day. Return the result table ordered by sale_date in format ('YYYY-MM-DD'). The query result format is in the following example: Sales table: +------------+------------+-------------+ | sale_date | fruit | sold_num | +------------+------------+-------------+ | 2020-05-01 | apples | 10 | | 2020-05-01 | oranges | 8 | | 2020-05-02 | apples | 15 | | 2020-05-02 | oranges | 15 | | 2020-05-03 | apples | 20 | | 2020-05-03 | oranges | 0 | | 2020-05-04 | apples | 15 | | 2020-05-04 | oranges | 16 | +------------+------------+-------------+ Result table: +------------+--------------+ | sale_date | diff | +------------+--------------+ | 2020-05-01 | 2 | | 2020-05-02 | 0 | | 2020-05-03 | 20 | | 2020-05-04 | -1 | +------------+--------------+ Day 2020-05-01, 10 apples and 8 oranges were sold (Difference 10 - 8 = 2). Day 2020-05-02, 15 apples and 15 oranges were sold (Difference 15 - 15 = 0). Day 2020-05-03, 20 apples and 0 oranges were sold (Difference 20 - 0 = 20). Day 2020-05-04, 15 apples and 16 oranges were sold (Difference 15 - 16 = -1). Ans. KL - select coalesce(to_char(a.sale_date,'YYYY-MM-DD'),to_char(b.sale_date,'YYYY-MM-DD')) as sale_date, coalesce(a.sold_num,0) - coalesce(b.sold_num,0) as diff from (select sale_date , sold_num from sales where fruit='apples') a full join (select sale_date , sold_num from sales where fruit='oranges') b on to_char(a.sale_date,'YYYY-MM-DD')=to_char(b.sale_date,'YYYY-MM-DD') order by coalesce(to_char(a.sale_date,'YYYY-MM-DD'),to_char(b.sale_date,'YYYY-MM-DD')); NG - select sale_date, sum(case when fruit = 'apples' then sold_num else 0-sold_num end) as \"diff\" from sales group by sale_date Evaluate Boolean Expression https://leetcode.com/problems/evaluate-boolean-expression/ Table Variables: Column Name Type name varchar value int name is the primary key for this table. This table contains the stored variables and their values. Table Expressions: Column Name Type left_operand varchar operator enum right_operand varchar (left_operand, operator, right_operand) is the primary key for this table. This table contains a boolean expression that should be evaluated. operator is an enum that takes one of the values ('<', '>', '=') The values of left_operand and right_operand are guaranteed to be in the Variables table. Write an SQL query to evaluate the boolean expressions in Expressions table. Return the result table in any order. The query result format is in the following example. Variables table: +------+-------+ | name | value | +------+-------+ | x | 66 | | y | 77 | +------+-------+ Expressions table: +--------------+----------+---------------+ | left_operand | operator | right_operand | +--------------+----------+---------------+ | x | > | y | | x | < | y | | x | = | y | | y | > | x | | y | < | x | | x | = | x | +--------------+----------+---------------+ Result table: +--------------+----------+---------------+-------+ | left_operand | operator | right_operand | value | +--------------+----------+---------------+-------+ | x | > | y | false | | x | < | y | true | | x | = | y | false | | y | > | x | true | | y | < | x | false | | x | = | x | true | +--------------+----------+---------------+-------+ As shown, you need find the value of each boolean exprssion in the table using the variables table. Ans. select left_operand as \"left_operand\", operator as \"operator\", right_operand as \"right_operand\", case when operator='>' then case when left_value>right_value then 'true' else 'false' end when operator='<' then case when left_value<right_value then 'true' else 'false' end when operator='=' then case when left_value=right_value then 'true' else 'false' end end as \"value\" from ( select a.left_operand , b.value as left_value, a.operator , a.right_operand , c.value as right_value from expressions a left join variables b on a.left_operand = b.name left join variables c on a.right_operand = c.name ); Customers Who Bought Products A and B but Not C https://leetcode.com/problems/customers-who-bought-products-a-and-b-but-not-c/submissions/ Table: Customers Column Name Type customer_id int customer_name varchar customer_id is the primary key for this table. customer_name is the name of the customer. Table: Orders Column Name Type order_id int customer_id int product_name varchar order_id is the primary key for this table. customer_id is the id of the customer who bought the product \"product_name\". Write an SQL query to report the customer_id and customer_name of customers who bought products \"A\", \"B\" but did not buy the product \"C\" since we want to recommend them buy this product. Return the result table ordered by customer_id. The query result format is in the following example. Customers table: +-------------+---------------+ | customer_id | customer_name | +-------------+---------------+ | 1 | Daniel | | 2 | Diana | | 3 | Elizabeth | | 4 | Jhon | +-------------+---------------+ Orders table: +------------+--------------+---------------+ | order_id | customer_id | product_name | +------------+--------------+---------------+ | 10 | 1 | A | | 20 | 1 | B | | 30 | 1 | D | | 40 | 1 | C | | 50 | 2 | A | | 60 | 3 | A | | 70 | 3 | B | | 80 | 3 | D | | 90 | 4 | C | +------------+--------------+---------------+ Result table: +-------------+---------------+ | customer_id | customer_name | +-------------+---------------+ | 3 | Elizabeth | +-------------+---------------+ Only the customer_id with id 3 bought the product A and B but not the product C. Ans. select customer_id, customer_name from ( select distinct a.customer_id, b.customer_name, a.product_name from orders a left join customers b on a.customer_id = b.customer_id where a.product_name in ('A','B','C') ) group by customer_id,customer_name having sum(case when product_name='A' then 1 when product_name='B' then 2 else 100 end)=3 order by customer_id; Capital Gain/Loss https://leetcode.com/problems/capital-gainloss/ Table: Stocks Column Name Type stock_name varchar operation enum operation_day int price int (stock_name, day) is the primary key for this table. The operation column is an ENUM of type ('Sell', 'Buy') Each row of this table indicates that the stock which has stock_name had an operation on the day operation_day with the price. It is guaranteed that each 'Sell' operation for a stock has a corresponding 'Buy' operation in a previous day. Write an SQL query to report the Capital gain/loss for each stock. The capital gain/loss of a stock is total gain or loss after buying and selling the stock one or many times. Return the result table in any order. The query result format is in the following example: Stocks table: +---------------+-----------+---------------+--------+ | stock_name | operation | operation_day | price | +---------------+-----------+---------------+--------+ | Leetcode | Buy | 1 | 1000 | | Corona Masks | Buy | 2 | 10 | | Leetcode | Sell | 5 | 9000 | | Handbags | Buy | 17 | 30000 | | Corona Masks | Sell | 3 | 1010 | | Corona Masks | Buy | 4 | 1000 | | Corona Masks | Sell | 5 | 500 | | Corona Masks | Buy | 6 | 1000 | | Handbags | Sell | 29 | 7000 | | Corona Masks | Sell | 10 | 10000 | +---------------+-----------+---------------+--------+ Result table: +---------------+-------------------+ | stock_name | capital_gain_loss | +---------------+-------------------+ | Corona Masks | 9500 | | Leetcode | 8000 | | Handbags | -23000 | +---------------+-------------------+ Leetcode stock was bought at day 1 for 1000$ and was sold at day 5 for 9000$. Capital gain = 9000 - 1000 = 8000$. Handbags stock was bought at day 17 for 30000$ and was sold at day 29 for 7000$. Capital loss = 7000 - 30000 = -23000$. Corona Masks stock was bought at day 1 for 10$ and was sold at day 3 for 1010$. It was bought again at day 4 for 1000$ and was sold at day 5 for 500$. At last, it was bought at day 6 for 1000$ and was sold at day 10 for 10000$. Capital gain/loss is the sum of capital gains/losses for each ('Buy' --> 'Sell') operation = (1010 - 10) + (500 - 1000) + (10000 - 1000) = 1000 - 500 + 9000 = 9500$. Ans. select stock_name, sum(case when operation ='Sell' then price else 0-price end) as capital_gain_loss from stocks group by stock_name order by stock_name; Number of Trusted Contacts of a Customer https://leetcode.com/problems/number-of-trusted-contacts-of-a-customer/ Table: Customers Column Name Type customer_id int customer_name varchar email varchar customer_id is the primary key for this table. Each row of this table contains the name and the email of a customer of an online shop. Table: Contacts Column Name Type user_id id contact_name varchar contact_email varchar (user_id, contact_email) is the primary key for this table. Each row of this table contains the name and email of one contact of customer with user_id. This table contains information about people each customer trust. The contact may or may not exist in the Customers table. Table: Invoices Column Name Type invoice_id int price int user_id int invoice_id is the primary key for this table. Each row of this table indicates that user_id has an invoice with invoice_id and a price. Write an SQL query to find the following for each invoice_id: customer_name: The name of the customer the invoice is related to. price: The price of the invoice. contacts_cnt: The number of contacts related to the customer. trusted_contacts_cnt: The number of contacts related to the customer and at the same time they are customers to the shop. (i.e His/Her email exists in the Customers table.) Order the result table by invoice_id. The query result format is in the following example: Customers table: customer_id customer_name email 1 Alice alice@leetcode.com 2 Bob bob@leetcode.com 13 John john@leetcode.com 6 Alex alex@leetcode.com Contacts table: user_id contact_name contact_email 1 Bob bob@leetcode.com 1 John john@leetcode.com 1 Jal jal@leetcode.com 2 Omar omar@leetcode.com 2 Meir meir@leetcode.com 6 Alice alice@leetcode.com Invoices table: invoice_id price user_id 77 100 1 88 200 1 99 300 2 66 400 2 55 500 13 44 60 6 Result table: invoice_id customer_name price contacts_cnt trusted_contacts_cnt 44 Alex 60 1 1 55 John 500 0 0 66 Bob 400 2 0 77 Alice 100 3 2 88 Alice 200 3 2 99 Bob 300 2 0 Alice has three contacts, two of them are trusted contacts (Bob and John). Bob has two contacts, none of them is a trusted contact. Alex has one contact and it is a trusted contact (Alice). John doesn't have any contacts. Ans. select a.invoice_id , b.customer_name, a.price, (select count(*) from contacts where user_id = a.user_id) as contacts_cnt , (select count(*) from contacts where user_id = a.user_id and contact_email in (select email from customers) ) as trusted_contacts_cnt from invoices a left join customers b on a.user_id = b.customer_id order by invoice_id; -- Other Approach: Using Joins select a.invoice_id , b.customer_name, a.price, count(e.contact_email) as contacts_cnt , sum(case when e.email is not NULL then 1 else 0 end) as trusted_contacts_cnt from invoices a left join customers b on a.user_id = b.customer_id left join ( select c.user_id, c.contact_email, d.email from contacts c left join customers d on c.contact_email = d.email ) e on a.user_id = e.user_id group by a.invoice_id , b.customer_name, a.price order by a.invoice_id; Activity Participants https://leetcode.com/problems/activity-participants/ Table: Friends Column Name Type id int name varchar activity varchar id is the id of the friend and primary key for this table. name is the name of the friend. activity is the name of the activity which the friend takes part in. Table: Activities Column Name Type id int name varchar id is the primary key for this table. name is the name of the activity. Write an SQL query to find the names of all the activities with neither maximum, nor minimum number of participants. Return the result table in any order. Each activity in table Activities is performed by any person in the table Friends. The query result format is in the following example: Friends table: +------+--------------+---------------+ | id | name | activity | +------+--------------+---------------+ | 1 | Jonathan D. | Eating | | 2 | Jade W. | Singing | | 3 | Victor J. | Singing | | 4 | Elvis Q. | Eating | | 5 | Daniel A. | Eating | | 6 | Bob B. | Horse Riding | +------+--------------+---------------+ Activities table: +------+--------------+ | id | name | +------+--------------+ | 1 | Eating | | 2 | Singing | | 3 | Horse Riding | +------+--------------+ Result table: +--------------+ | activity | +--------------+ | Singing | +--------------+ Eating activity is performed by 3 friends, maximum number of participants, (Jonathan D. , Elvis Q. and Daniel A.) Horse Riding activity is performed by 1 friend, minimum number of participants, (Bob B.) Singing is performed by 2 friends (Victor J. and Jade W.) Ans. select activity from ( select activity, count(*) as act_count, min(count(*)) over () as min_cnt, max(count(*)) over () as max_cnt from friends group by activity order by activity ) where min_cnt<act_count and act_count<max_cnt; Second Degree Follower https://leetcode.com/problems/second-degree-follower/submissions/ In facebook, there is a follow table with two columns: followee, follower. Please write a sql query to get the amount of each follower\u2019s follower if he/she has one. For example: followee follower A B B C B D D E should output: follower num B 2 D 1 Explaination: Both B and D exist in the follower list, when as a followee, B's follower is C and D, and D's follower is E. A does not exist in follower list. Note: Followee would not follow himself/herself in all cases. Ans. There could be duplicates in table, so use count distinct for counting followers. select main as follower, count(distinct follower) as num from ( select a.follower as main, b.follower as follower from follow a inner join follow b on a.follower = b.followee ) group by main order by main;","title":"SQL Practise Questions"},{"location":"SQL/docs/sql_performance_tuning/","text":"SQL Performance Tuning : Summary Tip 1: Never use *(Star) to fetch all records from table Sql query become fast if you use actual columns instead of * to fetch all the records from the table. Not Recommended - Select * from Employee; Recommended Select Eno,Ename,Address from Employee; Tip 2: Try to avoid DISTINCT keyword from the query Try to avoid DISTINCT keyword from select statements. DISTINCT keyword has high cost and low performance. When anyone uses DISTINCT keyword, it first SORTS the data from column and then fetches the distinct values. Use EXISTS operator instead of DISTINCT keyword. Not Recommended: SELECT DISTINCT d.dept_no, d.department_name FROM Department d,Employee e WHERE d.dept_no= e.dept_no; Recommended: SELECT d.dept_no, d.department_name FROM Department d WHERE EXISTS ( SELECT \u2018X\u2019 FROM Employee e WHERE d.dept_no= e.dept_no); Tip 3: Carefully use WHERE conditions in sql Try to use correct operator as per requirement given. Not Recommended: Select * from Employee WHERE salary != 65000; Recommended: Select * from Employee WHERE salary > 65000 and salary < 65000; Tip 4: Use Like operator instead of equal to (=) Not Recommended: Select * from Employee where name=\u2019Amit\u2019; Recommended: Select * from Employee where name like \u2018Amit%\u2019; Tip 5: Avoid HAVING clause/GROUP BY statements HAVING clause and GROUP BY statements have high cost. So try to avoid it in sql query. Not Recommended - Select * from Employee WHERE name=\u2019Amit\u2019 GROUP BY department HAVING salary=45000; Recommended - Select * from Employee WHERE name=\u2019Amit\u2019 and salary=45000; More added: - Having clause- We use Having clause to eliminate some of the group values. Issue \u2013 Having clause restricts the rows AFTER they are read. So if no restriction in \u201cwhere clause\u201d, optimizer will use full table scan. This is really important coz all predicates in the HAVING Clause will not be used as access predicates. So they will not make optimizer use indexes, partitions etc. This is because to perform HAVING clause, it first reads all the rows and then eliminates unnecessary rows. Tip 6: Use of EXISTS and IN Operators Basically, Operator IN has lowest performance. IN operator is used when Filter criteria is in subquery, whereas EXISTS operator is used when filter criteria is in main query. Example: IN Operator Select * from Employee where Department_name IN ( Select Department_name from Department where Dno=10); Exist operator Select * from Employee where EXISTS ( Select Department_name from Department where Dno=10); More added When you run a query with IN clause, database will process it in below format \u2013 That is, in case of use of IN clause, each value of sub query is joined with outer query. Treats below Query - select * from T1 where x in (select x from T2); as - select * from t1, (select x from t2) T2 where t1.x = t2.x; But when you use EXIST clause, database will exit as soon as it gets the first match . So, in case of EXIST clause it runs executes query in below format \u2013 Treats below query - select * from T1 where exists (select x from T2 where t1.x=t2.x); as - FOR x IN (select * from t1) LOOP IF (EXISTS ( SELECT X FROM T2) ) THEN OUTPUT THE RECORD END IF; END; That is, using EXIST clause will imply database will run it like a FOR loop and as soon as match is found, it moves to next record. So which one is faster \u2013 IN or EXIST? a. This totally depends on situation. Use IN when - outer table = Big and Subquery = Small Use EXISTS when \u2013 outer table = Small and Subquery = Big b. Even above rules are not fixed. For example, if subquery has bigger table, but it has an index, in this case use of IN is suggested. c. So- EXISTS doesn\u2019t work better than IN all the times. IN is better than EXISTS if \u2013 outer table = Big and Subquery = Small outer table = Small and Subquery = Big + Indexed NOT EXISTS vs NOT IN \u2022 NOT EXISTS is not equivalent of NOT IN. \u2022 NOT EXISTS cannot be used instead of NOT IN all the times. \u2022 More specifically, if there is any NULL value in your data, they will show different result. \u2022 If your subquery returns even one NULL value, NOT IN will not match any rows. \u2022 On other hand, if you have a chance to use NOT EXISTS instead of NOT IN, you should test it. \u2022 In most database versions of oracle, EXISTS and IN are treated similarly in terms of execution plan. Tip 7: Try to use UNION ALL instead of UNION UNION scans all data first and then eliminate duplicate so it has slow performance. Not Recommended Select * from Employee where dept_no=10 UNION Select * from Employee where dept_no=20; Recommended Select * from Employee where dept_no=10 UNION ALL Select * from Employee where dept_no=20; Tip 8: Avoid use of Functions in Where condition. Not Recommended Select * from Employee where Substr(name,1,3)=\u2019Ami\u2019; Recommended Select * from Employee where name like \u2018Ami%\u2019; Tip 9: convert OR to AND If we use OR clause, it will PREVENT index usages. Instead, we should use AND where possible. Not Recommended select * from sales where prod_id = 13 or promo_id=14; Recommended select * from sales where prod_id = 13 UNION All select * from sales where promo_id=14 AND prod_id <> 13; Tip 10: Subquery Unnesting Nested queries are very costly, and so transformer tries to convert them to equivalent join statements. Not Recommended select * from sales where cust_id IN (select cust_id from customers); Recommended select sales.* from sales, customers where sales.cust_id=customers.cust_id; Tip 11: IN and BETWEEN select * from employees where emp_id in (2,3,4,5); The above is equivalent to select * from employees where emp_id = 2 OR emp_id=3 OR emp_id=4 OR emp_id=5 ---this implies full table scan. Solution - select * from employees where emp_id between 2 and 5; Tip 12: Fetching first N records Suppose we want to see only 10 records in our select statement output. There are 2 ways to do this \u2013 Using rownum (Recommended) SELECT * FROM EMPLOYEE where rownum<11; Using fetch first (not recommended) SELECT * FROM EMPLOYEE FETCH FIRST 10 ROWS ONLY; In case of rownum- Here it reads first 10 rows use count STOPKEY operator, and so faster than fetch first method. In case of fetch first \u2013 Here we read whole table, and then applied a windowing function to select 1st 10 records. Tip 13: UNION vs UNION ALL: UNION \u2013 combines data and drops duplicate rows. UNION ALL \u2013 combines data and retains duplicate rows. Suggest: Some key points- \u2022 by default, UNION ALL is less costly than UNION, as latter sorts data internally to remove duplicates. \u2022 But if your table is indexed, then sort operation in UNION wont be that costly, and so you can use UNION also. So \u2013 - Use UNION if table is indexed and you don\u2019t want duplicates in output. - Use UNION ALL if\u2013 - There is no duplicate in your data, or - You are ok with having duplicate data in output. - But overall, UNION ALL gives better performance than UNION. Tip 14: INTERSECT Vs EXISTS operator Intersect gives common rows of 2 intersection in a Sorted order. As part of intersect, 2 rows sources are first sorted, and then common records are fetched. In place of INTERSECT operator, we should try and use EXISTS clause, which is more efficient. One caveat is that, in case of EXISTS clause, output is not sorted, unlike in case of INTERSECT clause. Not Recommended SELECT employee_id FROM employees where employee_id between 145 and 179 INTERSECT SELECT employee_id FROM employees WHERE first_name LIKE 'A%'; Recommended SELECT employee_id FROM employees e where employee_id between 145 and 179 and exists (SELECT employee_id FROM employees t WHERE first_name LIKE 'A%' and e.employee_id = t.employee_id); Tip 15: MINUS Vs NOT EXISTS MINUS operator eliminates matched rows of 1st (with 2nd) and returns rest of the rows of 1st. NOT EXISTS clause can also do same work as MINUS, but has much better performance. Not Recommended SELECT employee_id FROM employees where employee_id between 145 and 179 MINUS SELECT employee_id FROM employees WHERE first_name LIKE 'A%'; Recommended SELECT employee_id FROM employees e where employee_id between 145 and 179 and not exists (SELECT employee_id FROM employees t WHERE first_name LIKE 'A%' and e.employee_id = t.employee_id); Tip 16: Using Like conditions To enable use of indexes, avoid use of wild card character at beginning of source text scan. If you are forced to use wild card character at beginning, you can create reverse index, and handle that problem using that index. In this case, when you reverse index, then source text will eliminate use of wild card at beginning. Eg \u2013 Suppose you want to find records where last name ends is \u201chhar\u201d, then create reverse() index on last_name to reverse it and then use condition where reversed last name begins with \u201crahh\u201d. Though reverse() index usage will have cost, but if your column is selective enough, it wont be much cost. Tip 17: Using Functions on Indexed Columns will suppress index usage. Use of function on indexed column will suppress index usage. So, rewrite query to avoid use of function. BAD QUERY select employee_id, first_name, last_name from employees where trunc(hire_date,'YEAR') = '01-JAN-2002'; GOOD: rewritten query select employee_id, first_name, last_name from employees where hire_date between '01-JAN-2002' and '31-DEC-2002'; Eg2 - Bad select * from mytable where substr(emp_name,1,2) = 'Po'; Good select * from mytable where emp_name like 'Po%'; Note - In Spark, we have HashAggregates and SortAggregates. Hash Aggregates are more performant, and work only on mutable data types. That is, if all elements in your Select clause (except those in Group by clause) are mutable types like INT, FLOAT, etc, then spark will use Hash Aggregates. This means, sometimes, for performance gain, we need to apply Function to transform values. See #14 at below link for details https://github.com/kushal-luthra/spark-development/blob/master/notes/spark_opimization.md Tip 18: Handling NULL Values Failing to deal with NULL values will lead to unintentional results or performance losses. Why - \u2022B-Tree indexes do not index NULL values. \u2022If there are any NULL values in your indexed columns and you need to get rows which have NULL values, optimizer will not use your index, and perform a full table scan instead. \u2022That is, having Null values in your index may sometimes suppress index usage. Ways to handle NULL value-based performance loss a. Use IS NOT NULL condition in your WHERE clause. Use IS NOT NULL condition in your WHERE clause if you don\u2019t need to have NULL values in result set. That is, even if you now your result will not be having any NULL values, you should use \u201cis not null \u201c clause to make optimizer use indexes. Eg \u2013 Query 1: select emp_name, emp_id from employee where emp_id <> 1; Query 2: select emp_name, emp_id from employee where emp_id <> 1 and emp_id is not null; In query 1, we will see FULL Table scan and in case of query 2, we see index-based scan, and lower query cost. b. Adding not null constraint to your columns and insert a specific value for NULL values like \u20180\u2019 if value in a column is null. c. If reasonable, create a BITMAP index instead of B-Tree index. BITMAP indexes store NULL values. So even if there are null values in our column, optimizer will be able to use our indexes. However, you need to take into consideration index efficiency between B-Tree and BITMAP, as former as more efficient than latter. - We use BITMAP indexes when \u2013 cardinality is LOW and index not modified often. - We use B-Tree index when \u2013 cardinality/selectivity is high. Tip 19 : Use Truncate instead of Delete Truncate is always faster than DELETE command. This is because when you run delete command, oracle database generates lots of UNDO data for deleted rows and generating UNDO data is an expensive operation. Truncate doesn\u2019t generate UNDO Data. But before using Truncate command, there are few things to note about it- \u2022 No rollback \u2013 Truncate operation cannot be rollbacked, and Flashback is also not so easy after truncate operations. You may need to use Flashback Data Archive or some other external tools in this case. \u2022 Truncate is a DDL operation \u2013 So when you run Truncate, your transaction will be committed. It performs commit before and after Truncate operation. Since it does 2 commits, and even if truncate operation fails in between, the changes you did before will be permanent in any case. \u2022 Truncate a partition - We don\u2019t need to truncate whole table all the times. You can truncate partition as well. \u2022 Truncate doesn\u2019t fire DML triggers - So you wont be able to log your truncate operation because of that. But it can fire the DDL triggers. \u2022 Truncate makes unusable indexes usable again. But delete does not. Tip 20: Data Type Mismatches If data types of column and compared value dont match, this may suppress index usage. select cust_id, cust_code from customers where cust_code = 101; Vs select cust_id, cust_code from customers where cust_code = '101'; Tip 21: Tuning Ordered queries- Order By clause Order by mostly requires SORT operations . This sort operation is done in PGA or in disk (if PGA doesn\u2019t have enough memory) This disk is shown as \u2018temporary table space\u2019 in execution plan. Issue \u2013 sorting in disk is a costly operation. Solution \u2013 \u2022 Create a B-Tree index on column used in Order by Clause, or \u2022 Modify a B-Tree index to include column used in Order by Clause. \u2022 Why\u2013> B-Tree indexes store columns in Order and using B-Tree index will eliminate sort operations. Tip 22 : Retrieving MIN and MAX Values B-Tree indexes increase the performance a lot for min and max value searches. If no B-Tree index, optimizer will need to read whole table. If our query has another column or another aggregate function in your query, it will be reading whole index or whole table. For example- When you see below, if we are looking for min() and max() values individually, output is just 2 for each. But when we want to get min() and max() together, database will read full table, and hence cost is 8 times. This is coz we have 2 aggregate functions in our query. Bad - select min(), max() from mytable; Good - select * FROM (select min() from mytable) min_cust, (select max() from mytable) max_cust; Tip 23 : Views Simple view = view created from single table. Complex view = view created by using multiple tables. Some suggestions w.r.t. views- 1. If you don\u2019t need to use all the tables in a view, then don\u2019t use the view. Instead use the actual tables. Otherwise, server will have to join all tables, do aggregation etc on them for a view. i.e. use view for the purpose for which it was created. 2. Else create another view. 3. Don\u2019t join complex views with a table or another view - This is because most of the times view is first executed completely at first, and then result is used as row source to other table or view. So, in this case you be reading lots of unnecessary data and performing unnecessary join and group by. This will increase cost a lot. 4. Avoid performing outer join to the views \u2013 because if you use equality predicate on view column, the optimizer gets wrong if the table of that column has an outer join in the view as well. Because outer join may not know performance of view and may lead to bad execution as well. E.g. \u2013 if we do outer join, optimizer may not be able to push predicate inside the view definition at times of execution plan. Materialized Views- Unlike basic and complex views, materialized views store both query and data. Materialized view data can be refreshed manually or via auto-refresh. But materialized view maintenance is a burden to database \u2013 it needs to be kept up to date for each modification on each change. As compared to normal views, materialized view will improve performance as we will select data directly from materialized view, and there will be no sorts, joins etc. We can create index, partitions etc on materialized view like in an ordinary table. Summary \u2022 If you don\u2019t need to use all the tables in a view, then don\u2019t use the view. Instead use the actual tables. \u2022 Don\u2019t join complex views with a table or another view. \u2022 Avoid performing outer join to the views. \u2022 Use Materialized View - Unlike basic and complex views, materialized views store both query and data. Materialized view data can be refreshed manually or via auto-refresh. But materialized view maintenance is a burden to database . Tip 24: Frequent commit is not desired make REDO logs bulky as we may be committing prior to period. make lock on modified rows, making them unavailable to other applications. Tip 25: Multitable DML operations (skip for big data) Sometimes we have to read same table as input to different tables in our data warehouse. So, if we have 5 different tables requiring input from 1 table, we should ideally be reading input table just once, and keep on feeding into different output tables as per requirements. For this we have 2 options \u2013 \u2022 INSERT ALL \u2022 MERGE INTO","title":"SQL Performance Tuning"},{"location":"SQL/docs/sql_performance_tuning/#sql-performance-tuning-summary","text":"","title":"SQL Performance Tuning : Summary"},{"location":"SQL/docs/sql_performance_tuning/#tip-1-never-use-star-to-fetch-all-records-from-table","text":"Sql query become fast if you use actual columns instead of * to fetch all the records from the table. Not Recommended - Select * from Employee; Recommended Select Eno,Ename,Address from Employee;","title":"Tip 1: Never use *(Star) to fetch all records from table"},{"location":"SQL/docs/sql_performance_tuning/#tip-2-try-to-avoid-distinct-keyword-from-the-query","text":"Try to avoid DISTINCT keyword from select statements. DISTINCT keyword has high cost and low performance. When anyone uses DISTINCT keyword, it first SORTS the data from column and then fetches the distinct values. Use EXISTS operator instead of DISTINCT keyword. Not Recommended: SELECT DISTINCT d.dept_no, d.department_name FROM Department d,Employee e WHERE d.dept_no= e.dept_no; Recommended: SELECT d.dept_no, d.department_name FROM Department d WHERE EXISTS ( SELECT \u2018X\u2019 FROM Employee e WHERE d.dept_no= e.dept_no);","title":"Tip 2: Try to avoid DISTINCT keyword from the query"},{"location":"SQL/docs/sql_performance_tuning/#tip-3-carefully-use-where-conditions-in-sql","text":"Try to use correct operator as per requirement given. Not Recommended: Select * from Employee WHERE salary != 65000; Recommended: Select * from Employee WHERE salary > 65000 and salary < 65000;","title":"Tip 3: Carefully use WHERE conditions in sql"},{"location":"SQL/docs/sql_performance_tuning/#tip-4-use-like-operator-instead-of-equal-to","text":"Not Recommended: Select * from Employee where name=\u2019Amit\u2019; Recommended: Select * from Employee where name like \u2018Amit%\u2019;","title":"Tip 4: Use Like operator instead of equal to (=)"},{"location":"SQL/docs/sql_performance_tuning/#tip-5-avoid-having-clausegroup-by-statements","text":"HAVING clause and GROUP BY statements have high cost. So try to avoid it in sql query. Not Recommended - Select * from Employee WHERE name=\u2019Amit\u2019 GROUP BY department HAVING salary=45000; Recommended - Select * from Employee WHERE name=\u2019Amit\u2019 and salary=45000; More added: - Having clause- We use Having clause to eliminate some of the group values. Issue \u2013 Having clause restricts the rows AFTER they are read. So if no restriction in \u201cwhere clause\u201d, optimizer will use full table scan. This is really important coz all predicates in the HAVING Clause will not be used as access predicates. So they will not make optimizer use indexes, partitions etc. This is because to perform HAVING clause, it first reads all the rows and then eliminates unnecessary rows.","title":"Tip 5: Avoid HAVING clause/GROUP BY statements"},{"location":"SQL/docs/sql_performance_tuning/#tip-6-use-of-exists-and-in-operators","text":"Basically, Operator IN has lowest performance. IN operator is used when Filter criteria is in subquery, whereas EXISTS operator is used when filter criteria is in main query. Example: IN Operator Select * from Employee where Department_name IN ( Select Department_name from Department where Dno=10); Exist operator Select * from Employee where EXISTS ( Select Department_name from Department where Dno=10); More added When you run a query with IN clause, database will process it in below format \u2013 That is, in case of use of IN clause, each value of sub query is joined with outer query. Treats below Query - select * from T1 where x in (select x from T2); as - select * from t1, (select x from t2) T2 where t1.x = t2.x; But when you use EXIST clause, database will exit as soon as it gets the first match . So, in case of EXIST clause it runs executes query in below format \u2013 Treats below query - select * from T1 where exists (select x from T2 where t1.x=t2.x); as - FOR x IN (select * from t1) LOOP IF (EXISTS ( SELECT X FROM T2) ) THEN OUTPUT THE RECORD END IF; END; That is, using EXIST clause will imply database will run it like a FOR loop and as soon as match is found, it moves to next record. So which one is faster \u2013 IN or EXIST? a. This totally depends on situation. Use IN when - outer table = Big and Subquery = Small Use EXISTS when \u2013 outer table = Small and Subquery = Big b. Even above rules are not fixed. For example, if subquery has bigger table, but it has an index, in this case use of IN is suggested. c. So- EXISTS doesn\u2019t work better than IN all the times. IN is better than EXISTS if \u2013 outer table = Big and Subquery = Small outer table = Small and Subquery = Big + Indexed NOT EXISTS vs NOT IN \u2022 NOT EXISTS is not equivalent of NOT IN. \u2022 NOT EXISTS cannot be used instead of NOT IN all the times. \u2022 More specifically, if there is any NULL value in your data, they will show different result. \u2022 If your subquery returns even one NULL value, NOT IN will not match any rows. \u2022 On other hand, if you have a chance to use NOT EXISTS instead of NOT IN, you should test it. \u2022 In most database versions of oracle, EXISTS and IN are treated similarly in terms of execution plan.","title":"Tip 6: Use of EXISTS and IN Operators"},{"location":"SQL/docs/sql_performance_tuning/#tip-7-try-to-use-union-all-instead-of-union","text":"UNION scans all data first and then eliminate duplicate so it has slow performance. Not Recommended Select * from Employee where dept_no=10 UNION Select * from Employee where dept_no=20; Recommended Select * from Employee where dept_no=10 UNION ALL Select * from Employee where dept_no=20;","title":"Tip 7: Try to use UNION ALL instead of UNION"},{"location":"SQL/docs/sql_performance_tuning/#tip-8-avoid-use-of-functions-in-where-condition","text":"Not Recommended Select * from Employee where Substr(name,1,3)=\u2019Ami\u2019; Recommended Select * from Employee where name like \u2018Ami%\u2019;","title":"Tip 8: Avoid use of Functions in Where condition."},{"location":"SQL/docs/sql_performance_tuning/#tip-9-convert-or-to-and","text":"If we use OR clause, it will PREVENT index usages. Instead, we should use AND where possible. Not Recommended select * from sales where prod_id = 13 or promo_id=14; Recommended select * from sales where prod_id = 13 UNION All select * from sales where promo_id=14 AND prod_id <> 13;","title":"Tip 9: convert OR to AND"},{"location":"SQL/docs/sql_performance_tuning/#tip-10-subquery-unnesting","text":"Nested queries are very costly, and so transformer tries to convert them to equivalent join statements. Not Recommended select * from sales where cust_id IN (select cust_id from customers); Recommended select sales.* from sales, customers where sales.cust_id=customers.cust_id;","title":"Tip 10: Subquery Unnesting"},{"location":"SQL/docs/sql_performance_tuning/#tip-11-in-and-between","text":"select * from employees where emp_id in (2,3,4,5); The above is equivalent to select * from employees where emp_id = 2 OR emp_id=3 OR emp_id=4 OR emp_id=5 ---this implies full table scan. Solution - select * from employees where emp_id between 2 and 5;","title":"Tip 11: IN and BETWEEN"},{"location":"SQL/docs/sql_performance_tuning/#tip-12-fetching-first-n-records","text":"Suppose we want to see only 10 records in our select statement output. There are 2 ways to do this \u2013 Using rownum (Recommended) SELECT * FROM EMPLOYEE where rownum<11; Using fetch first (not recommended) SELECT * FROM EMPLOYEE FETCH FIRST 10 ROWS ONLY; In case of rownum- Here it reads first 10 rows use count STOPKEY operator, and so faster than fetch first method. In case of fetch first \u2013 Here we read whole table, and then applied a windowing function to select 1st 10 records.","title":"Tip 12: Fetching first N records"},{"location":"SQL/docs/sql_performance_tuning/#tip-13-union-vs-union-all","text":"UNION \u2013 combines data and drops duplicate rows. UNION ALL \u2013 combines data and retains duplicate rows. Suggest: Some key points- \u2022 by default, UNION ALL is less costly than UNION, as latter sorts data internally to remove duplicates. \u2022 But if your table is indexed, then sort operation in UNION wont be that costly, and so you can use UNION also. So \u2013 - Use UNION if table is indexed and you don\u2019t want duplicates in output. - Use UNION ALL if\u2013 - There is no duplicate in your data, or - You are ok with having duplicate data in output. - But overall, UNION ALL gives better performance than UNION.","title":"Tip 13: UNION vs UNION ALL:"},{"location":"SQL/docs/sql_performance_tuning/#tip-14-intersect-vs-exists-operator","text":"Intersect gives common rows of 2 intersection in a Sorted order. As part of intersect, 2 rows sources are first sorted, and then common records are fetched. In place of INTERSECT operator, we should try and use EXISTS clause, which is more efficient. One caveat is that, in case of EXISTS clause, output is not sorted, unlike in case of INTERSECT clause. Not Recommended SELECT employee_id FROM employees where employee_id between 145 and 179 INTERSECT SELECT employee_id FROM employees WHERE first_name LIKE 'A%'; Recommended SELECT employee_id FROM employees e where employee_id between 145 and 179 and exists (SELECT employee_id FROM employees t WHERE first_name LIKE 'A%' and e.employee_id = t.employee_id);","title":"Tip 14: INTERSECT Vs EXISTS operator"},{"location":"SQL/docs/sql_performance_tuning/#tip-15-minus-vs-not-exists","text":"MINUS operator eliminates matched rows of 1st (with 2nd) and returns rest of the rows of 1st. NOT EXISTS clause can also do same work as MINUS, but has much better performance. Not Recommended SELECT employee_id FROM employees where employee_id between 145 and 179 MINUS SELECT employee_id FROM employees WHERE first_name LIKE 'A%'; Recommended SELECT employee_id FROM employees e where employee_id between 145 and 179 and not exists (SELECT employee_id FROM employees t WHERE first_name LIKE 'A%' and e.employee_id = t.employee_id);","title":"Tip 15: MINUS Vs NOT EXISTS"},{"location":"SQL/docs/sql_performance_tuning/#tip-16-using-like-conditions","text":"To enable use of indexes, avoid use of wild card character at beginning of source text scan. If you are forced to use wild card character at beginning, you can create reverse index, and handle that problem using that index. In this case, when you reverse index, then source text will eliminate use of wild card at beginning. Eg \u2013 Suppose you want to find records where last name ends is \u201chhar\u201d, then create reverse() index on last_name to reverse it and then use condition where reversed last name begins with \u201crahh\u201d. Though reverse() index usage will have cost, but if your column is selective enough, it wont be much cost.","title":"Tip 16: Using Like conditions"},{"location":"SQL/docs/sql_performance_tuning/#tip-17-using-functions-on-indexed-columns-will-suppress-index-usage","text":"Use of function on indexed column will suppress index usage. So, rewrite query to avoid use of function. BAD QUERY select employee_id, first_name, last_name from employees where trunc(hire_date,'YEAR') = '01-JAN-2002'; GOOD: rewritten query select employee_id, first_name, last_name from employees where hire_date between '01-JAN-2002' and '31-DEC-2002'; Eg2 - Bad select * from mytable where substr(emp_name,1,2) = 'Po'; Good select * from mytable where emp_name like 'Po%'; Note - In Spark, we have HashAggregates and SortAggregates. Hash Aggregates are more performant, and work only on mutable data types. That is, if all elements in your Select clause (except those in Group by clause) are mutable types like INT, FLOAT, etc, then spark will use Hash Aggregates. This means, sometimes, for performance gain, we need to apply Function to transform values. See #14 at below link for details https://github.com/kushal-luthra/spark-development/blob/master/notes/spark_opimization.md","title":"Tip 17: Using Functions on Indexed Columns will suppress index usage."},{"location":"SQL/docs/sql_performance_tuning/#tip-18-handling-null-values","text":"Failing to deal with NULL values will lead to unintentional results or performance losses. Why - \u2022B-Tree indexes do not index NULL values. \u2022If there are any NULL values in your indexed columns and you need to get rows which have NULL values, optimizer will not use your index, and perform a full table scan instead. \u2022That is, having Null values in your index may sometimes suppress index usage. Ways to handle NULL value-based performance loss a. Use IS NOT NULL condition in your WHERE clause. Use IS NOT NULL condition in your WHERE clause if you don\u2019t need to have NULL values in result set. That is, even if you now your result will not be having any NULL values, you should use \u201cis not null \u201c clause to make optimizer use indexes. Eg \u2013 Query 1: select emp_name, emp_id from employee where emp_id <> 1; Query 2: select emp_name, emp_id from employee where emp_id <> 1 and emp_id is not null; In query 1, we will see FULL Table scan and in case of query 2, we see index-based scan, and lower query cost. b. Adding not null constraint to your columns and insert a specific value for NULL values like \u20180\u2019 if value in a column is null. c. If reasonable, create a BITMAP index instead of B-Tree index. BITMAP indexes store NULL values. So even if there are null values in our column, optimizer will be able to use our indexes. However, you need to take into consideration index efficiency between B-Tree and BITMAP, as former as more efficient than latter. - We use BITMAP indexes when \u2013 cardinality is LOW and index not modified often. - We use B-Tree index when \u2013 cardinality/selectivity is high.","title":"Tip 18: Handling NULL Values"},{"location":"SQL/docs/sql_performance_tuning/#tip-19-use-truncate-instead-of-delete","text":"Truncate is always faster than DELETE command. This is because when you run delete command, oracle database generates lots of UNDO data for deleted rows and generating UNDO data is an expensive operation. Truncate doesn\u2019t generate UNDO Data. But before using Truncate command, there are few things to note about it- \u2022 No rollback \u2013 Truncate operation cannot be rollbacked, and Flashback is also not so easy after truncate operations. You may need to use Flashback Data Archive or some other external tools in this case. \u2022 Truncate is a DDL operation \u2013 So when you run Truncate, your transaction will be committed. It performs commit before and after Truncate operation. Since it does 2 commits, and even if truncate operation fails in between, the changes you did before will be permanent in any case. \u2022 Truncate a partition - We don\u2019t need to truncate whole table all the times. You can truncate partition as well. \u2022 Truncate doesn\u2019t fire DML triggers - So you wont be able to log your truncate operation because of that. But it can fire the DDL triggers. \u2022 Truncate makes unusable indexes usable again. But delete does not.","title":"Tip 19 : Use Truncate instead of Delete"},{"location":"SQL/docs/sql_performance_tuning/#tip-20-data-type-mismatches","text":"If data types of column and compared value dont match, this may suppress index usage. select cust_id, cust_code from customers where cust_code = 101; Vs select cust_id, cust_code from customers where cust_code = '101';","title":"Tip 20: Data Type Mismatches"},{"location":"SQL/docs/sql_performance_tuning/#tip-21-tuning-ordered-queries-order-by-clause","text":"Order by mostly requires SORT operations . This sort operation is done in PGA or in disk (if PGA doesn\u2019t have enough memory) This disk is shown as \u2018temporary table space\u2019 in execution plan. Issue \u2013 sorting in disk is a costly operation. Solution \u2013 \u2022 Create a B-Tree index on column used in Order by Clause, or \u2022 Modify a B-Tree index to include column used in Order by Clause. \u2022 Why\u2013> B-Tree indexes store columns in Order and using B-Tree index will eliminate sort operations.","title":"Tip 21: Tuning Ordered queries- Order By clause"},{"location":"SQL/docs/sql_performance_tuning/#tip-22-retrieving-min-and-max-values","text":"B-Tree indexes increase the performance a lot for min and max value searches. If no B-Tree index, optimizer will need to read whole table. If our query has another column or another aggregate function in your query, it will be reading whole index or whole table. For example- When you see below, if we are looking for min() and max() values individually, output is just 2 for each. But when we want to get min() and max() together, database will read full table, and hence cost is 8 times. This is coz we have 2 aggregate functions in our query. Bad - select min(), max() from mytable; Good - select * FROM (select min() from mytable) min_cust, (select max() from mytable) max_cust;","title":"Tip 22 : Retrieving MIN and MAX Values"},{"location":"SQL/docs/sql_performance_tuning/#tip-23-views","text":"Simple view = view created from single table. Complex view = view created by using multiple tables. Some suggestions w.r.t. views- 1. If you don\u2019t need to use all the tables in a view, then don\u2019t use the view. Instead use the actual tables. Otherwise, server will have to join all tables, do aggregation etc on them for a view. i.e. use view for the purpose for which it was created. 2. Else create another view. 3. Don\u2019t join complex views with a table or another view - This is because most of the times view is first executed completely at first, and then result is used as row source to other table or view. So, in this case you be reading lots of unnecessary data and performing unnecessary join and group by. This will increase cost a lot. 4. Avoid performing outer join to the views \u2013 because if you use equality predicate on view column, the optimizer gets wrong if the table of that column has an outer join in the view as well. Because outer join may not know performance of view and may lead to bad execution as well. E.g. \u2013 if we do outer join, optimizer may not be able to push predicate inside the view definition at times of execution plan. Materialized Views- Unlike basic and complex views, materialized views store both query and data. Materialized view data can be refreshed manually or via auto-refresh. But materialized view maintenance is a burden to database \u2013 it needs to be kept up to date for each modification on each change. As compared to normal views, materialized view will improve performance as we will select data directly from materialized view, and there will be no sorts, joins etc. We can create index, partitions etc on materialized view like in an ordinary table. Summary \u2022 If you don\u2019t need to use all the tables in a view, then don\u2019t use the view. Instead use the actual tables. \u2022 Don\u2019t join complex views with a table or another view. \u2022 Avoid performing outer join to the views. \u2022 Use Materialized View - Unlike basic and complex views, materialized views store both query and data. Materialized view data can be refreshed manually or via auto-refresh. But materialized view maintenance is a burden to database .","title":"Tip 23 : Views"},{"location":"SQL/docs/sql_performance_tuning/#tip-24-frequent-commit-is-not-desired","text":"make REDO logs bulky as we may be committing prior to period. make lock on modified rows, making them unavailable to other applications.","title":"Tip 24: Frequent commit is not desired"},{"location":"SQL/docs/sql_performance_tuning/#tip-25-multitable-dml-operations-skip-for-big-data","text":"Sometimes we have to read same table as input to different tables in our data warehouse. So, if we have 5 different tables requiring input from 1 table, we should ideally be reading input table just once, and keep on feeding into different output tables as per requirements. For this we have 2 options \u2013 \u2022 INSERT ALL \u2022 MERGE INTO","title":"Tip 25: Multitable DML operations (skip for big data)"},{"location":"SQL/docs/sql_performance_tuning_summary/","text":"SQL Performance Tuning : Summary Tip 1: Never use *(Star) to fetch all records from table Tip 2: Try to avoid DISTINCT keyword from the query Not Recommended: SELECT DISTINCT d.dept_no, d.department_name FROM Department d,Employee e WHERE d.dept_no= e.dept_no; Recommended: SELECT d.dept_no d.department_name FROM Department d WHERE EXISTS ( SELECT \u2018X\u2019 FROM Employee e WHERE d.dept_no= e.dept_no); Tip 3: Carefully use WHERE conditions in sql. Tip 4: Use Like operator instead of equal to (=) Tip 5: Avoid HAVING clause/GROUP BY statements Tip 6: Use of EXISTS and IN Operators Tip 7: Try to use UNION ALL instead of UNION as UNION scans all data first and then eliminate duplicate so it has slow performance. Tip 9: convert OR to AND Tip 10: Subquery Unnesting Tip 11: IN and BETWEEN Tip 12: Fetching first N records: SELECT * FROM EMPLOYEE where rownum<11 Tip 13: UNION vs UNION ALL: \u2022 by default, UNION ALL is less costly than UNION, as latter sorts data internally to remove duplicates. \u2022 But if your table is indexed, then sort operation in UNION wont be that costly, and so you can use UNION also. Tip 14: INTERSECT Vs EXISTS operator Tip 15: MINUS Vs NOT EXISTS Tip 16: Using Like conditions To enable use of indexes, avoid use of wild card character at beginning of source text scan. If you are forced to use wild card character at beginning, you can create reverse index, and handle that problem using that index. In this case, when you reverse index, then source text will eliminate use of wild card at beginning. Tip 17: Using Functions on Indexed Columns will suppress index usage. So, rewrite query to avoid use of function. BAD QUERY select employee_id, first_name, last_name from employees where trunc(hire_date,'YEAR') = '01-JAN-2002'; GOOD: rewritten query select employee_id, first_name, last_name from employees where hire_date between '01-JAN-2002' and '31-DEC-2002'; Eg2 - Bad select * from mytable where substr(emp_name,1,2) = 'Po'; Good select * from mytable where emp_name like 'Po%'; Tip 18: Handling NULL Values \u2022B-Tree indexes do not index NULL values. \u2022If there are any NULL values in your indexed columns and you need to get rows which have NULL values, optimizer will not use your index, and perform a full table scan instead. \u2022That is, having Null values in your index may sometimes suppress index usage. Solution - - Use IS NOT NULL condition in your WHERE clause. - Adding not null constraint to your columns and insert a specific value for NULL values like \u20180\u2019 if value in a column is null. - If reasonable, create a BITMAP index instead of B-Tree index. Tip 19 : Use Truncate instead of Delete Tip 20: Data Type Mismatches If data types of column and compared value dont match, this may suppress index usage. Tip 21: Tuning Ordered queries- Order By clause Order by mostly requires sort operations. This sort operation is done in PGA or in disk (if PGA doesn\u2019t have enough memory) This disk is shown as \u2018temporary table space\u2019 in execution plan. Issue \u2013 sorting in disk is a costly operation. Solution \u2013 \u2022 Create a B-Tree index on column used in Order by Clause, or \u2022 Modify a B-Tree index to include column used in Order by Clause. \u2022 Why \u2013 B-Tree indexes store columns in Order and using B-Tree index will eliminate sort operations. Tip 22 : Retrieving MIN and MAX Values B-Tree indexes increase the performance a lot for min and max value searches. If no B-Tree index, optimizer will need to read whole table. Bad - select min(), max() from mytable; Good - select * FROM (select min() from mytable) min_cust, (select max() from mytable) max_cust; Tip 23 : Views \u2022 If you don\u2019t need to use all the tables in a view, then don\u2019t use the view. Instead use the actual tables. \u2022 Don\u2019t join complex views with a table or another view. \u2022 Avoid performing outer join to the views. \u2022 Unlike basic and complex views, materialized views store both query and data. Materialized view data can be refreshed manually or via auto-refresh. But materialized view maintenance is a burden to database . Tip 24: Frequent commit is not desired: make REDO logs bulky as we may be committing prior to period. make lock on modified rows, making them unavailable to other applications.","title":"SQL Performance Tuning Summary"},{"location":"SQL/docs/sql_performance_tuning_summary/#sql-performance-tuning-summary","text":"","title":"SQL Performance Tuning : Summary"},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-1-never-use-star-to-fetch-all-records-from-table","text":"","title":"Tip 1: Never use *(Star) to fetch all records from table"},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-2-try-to-avoid-distinct-keyword-from-the-query","text":"Not Recommended: SELECT DISTINCT d.dept_no, d.department_name FROM Department d,Employee e WHERE d.dept_no= e.dept_no; Recommended: SELECT d.dept_no d.department_name FROM Department d WHERE EXISTS ( SELECT \u2018X\u2019 FROM Employee e WHERE d.dept_no= e.dept_no);","title":"Tip 2: Try to avoid DISTINCT keyword from the query "},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-3-carefully-use-where-conditions-in-sql","text":"","title":"Tip 3: Carefully use WHERE conditions in sql. "},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-4-use-like-operator-instead-of-equal-to","text":"","title":"Tip 4: Use Like operator instead of equal to (=)"},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-5-avoid-having-clausegroup-by-statements","text":"","title":"Tip 5: Avoid HAVING clause/GROUP BY statements"},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-6-use-of-exists-and-in-operators","text":"","title":"Tip 6: Use of EXISTS and IN Operators"},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-7-try-to-use-union-all-instead-of-union-as-union-scans-all-data-first-and-then-eliminate-duplicate-so-it-has-slow-performance","text":"","title":"Tip 7: Try to use UNION ALL instead of UNION as UNION scans all data first and then eliminate duplicate so it has slow performance."},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-9-convert-or-to-and","text":"","title":"Tip 9: convert OR to AND"},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-10-subquery-unnesting","text":"","title":"Tip 10: Subquery Unnesting"},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-11-in-and-between","text":"","title":"Tip 11: IN and BETWEEN"},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-12-fetching-first-n-records-select-from-employee-where-rownum11","text":"","title":"Tip 12: Fetching first N records:  SELECT * FROM EMPLOYEE where rownum&lt;11"},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-13-union-vs-union-all","text":"\u2022 by default, UNION ALL is less costly than UNION, as latter sorts data internally to remove duplicates. \u2022 But if your table is indexed, then sort operation in UNION wont be that costly, and so you can use UNION also.","title":"Tip 13: UNION vs UNION ALL:"},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-14-intersect-vs-exists-operator","text":"","title":"Tip 14: INTERSECT Vs EXISTS operator"},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-15-minus-vs-not-exists","text":"","title":"Tip 15: MINUS Vs NOT EXISTS"},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-16-using-like-conditions","text":"To enable use of indexes, avoid use of wild card character at beginning of source text scan. If you are forced to use wild card character at beginning, you can create reverse index, and handle that problem using that index. In this case, when you reverse index, then source text will eliminate use of wild card at beginning.","title":"Tip 16: Using Like conditions"},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-17-using-functions-on-indexed-columns-will-suppress-index-usage","text":"So, rewrite query to avoid use of function. BAD QUERY select employee_id, first_name, last_name from employees where trunc(hire_date,'YEAR') = '01-JAN-2002'; GOOD: rewritten query select employee_id, first_name, last_name from employees where hire_date between '01-JAN-2002' and '31-DEC-2002'; Eg2 - Bad select * from mytable where substr(emp_name,1,2) = 'Po'; Good select * from mytable where emp_name like 'Po%';","title":"Tip 17: Using Functions on Indexed Columns will suppress index usage."},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-18-handling-null-values","text":"\u2022B-Tree indexes do not index NULL values. \u2022If there are any NULL values in your indexed columns and you need to get rows which have NULL values, optimizer will not use your index, and perform a full table scan instead. \u2022That is, having Null values in your index may sometimes suppress index usage. Solution - - Use IS NOT NULL condition in your WHERE clause. - Adding not null constraint to your columns and insert a specific value for NULL values like \u20180\u2019 if value in a column is null. - If reasonable, create a BITMAP index instead of B-Tree index.","title":"Tip 18: Handling NULL Values"},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-19-use-truncate-instead-of-delete","text":"","title":"Tip 19 : Use Truncate instead of Delete"},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-20-data-type-mismatches","text":"If data types of column and compared value dont match, this may suppress index usage.","title":"Tip 20: Data Type Mismatches"},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-21-tuning-ordered-queries-order-by-clause","text":"Order by mostly requires sort operations. This sort operation is done in PGA or in disk (if PGA doesn\u2019t have enough memory) This disk is shown as \u2018temporary table space\u2019 in execution plan. Issue \u2013 sorting in disk is a costly operation. Solution \u2013 \u2022 Create a B-Tree index on column used in Order by Clause, or \u2022 Modify a B-Tree index to include column used in Order by Clause. \u2022 Why \u2013 B-Tree indexes store columns in Order and using B-Tree index will eliminate sort operations.","title":"Tip 21: Tuning Ordered queries- Order By clause"},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-22-retrieving-min-and-max-values","text":"B-Tree indexes increase the performance a lot for min and max value searches. If no B-Tree index, optimizer will need to read whole table. Bad - select min(), max() from mytable; Good - select * FROM (select min() from mytable) min_cust, (select max() from mytable) max_cust;","title":"Tip 22 : Retrieving MIN and MAX Values"},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-23-views","text":"\u2022 If you don\u2019t need to use all the tables in a view, then don\u2019t use the view. Instead use the actual tables. \u2022 Don\u2019t join complex views with a table or another view. \u2022 Avoid performing outer join to the views. \u2022 Unlike basic and complex views, materialized views store both query and data. Materialized view data can be refreshed manually or via auto-refresh. But materialized view maintenance is a burden to database .","title":"Tip 23 : Views"},{"location":"SQL/docs/sql_performance_tuning_summary/#tip-24-frequent-commit-is-not-desired","text":"make REDO logs bulky as we may be committing prior to period. make lock on modified rows, making them unavailable to other applications.","title":"Tip 24: Frequent commit is not desired:"},{"location":"Storage%20Layer%20Choices/File%20Compression%20techniques%20in%20Big%20Data/","text":"File Compression Techniques These techniques are common to Hadoop ecosystem, not just Hive. Why need compression? \u2022 Helps reduce storage especially when it comes to data being replicated across various nodes. \u2022 Helps us process data faster as size of data is less. \u2022 Since data is compressed, so I/O costs is less \u2013 A major overhead in processing large amounts of data is disk and network I/O, reducing the amount of data that needs to be read and written to disk can significantly decrease overall processing time. This includes compression of source data, but also the intermediate data generated as part of data processing. Compression and Decompression comes with some cost in terms of time taken to compress and decompress. But when we compare I/O gains, we can actually ignore this additional time to compress-decompress. Important Compression Techniques \u2013 1. Snappy 2. Lzo 3. Gzip 4. Bzip2 Some of the compression codecs are optimized for storage \u2013 they bring down size drastically. But this takes time. Some of compression codecs are optimized for speed \u2013 compression done quickly, but not efficiently. So trade-off is that \u2013 \u2022 if we want more compression ratio, we have to spend more time in compression. \u2022 If we want faster compression, we spend less time in compression. Snappy Snappy is a very fast compression. However, in terms of compression ratio, it is not that efficient. But in most production scenarios, snappy is used as it provides a fine balance between speed and compression efficiency. So, snappy is optimized for speed, not storage. Splittablity in compression techniques Although compression can greatly optimize processing performance, not all compression formats supported on Hadoop are splittable. Because the MapReduce framework splits data for input to multiple tasks, having a non splittable compression format is an impediment to efficient processing. If files cannot be split, that means the entire file needs to be passed to a single MapReduce task, eliminating the advantages of parallelism and data locality that Hadoop provides. For this reason, splitability is a major consideration in choosing a compression format as well as file format. Snappy by default is not splittable \u2013 so if we use non splittable file formats like JSON and XML, snappy won\u2019t give splittable output. Is this a big concern? No \u2013 because in production scenarios, we hardly use JSON and XML. In production scenarios, we use container-based formats like Avro, Parquet, Orc \u2013 which are splittable by their structure and no need for compression technique to handle this aspect. So, Snappy is intended to be used with a container format like Avro, Orc, Parquet since it\u2019s not inherently splittable. Lzo LZO is similar to Snappy in that it\u2019s optimized for speed as opposed to size. Unlike Snappy, LZO compressed files are splittable, but this requires an additional indexing step. This makes LZO a good choice for things like plain-text files (like json, text and xml files) that are not being stored as part of a container format. It should also be noted that LZO\u2019s license prevents it from being distributed with Hadoop and requires a separate install, unlike Snappy, which can be distributed with Hadoop. But snappy is fastest among all compression techniques. Gzip \u2022 Gzip provides very good compression performance (on average, about 2.5times the compression that\u2019d be offered by Snappy). \u2022 But in terms of processing speed its slow. \u2022 Gzip is also not splittable, so it should be used with a container format. \u2022 Note that one reason Gzip is sometimes slower than Snappy for processing is that Gzip compressed files take up fewer blocks, so fewer tasks are required for processing the same data. For this reason, using smaller blocks with Gzip can lead to better performance. \u2022 Eg \u2013 o 1 gb file \u2013 split into 8 blocks. o After gzip compression, we get 200 mb file \u2013 2 block \u2013 so number of blocks coming down, which reduces parallelism. o Solution \u2013 reduce block size to say, 50Mb, leading to 200mb file split into 4 blocks, and so parallelism doubles. Bzip2 \u2022 Bzip2 provides excellent compression performance, but can be significantly slower than other compression codecs such as Snappy in terms of processing performance. \u2022 Unlike Snappy and Gzip, bzip2 is inherently splittable. \u2022 In the examples we have seen, bzip2 will normally compress around 9% better than GZip, in terms of storage space. \u2022 However, this extra compression comes with a significant read/write performance cost. This performance difference will vary with different machines, but in general bzip2 is about 10 times slower than GZip. \u2022 For this reason, it\u2019s not an ideal codec for Hadoop storage, unless your primary need is reducing the storage footprint. One example of such a use case would be using Hadoop mainly for active archival purposes. Codec Splittable compression speed snappy N low Highest lzo Y low high gzip N High slow bzip2 Y Highest Slowest","title":"File Compression Techniques in Big Data Systems"},{"location":"Storage%20Layer%20Choices/File%20Compression%20techniques%20in%20Big%20Data/#file-compression-techniques","text":"These techniques are common to Hadoop ecosystem, not just Hive. Why need compression? \u2022 Helps reduce storage especially when it comes to data being replicated across various nodes. \u2022 Helps us process data faster as size of data is less. \u2022 Since data is compressed, so I/O costs is less \u2013 A major overhead in processing large amounts of data is disk and network I/O, reducing the amount of data that needs to be read and written to disk can significantly decrease overall processing time. This includes compression of source data, but also the intermediate data generated as part of data processing. Compression and Decompression comes with some cost in terms of time taken to compress and decompress. But when we compare I/O gains, we can actually ignore this additional time to compress-decompress. Important Compression Techniques \u2013 1. Snappy 2. Lzo 3. Gzip 4. Bzip2 Some of the compression codecs are optimized for storage \u2013 they bring down size drastically. But this takes time. Some of compression codecs are optimized for speed \u2013 compression done quickly, but not efficiently. So trade-off is that \u2013 \u2022 if we want more compression ratio, we have to spend more time in compression. \u2022 If we want faster compression, we spend less time in compression.","title":"File Compression Techniques"},{"location":"Storage%20Layer%20Choices/File%20Compression%20techniques%20in%20Big%20Data/#snappy","text":"Snappy is a very fast compression. However, in terms of compression ratio, it is not that efficient. But in most production scenarios, snappy is used as it provides a fine balance between speed and compression efficiency. So, snappy is optimized for speed, not storage.","title":"Snappy"},{"location":"Storage%20Layer%20Choices/File%20Compression%20techniques%20in%20Big%20Data/#splittablity-in-compression-techniques","text":"Although compression can greatly optimize processing performance, not all compression formats supported on Hadoop are splittable. Because the MapReduce framework splits data for input to multiple tasks, having a non splittable compression format is an impediment to efficient processing. If files cannot be split, that means the entire file needs to be passed to a single MapReduce task, eliminating the advantages of parallelism and data locality that Hadoop provides. For this reason, splitability is a major consideration in choosing a compression format as well as file format. Snappy by default is not splittable \u2013 so if we use non splittable file formats like JSON and XML, snappy won\u2019t give splittable output. Is this a big concern? No \u2013 because in production scenarios, we hardly use JSON and XML. In production scenarios, we use container-based formats like Avro, Parquet, Orc \u2013 which are splittable by their structure and no need for compression technique to handle this aspect. So, Snappy is intended to be used with a container format like Avro, Orc, Parquet since it\u2019s not inherently splittable.","title":"Splittablity in compression techniques"},{"location":"Storage%20Layer%20Choices/File%20Compression%20techniques%20in%20Big%20Data/#lzo","text":"LZO is similar to Snappy in that it\u2019s optimized for speed as opposed to size. Unlike Snappy, LZO compressed files are splittable, but this requires an additional indexing step. This makes LZO a good choice for things like plain-text files (like json, text and xml files) that are not being stored as part of a container format. It should also be noted that LZO\u2019s license prevents it from being distributed with Hadoop and requires a separate install, unlike Snappy, which can be distributed with Hadoop. But snappy is fastest among all compression techniques.","title":"Lzo"},{"location":"Storage%20Layer%20Choices/File%20Compression%20techniques%20in%20Big%20Data/#gzip","text":"\u2022 Gzip provides very good compression performance (on average, about 2.5times the compression that\u2019d be offered by Snappy). \u2022 But in terms of processing speed its slow. \u2022 Gzip is also not splittable, so it should be used with a container format. \u2022 Note that one reason Gzip is sometimes slower than Snappy for processing is that Gzip compressed files take up fewer blocks, so fewer tasks are required for processing the same data. For this reason, using smaller blocks with Gzip can lead to better performance. \u2022 Eg \u2013 o 1 gb file \u2013 split into 8 blocks. o After gzip compression, we get 200 mb file \u2013 2 block \u2013 so number of blocks coming down, which reduces parallelism. o Solution \u2013 reduce block size to say, 50Mb, leading to 200mb file split into 4 blocks, and so parallelism doubles.","title":"Gzip"},{"location":"Storage%20Layer%20Choices/File%20Compression%20techniques%20in%20Big%20Data/#bzip2","text":"\u2022 Bzip2 provides excellent compression performance, but can be significantly slower than other compression codecs such as Snappy in terms of processing performance. \u2022 Unlike Snappy and Gzip, bzip2 is inherently splittable. \u2022 In the examples we have seen, bzip2 will normally compress around 9% better than GZip, in terms of storage space. \u2022 However, this extra compression comes with a significant read/write performance cost. This performance difference will vary with different machines, but in general bzip2 is about 10 times slower than GZip. \u2022 For this reason, it\u2019s not an ideal codec for Hadoop storage, unless your primary need is reducing the storage footprint. One example of such a use case would be using Hadoop mainly for active archival purposes. Codec Splittable compression speed snappy N low Highest lzo Y low high gzip N High slow bzip2 Y Highest Slowest","title":"Bzip2"},{"location":"Storage%20Layer%20Choices/Text%20File%20Formats/","text":"Text File format Not used in production. Types include \u2013 \u2022 Csv \u2013 raw files \u2022 Xml, Json \u2013 structured text data - These are files with some structure attached to them These are human readable. CSV file \u2022 Advantage \u2013 o Data is stored in a human readable way \u2022 Drawbacks \u2013 o Everything is stored in a text form \u2013 so even an integer is stored as a text. Issue \u2013 more bytes used to store an integer than needed as it is stored as a string. Eg \u2013 val = 4561987. If val = INT \u2013 then takes only 4 bytes to get stored If val = STRING \u2013 then each position takes 2 bytes, so total 14 bytes used. To conclude \u2013 text file takes a lot of storage since everything is stored as string. o When data is stored as string, and you want to read it and use it as integer, this type of conversion is time consuming. So, processing on text files can be very slow. o Further, I/O operations also take a lot of time \u2013 coz text file take lot of space as compared to other file formats, and transferring such data across the network will be I/O intensive process. XML, JSON files Disadvantage \u2013 \u2022 Same as CSV \u2013 o Storage space is large o Processing = slow o I/O = huge \u2022 Not splittable \u2013 JSON and XML have schema attached to them, due to which they are not splittable. This defeats the whole purpose of using Hadoop as not being splittable means no parallelism possible. To summarize, in production we don\u2019t use text file formats.","title":"Text-based File Formats"},{"location":"Storage%20Layer%20Choices/Text%20File%20Formats/#text-file-format","text":"Not used in production. Types include \u2013 \u2022 Csv \u2013 raw files \u2022 Xml, Json \u2013 structured text data - These are files with some structure attached to them These are human readable. CSV file \u2022 Advantage \u2013 o Data is stored in a human readable way \u2022 Drawbacks \u2013 o Everything is stored in a text form \u2013 so even an integer is stored as a text. Issue \u2013 more bytes used to store an integer than needed as it is stored as a string. Eg \u2013 val = 4561987. If val = INT \u2013 then takes only 4 bytes to get stored If val = STRING \u2013 then each position takes 2 bytes, so total 14 bytes used. To conclude \u2013 text file takes a lot of storage since everything is stored as string. o When data is stored as string, and you want to read it and use it as integer, this type of conversion is time consuming. So, processing on text files can be very slow. o Further, I/O operations also take a lot of time \u2013 coz text file take lot of space as compared to other file formats, and transferring such data across the network will be I/O intensive process. XML, JSON files Disadvantage \u2013 \u2022 Same as CSV \u2013 o Storage space is large o Processing = slow o I/O = huge \u2022 Not splittable \u2013 JSON and XML have schema attached to them, due to which they are not splittable. This defeats the whole purpose of using Hadoop as not being splittable means no parallelism possible. To summarize, in production we don\u2019t use text file formats.","title":"Text File format"},{"location":"Storage%20Layer%20Choices/big%20data%20file%20formats/","text":"Avro, ORC and Parquet There major ones that work well with Big Data Environment are \u2013 \u2022 Avro \u2022 Orc \u2022 Parquet All of above are \u2013 \u2022 Splittable \u2022 Agnostic compression - Any compression can be used with them, without readers having to know the codec. This is possible because codec is stored in the header metadata of the file format. Reader needn\u2019t know in advance what kind of compression technique is used with these files. Compression codec is kept in file metadata, and whenever reader wants to read the data, he gets to know compression codec from metadata and can easily read the data. Avro File Formats 1. It is a row-based file format \u2013 data is stored row-by-row. So, it supports faster writes, but slower reads (when you want to read a subset of columns). 2. Self-describing schema - Schema is stored in JSON format and this metadata is embedded as part of data itself. 3. Actual data is stored in Compressed Binary format, which is quite efficient in terms of storage. 4. Language Neutral - Avro file format is general file format, and supports processing using lot of programming languages like C++, java, Python, Ruby, etc 5. Schema Evolution \u2013 Avro is quite mature in terms of schema evolution as compared to other file formats. Schema evolution includes aspects like \u2013 a. Adding new columns b. Removing old column c. Renaming columns, etc 6. Splittable \u2013 file can be divided into parts which can be processed independently. Avro is a Serialization format. Serialization \u2013 converting data into a form which can be easily transferred over a network and stored in a file system. Deserialization \u2013 reading data and converting it into form which can be read by human. In which scenario, Avro is best suited \u2013 \u2022 For storing data in landing zone of data lake \u2013 why \u2013 o In lake, chances are different team requiring raw data, and since Avro is language neutral, so different teams can use it. o In lake, data is unprocessed, that is no ETL done. For ETL kind of operations, we tend to read whole row and not a subset of it, and here again Avro is suited. KL \u2013 in dh, we read only subset of data in lake to build warehouse. So, this point is debatable. o Finally, in lake, data schema evolves over time, and so Avro is suited for handling Schema evolution. \u2022 Avro is typically the format of choice for write-heavy workloads given it is easy to append new rows. Avro Vs Orc Category Avro Parquet format row based column based reads slower reads faster reads - so useful for analytical querying writes faster writes - so suited to ETL operations. slower writes schema evolution it is quite mature wrt Schema Evolution Limited schema evolution support - you can add/delete column from the end. complex data types No such support. Provides support for deeply nested data structure. Orc Vs Parquet Category Orc Parquet format column based column based predicate push down provides predicate push down to push the predicates at storage level - that allows us to query relevant data. No such support ACID properties Now supports ACID properties to an extent. No such support. Compression Better - has lot more encodings used as compared to parquet. good complex data types No such support. Provides support for deeply nested data structure.","title":"Big Data File Formats"},{"location":"Storage%20Layer%20Choices/big%20data%20file%20formats/#avro-orc-and-parquet","text":"There major ones that work well with Big Data Environment are \u2013 \u2022 Avro \u2022 Orc \u2022 Parquet All of above are \u2013 \u2022 Splittable \u2022 Agnostic compression - Any compression can be used with them, without readers having to know the codec. This is possible because codec is stored in the header metadata of the file format. Reader needn\u2019t know in advance what kind of compression technique is used with these files. Compression codec is kept in file metadata, and whenever reader wants to read the data, he gets to know compression codec from metadata and can easily read the data. Avro File Formats 1. It is a row-based file format \u2013 data is stored row-by-row. So, it supports faster writes, but slower reads (when you want to read a subset of columns). 2. Self-describing schema - Schema is stored in JSON format and this metadata is embedded as part of data itself. 3. Actual data is stored in Compressed Binary format, which is quite efficient in terms of storage. 4. Language Neutral - Avro file format is general file format, and supports processing using lot of programming languages like C++, java, Python, Ruby, etc 5. Schema Evolution \u2013 Avro is quite mature in terms of schema evolution as compared to other file formats. Schema evolution includes aspects like \u2013 a. Adding new columns b. Removing old column c. Renaming columns, etc 6. Splittable \u2013 file can be divided into parts which can be processed independently. Avro is a Serialization format. Serialization \u2013 converting data into a form which can be easily transferred over a network and stored in a file system. Deserialization \u2013 reading data and converting it into form which can be read by human. In which scenario, Avro is best suited \u2013 \u2022 For storing data in landing zone of data lake \u2013 why \u2013 o In lake, chances are different team requiring raw data, and since Avro is language neutral, so different teams can use it. o In lake, data is unprocessed, that is no ETL done. For ETL kind of operations, we tend to read whole row and not a subset of it, and here again Avro is suited. KL \u2013 in dh, we read only subset of data in lake to build warehouse. So, this point is debatable. o Finally, in lake, data schema evolves over time, and so Avro is suited for handling Schema evolution. \u2022 Avro is typically the format of choice for write-heavy workloads given it is easy to append new rows. Avro Vs Orc Category Avro Parquet format row based column based reads slower reads faster reads - so useful for analytical querying writes faster writes - so suited to ETL operations. slower writes schema evolution it is quite mature wrt Schema Evolution Limited schema evolution support - you can add/delete column from the end. complex data types No such support. Provides support for deeply nested data structure. Orc Vs Parquet Category Orc Parquet format column based column based predicate push down provides predicate push down to push the predicates at storage level - that allows us to query relevant data. No such support ACID properties Now supports ACID properties to an extent. No such support. Compression Better - has lot more encodings used as compared to parquet. good complex data types No such support. Provides support for deeply nested data structure.","title":"Avro, ORC and Parquet"},{"location":"Storage%20Layer%20Choices/row%20based%20and%20column%20based%20file%20formats/","text":"Row Vs Column File Format When you are designing big data solution, one fundamental que is \u2013 \u201chow data will be stored\u201d It involves taking into consideration 2 things \u2013 \u2022 File formats \u2022 Compression techniques Why do we need different File Formats? \u2022 To save storage \u2022 To do fast processing \u2022 To have less time in I/O operations \u2013 since we are dealing in big data, I/O operations are a big bottleneck, and so spending as less time in this area as possible. Our file formats help us in all 3 above if we go with right file format. There are a lot of choices available on file formats. Below are key aspects for deciding a file format - 1. Faster reads. 2. Faster writes. 3. Splittable - Some are designed in such a way that they are splittable \u2013 if a file is splittable, we can do parallel processing \u2013 in big data solutions, we consider only splittable file formats. 4. Schema evolution support \u2013 some of the file formats support schema evolution - i.e. to facilitate change in input data by allowing schema changes. 5. Advanced compression techniques 6. Most compatible platform - some work well with hive, some with spark, etc All the file formats have been divided into 2 broad categories \u2013 \u2022 Row based \u2022 Column based Row based \u2013 Here data is stored row-by-row. At a time, whole record is saved. If a new record comes, it gets appended at the end. So, writing a record is very easy coz you simply append at the end. Now let\u2019s talk about reading \u2013 While write is easy, but in order to get subset of columns, it has to read entire record. That is, performance is degraded when it comes to read. In data warehousing, wherein we scan specific set of records, row-based formats is not suggested. Regarding Compression of row-based file\u2013 Since data is stored record by record, so different data types are present together, next to each other. That means, compression is not as efficient as it could be. Column based file format \u2013 All column values are stored together. For Reading data in column-based file format \u2013 it allows us to skip data and read only relevant columns. Column based file format is suggested for data warehouse based query system. For write on column-based file, it is time consuming as you need to write on multiple places. Regarding Compression of column-based file\u2013 Since data is stored column by column, so same data types are present together, next to each other. That means, compression can be applied efficiently for each data type. To summarize \u2013 If you write once but read multiple times, go for column-based file format. If you read once, but write multiple times, go for row-based file format. Category row based column based read slower reads faster reads writes faster writes slower writes compression poor good","title":"Row-based Vs Column-based File Formats"},{"location":"Storage%20Layer%20Choices/row%20based%20and%20column%20based%20file%20formats/#row-vs-column-file-format","text":"When you are designing big data solution, one fundamental que is \u2013 \u201chow data will be stored\u201d It involves taking into consideration 2 things \u2013 \u2022 File formats \u2022 Compression techniques Why do we need different File Formats? \u2022 To save storage \u2022 To do fast processing \u2022 To have less time in I/O operations \u2013 since we are dealing in big data, I/O operations are a big bottleneck, and so spending as less time in this area as possible. Our file formats help us in all 3 above if we go with right file format. There are a lot of choices available on file formats. Below are key aspects for deciding a file format - 1. Faster reads. 2. Faster writes. 3. Splittable - Some are designed in such a way that they are splittable \u2013 if a file is splittable, we can do parallel processing \u2013 in big data solutions, we consider only splittable file formats. 4. Schema evolution support \u2013 some of the file formats support schema evolution - i.e. to facilitate change in input data by allowing schema changes. 5. Advanced compression techniques 6. Most compatible platform - some work well with hive, some with spark, etc All the file formats have been divided into 2 broad categories \u2013 \u2022 Row based \u2022 Column based Row based \u2013 Here data is stored row-by-row. At a time, whole record is saved. If a new record comes, it gets appended at the end. So, writing a record is very easy coz you simply append at the end. Now let\u2019s talk about reading \u2013 While write is easy, but in order to get subset of columns, it has to read entire record. That is, performance is degraded when it comes to read. In data warehousing, wherein we scan specific set of records, row-based formats is not suggested. Regarding Compression of row-based file\u2013 Since data is stored record by record, so different data types are present together, next to each other. That means, compression is not as efficient as it could be. Column based file format \u2013 All column values are stored together. For Reading data in column-based file format \u2013 it allows us to skip data and read only relevant columns. Column based file format is suggested for data warehouse based query system. For write on column-based file, it is time consuming as you need to write on multiple places. Regarding Compression of column-based file\u2013 Since data is stored column by column, so same data types are present together, next to each other. That means, compression can be applied efficiently for each data type. To summarize \u2013 If you write once but read multiple times, go for column-based file format. If you read once, but write multiple times, go for row-based file format. Category row based column based read slower reads faster reads writes faster writes slower writes compression poor good","title":"Row Vs Column File Format"},{"location":"random/Amazon-Leadership-Principles-Jeff-Bezos-thought-process/","text":"Leaders are right, a lot Such people who are right a lot are hvaing following characteristics-> a. they listen a lot. b. they change their mind a lot. Why -> Life is complicated rules are complicated and so when you get more data, you change your mind. Sometimes you don't get data but just re-analyze a situation and change your mind. Anybody who doesn't change mind a lot underestimates the complexity of world we live in. c. they seek to disconfirm their most profoundly held convictions. Frugality Having small team size makes sense - so that essence of human organization - team work and cohesion comes up. If team size is large, leader should make more sub-leaders so that communication and performance can be effective. Hire smart people No single individual person can keep in touch with plethora of new things that are happening. So you need a recruitment process to hire and retain smart talented hard-working people who want to be part of your mission. What is Amazon's Mission : be the world's most customer-centric company Its a goal larger than Amazon. This involves 3 things -> listen, invent, personalize. - Companies that don't listen to customer cannot survive. - Companies that only listen to customer fail. They need to invent and find novice solutions to customer's problems. It is not customer job to invent, it is company's job and if they fail to do so, they cannot survive. - See each and every customer as centre of their own universe ~ personalize. On work=life balance. We should use then term work-life harmony . If you are happy at work, you come home as a better husband, father and person. And if you have harmony at home, you come to office as better employee. Organizations must strive to ensure harmony at workspace. Learn & be curious, bias for actions For most people harmony at work means finding work which is meaningful, interesting and impactful. Because of kind of challenges we have chosen for ourselves, we get to work for future and its super fun to work for the future. We work in environment where there is constant change. This is how high-tech domain is. If one doesnt enjoy this environment, they wont be able to sustain in long term. We need to build people, and it is human nature to focus on things that aren't working, and its incredibly hard to get people to take bold bets. That is something we need to encourage because bets are prone to failure, but those that succeed, they can compensate for dozens of things that didn't work. At amazon, it took some bold bets that worked out really well- Kindle, AWS, Prime, etc. Companies that don't embrace failure, they eventually get into position of desperation. Have backbone , disagree and commit | dive deep ; be skeptical when metrics and anecdote differ Generally assumed that companies need to eventually raise prices in order to increase profit margins. Amazon wants to lower price further because it sees things in terms of scale. Unlike physical stores wherein good service and low prices both aren't available, online stores can have both best service and lowest prices. Need to increase profit not in terms of % margins but in terms of dollar margins, i.e. sell more product - when you lower price, you attract more people and more sales - so more profit. Credits : Link","title":"Jeff Bezos on Amazon Leadership principles"},{"location":"random/Amazon-Leadership-Principles-Jeff-Bezos-thought-process/#leaders-are-right-a-lot","text":"Such people who are right a lot are hvaing following characteristics-> a. they listen a lot. b. they change their mind a lot. Why -> Life is complicated rules are complicated and so when you get more data, you change your mind. Sometimes you don't get data but just re-analyze a situation and change your mind. Anybody who doesn't change mind a lot underestimates the complexity of world we live in. c. they seek to disconfirm their most profoundly held convictions.","title":"Leaders are right, a lot"},{"location":"random/Amazon-Leadership-Principles-Jeff-Bezos-thought-process/#frugality","text":"Having small team size makes sense - so that essence of human organization - team work and cohesion comes up. If team size is large, leader should make more sub-leaders so that communication and performance can be effective.","title":"Frugality"},{"location":"random/Amazon-Leadership-Principles-Jeff-Bezos-thought-process/#hire-smart-people","text":"No single individual person can keep in touch with plethora of new things that are happening. So you need a recruitment process to hire and retain smart talented hard-working people who want to be part of your mission. What is Amazon's Mission : be the world's most customer-centric company Its a goal larger than Amazon. This involves 3 things -> listen, invent, personalize. - Companies that don't listen to customer cannot survive. - Companies that only listen to customer fail. They need to invent and find novice solutions to customer's problems. It is not customer job to invent, it is company's job and if they fail to do so, they cannot survive. - See each and every customer as centre of their own universe ~ personalize.","title":"Hire smart people"},{"location":"random/Amazon-Leadership-Principles-Jeff-Bezos-thought-process/#on-worklife-balance","text":"We should use then term work-life harmony . If you are happy at work, you come home as a better husband, father and person. And if you have harmony at home, you come to office as better employee. Organizations must strive to ensure harmony at workspace.","title":"On work=life balance."},{"location":"random/Amazon-Leadership-Principles-Jeff-Bezos-thought-process/#learn-be-curious-bias-for-actions","text":"For most people harmony at work means finding work which is meaningful, interesting and impactful. Because of kind of challenges we have chosen for ourselves, we get to work for future and its super fun to work for the future. We work in environment where there is constant change. This is how high-tech domain is. If one doesnt enjoy this environment, they wont be able to sustain in long term. We need to build people, and it is human nature to focus on things that aren't working, and its incredibly hard to get people to take bold bets. That is something we need to encourage because bets are prone to failure, but those that succeed, they can compensate for dozens of things that didn't work. At amazon, it took some bold bets that worked out really well- Kindle, AWS, Prime, etc. Companies that don't embrace failure, they eventually get into position of desperation.","title":"Learn &amp; be curious, bias for actions"},{"location":"random/Amazon-Leadership-Principles-Jeff-Bezos-thought-process/#have-backbone-disagree-and-commit-dive-deep-be-skeptical-when-metrics-and-anecdote-differ","text":"Generally assumed that companies need to eventually raise prices in order to increase profit margins. Amazon wants to lower price further because it sees things in terms of scale. Unlike physical stores wherein good service and low prices both aren't available, online stores can have both best service and lowest prices. Need to increase profit not in terms of % margins but in terms of dollar margins, i.e. sell more product - when you lower price, you attract more people and more sales - so more profit. Credits : Link","title":"Have backbone , disagree and commit | dive deep ; be skeptical when metrics and anecdote differ"},{"location":"random/ELK%20Stack/","text":"What is ELK Stack and why is it so popular for log management? The ELK Stack is composed of three open-source products. ELK stands for Elasticsearch, Logstash, and Kibana. \ud83d\udd39 Elasticsearch is a full-text search and analysis engine, leveraging Apache Lucene search engine as its core component. \ud83d\udd39 Logstash collects data from all kinds of edge collectors, then transforms that data and sends it to various destinations for further processing or visualization. In order to scale the edge data ingestion, a new product Beats is later developed as lightweight agents installed on edge hosts to collect and ship logs to Logstash. \ud83d\udd39 Kibana is a visualization layer with which users analyze and visualize the data. The diagram below shows how ELK Stack works: Step 1 - Beats collects data from various data sources. For example, Filebeat and Winlogbeat work with logs, and Packetbeat works with network traffic. Step 2 - Beats sends data to Logstash for aggregation and transformation. If we work with massive data, we can add a message queue (Kafka) to decouple the data producers and consumers. Step 3 - Logstash writes data into Elasticsearch for data indexing and storage. Step 4 - Kibana builds on top of Elasticsearch and provides users with various search tools and dashboards with which to visualize the data. ELK Stack is pretty convenient for troubleshooting and monitoring. It became popular by providing a simple and robust suite in the log analytics space, for a reasonable price. Credits : Alex Wu","title":"ELK"},{"location":"random/ELK%20Stack/#what-is-elk-stack-and-why-is-it-so-popular-for-log-management","text":"The ELK Stack is composed of three open-source products. ELK stands for Elasticsearch, Logstash, and Kibana. \ud83d\udd39 Elasticsearch is a full-text search and analysis engine, leveraging Apache Lucene search engine as its core component. \ud83d\udd39 Logstash collects data from all kinds of edge collectors, then transforms that data and sends it to various destinations for further processing or visualization. In order to scale the edge data ingestion, a new product Beats is later developed as lightweight agents installed on edge hosts to collect and ship logs to Logstash. \ud83d\udd39 Kibana is a visualization layer with which users analyze and visualize the data. The diagram below shows how ELK Stack works: Step 1 - Beats collects data from various data sources. For example, Filebeat and Winlogbeat work with logs, and Packetbeat works with network traffic. Step 2 - Beats sends data to Logstash for aggregation and transformation. If we work with massive data, we can add a message queue (Kafka) to decouple the data producers and consumers. Step 3 - Logstash writes data into Elasticsearch for data indexing and storage. Step 4 - Kibana builds on top of Elasticsearch and provides users with various search tools and dashboards with which to visualize the data. ELK Stack is pretty convenient for troubleshooting and monitoring. It became popular by providing a simple and robust suite in the log analytics space, for a reasonable price. Credits : Alex Wu","title":"What is ELK Stack and why is it so popular for log management?"},{"location":"random/ETL%20VS%20ELT/","text":"Database \u2022 Meant for Transactional data - OLTP (online transaction processing) \u2022 Such data should be Structured data \u2022 Meant for recent data - day to day data. Example - online banking transaction. \u2022 Common databases are Oracle, Mysql \u2022 It is Schema on Write \u2013 i.e. while writing any data into database, its data type and table structure gets validated, and if any mismatch occurs, it raises exception. \u2022 the cost to store the data in database is high. Datawarehouse \u2013 DWH \u2022 Its purpose is Analytical processing, wherein we require a lot of historical data to find the insights. \u2022 Querying in data warehouse involves writing complex queries that scans across history. \u2022 Why not use database for this purpose? o The moment we run complex queries on our database with an intent to do some analysis then your day to day transaction will become slow. \u2022 We take the data from databases and migrate it to Datawarehouse to do analytical processing. \u2022 we get the data from multiple sources. \u2022 Meant for Structured Data. \u2022 It is also Schema on write. Eg \u2013 Teradata \u2022 storage cost is high but lesser than your database. \u2022 In case of data warehouse, its an ETL process \u2013 i.e. data is extracted, then transformed, and finally Loaded into database. - Issue \u2013 we lose flexibility in terms of managing data. - This is because even before writing, we are to decide how to store it. We cant always look into future and plan use case of our data. Suppose your data is in database, wherein you -> - extract the data - Transform it (is a complex process) - Load it to Datawarehouse This approach reduces our flexibility. Data Lake \u2022 Its aim, like data warehouse, is to get insights from huge amount of data. \u2022 Here the data is present in its raw form. \u2022 It can be structured or unstructured. \u2022 Eg - Log File - we can directly have this file in raw form in data lake. \u2022 Its an ELT process - Extract Load & Transform. Eg - HDFS, Amazon S3 \u2022 Benefit \u2013 o Cost effective \u2013 cheapest storage o Schema on Read. o create structure to visualize or see the data. o Since data stored in raw form, it gives you enough flexibility.","title":"Database Vs Datawarehouse Vs Datalake | ETL Vs ELT"},{"location":"random/ETL%20VS%20ELT/#database","text":"\u2022 Meant for Transactional data - OLTP (online transaction processing) \u2022 Such data should be Structured data \u2022 Meant for recent data - day to day data. Example - online banking transaction. \u2022 Common databases are Oracle, Mysql \u2022 It is Schema on Write \u2013 i.e. while writing any data into database, its data type and table structure gets validated, and if any mismatch occurs, it raises exception. \u2022 the cost to store the data in database is high.","title":"Database"},{"location":"random/ETL%20VS%20ELT/#datawarehouse-dwh","text":"\u2022 Its purpose is Analytical processing, wherein we require a lot of historical data to find the insights. \u2022 Querying in data warehouse involves writing complex queries that scans across history. \u2022 Why not use database for this purpose? o The moment we run complex queries on our database with an intent to do some analysis then your day to day transaction will become slow. \u2022 We take the data from databases and migrate it to Datawarehouse to do analytical processing. \u2022 we get the data from multiple sources. \u2022 Meant for Structured Data. \u2022 It is also Schema on write. Eg \u2013 Teradata \u2022 storage cost is high but lesser than your database. \u2022 In case of data warehouse, its an ETL process \u2013 i.e. data is extracted, then transformed, and finally Loaded into database. - Issue \u2013 we lose flexibility in terms of managing data. - This is because even before writing, we are to decide how to store it. We cant always look into future and plan use case of our data. Suppose your data is in database, wherein you -> - extract the data - Transform it (is a complex process) - Load it to Datawarehouse This approach reduces our flexibility.","title":"Datawarehouse \u2013 DWH"},{"location":"random/ETL%20VS%20ELT/#data-lake","text":"\u2022 Its aim, like data warehouse, is to get insights from huge amount of data. \u2022 Here the data is present in its raw form. \u2022 It can be structured or unstructured. \u2022 Eg - Log File - we can directly have this file in raw form in data lake. \u2022 Its an ELT process - Extract Load & Transform. Eg - HDFS, Amazon S3 \u2022 Benefit \u2013 o Cost effective \u2013 cheapest storage o Schema on Read. o create structure to visualize or see the data. o Since data stored in raw form, it gives you enough flexibility.","title":"Data Lake"},{"location":"random/LinkedIn%20growth/","text":"Tip for Beginners If you are just starting on LinkedIn: Decide what topic you want to be known for Post content 1x per day on that specific topic Comment under large accounts 3-5x per day At 6 months go 2x per day w/ repurposing Start a newsletter at 15,000 followers Follow this for 12 months, and you'll have an audience of raving fans. An incredible asset in 2022. Credits : Justin Welsh","title":"LinkedIn Growth"},{"location":"random/LinkedIn%20growth/#tip-for-beginners","text":"If you are just starting on LinkedIn: Decide what topic you want to be known for Post content 1x per day on that specific topic Comment under large accounts 3-5x per day At 6 months go 2x per day w/ repurposing Start a newsletter at 15,000 followers Follow this for 12 months, and you'll have an audience of raving fans. An incredible asset in 2022. Credits : Justin Welsh","title":"Tip for Beginners"},{"location":"random/Top-10-hacks-to-be-a-Bad-Developer/","text":"Top 10 hacks to be a BAD developer Code first, design later. If people can't understand your code, they are not smart enough Never write unit tests and when you are forced to write them, write short and easy ones The only job of test coverage is to please engineering managers Take all suggestions in code reviews personally Always avoid documentation. Only work on it long after your project is complete. Logging, Metrics & Alerts are just there to add more story points. Your code is always reliable. Just get things to work once and push them to production quickly. Never actively monitor your changes in production. Don't read software documentation, only rely on internet blogs Never think about the end user, you are an engineer not a product freak Of course, above is meant for what you should not do at you job. Credits : Shivam Gupta","title":"Top 10 hacks to be a Bad developer"},{"location":"random/Top-10-hacks-to-be-a-Bad-Developer/#top-10-hacks-to-be-a-bad-developer","text":"Code first, design later. If people can't understand your code, they are not smart enough Never write unit tests and when you are forced to write them, write short and easy ones The only job of test coverage is to please engineering managers Take all suggestions in code reviews personally Always avoid documentation. Only work on it long after your project is complete. Logging, Metrics & Alerts are just there to add more story points. Your code is always reliable. Just get things to work once and push them to production quickly. Never actively monitor your changes in production. Don't read software documentation, only rely on internet blogs Never think about the end user, you are an engineer not a product freak Of course, above is meant for what you should not do at you job. Credits : Shivam Gupta","title":"Top 10 hacks to be a BAD developer"},{"location":"random/Work-Time-Motion-Study/","text":"Fredrick Taylor, often referred to as God who saved Capitalism from the clutches of Cummunism in 1920s, made a Scientific Study of doing work in industries. One of the key recommendation is with respect to Time-Work-Motion Study. He wanted answer to the question - How to increase Profits of organizations? Till that time, the mindset was that profits are directly proportional to number of hours spent by employee in factory, i.e. more he worked, more the organizations made items, and more the profts. This led to miserable working conditions in factories, wherein prolonged working hours drained the workers physically, emotionally and mentally. Taylor took a different view to address this problem. He wanted to increae profits, but not by making workers work for longer periods of time, but by working more efficiently. One fo the principles he gave, and is our discussion item today, is Work-Time-Motion Study .","title":"Work Time Motion Study"},{"location":"random/cold_mails/","text":"Cold Mails Tactics \ud83d\udfe2 Run the triple: Start every sequence with an email, social touch, and cold call. \ud83d\udfe2 5x5x5: 5 minutes of research. Find 5 insights on the person/account. Write your message in 5 minutes. \ud83d\udfe2 Subject lines shouldn\u2019t read like an advertisement. They should be 4 words max, not a complete sentence, and without punctuation. \ud83d\udfe2 Don't equate formality with professionalism. You don't need to address me as \"Dear Mr. Farrokh\"). Use conjunctions and conversational language \ud83d\udfe2 Follow the 3x3 Rule for email: 3 paragraphs, no more than 3 lines each when read on your phone. \ud83d\udfe2 Don't try to sell everything under the sun in your email. One pain point per email, or you risk losing them. Mix in multiple pain points across the sequence. \ud83d\udfe2 Avoid large images, multiple hyperlinks, and big attachments. These add-ons scream \"marketing email\" and hurt deliverability. \ud83d\udfe2 Use Interest-based CTAs. \"Open to learning more?\" is better than \u201cHow's Thursday at 4:00 for a discovery call?\u201d \ud83d\udfe2 Your emails should read like text messages. If you read it out loud and sound like a robot, you\u2019re doing it wrong. \ud83d\udfe2 Tie VMs to emails: Leave voicemails that reference your emails and vice-versa. Even if they never get answered, voicemails should boost email replies. Credits : Armand Farokh","title":"Cold Mail Tactics"},{"location":"random/cold_mails/#cold-mails-tactics","text":"\ud83d\udfe2 Run the triple: Start every sequence with an email, social touch, and cold call. \ud83d\udfe2 5x5x5: 5 minutes of research. Find 5 insights on the person/account. Write your message in 5 minutes. \ud83d\udfe2 Subject lines shouldn\u2019t read like an advertisement. They should be 4 words max, not a complete sentence, and without punctuation. \ud83d\udfe2 Don't equate formality with professionalism. You don't need to address me as \"Dear Mr. Farrokh\"). Use conjunctions and conversational language \ud83d\udfe2 Follow the 3x3 Rule for email: 3 paragraphs, no more than 3 lines each when read on your phone. \ud83d\udfe2 Don't try to sell everything under the sun in your email. One pain point per email, or you risk losing them. Mix in multiple pain points across the sequence. \ud83d\udfe2 Avoid large images, multiple hyperlinks, and big attachments. These add-ons scream \"marketing email\" and hurt deliverability. \ud83d\udfe2 Use Interest-based CTAs. \"Open to learning more?\" is better than \u201cHow's Thursday at 4:00 for a discovery call?\u201d \ud83d\udfe2 Your emails should read like text messages. If you read it out loud and sound like a robot, you\u2019re doing it wrong. \ud83d\udfe2 Tie VMs to emails: Leave voicemails that reference your emails and vice-versa. Even if they never get answered, voicemails should boost email replies. Credits : Armand Farokh","title":"Cold Mails Tactics"},{"location":"random/pytest/","text":"PyTest is a python unit testing framework. It provides the ability to create Tests, Test Modules, Test Classes, and Test Fixtures. It uses the built-in python assert statement which makes implementing unit tests much simpler than other frameworks. It also adds many useful command line arguments to help specify what tests should be run and in what order. Creating a Test Tests are python functions with \u201ctest\u201d at the beginning of the function name. Tests do verification of values using the standard python assert statement. Similar tests can be grouped together by including them in the same module or class # test_SomeFunction.py def test_SomeFunction(): assert 1 == 1 Test Discovery Pytest will automatically find your tests when you run it from the command line using several naming rules for the test files, test classes, and test functions. Test function names should begin with \u201ctest\u201d. Classes with tests in them should have the word \u201cTest\u201d with a capital T at the beginning of the class name. These classes should also have no \u201cinit\u201d method. The filenames for test modules should start with \u201ctest_\u201d or end with \u201c_test\u201d. XUNIT Style setup and tear down One key feature of all unit test frameworks is providing the ability to execute setup code before and after the tests. Pytest provides this capability with both XUnit style setup/teardown functions and with Pytest fixtures. The XUnit style setup and teardown functions allow you to execute code before and after: Test modules , Test Functions , Test Classes , and Test Methods in Test Classes. Using these setup and teardown functions can help reduce code duplication by letting you specify setup and teardown code once at each of the different levels as necessary rather than repeating the code in each individual unit test. This can help keep your code clean and manageable. Lets look at some examples to see how this works. def setup_module(): def teardown_module(): def setup_function(): def teardown_function(): def setup_class(): def teardown_class(): def setup_method(): def teardown_method(): To run below code use command : pytest -v -s class TestClass: @classmethod def setup_class(cls): print('\\n setup up TestClass') @classmethod def teardown_class(cls): print('\\n tear down TestClass') def setup_method(self, method): if method==self.test1: print ('\\n setting up test1') elif method==self.test2: print ('\\n setting up test2') else: print('\\n setting up unkown test') def teardown_method(self, method): if method==self.test1: print ('\\n tear down test1') elif method==self.test2: print ('\\n tear down test2') else: print('\\n tear down unkown test') def test1(self): print ('\\n Executing Test1') assert True def test2(self): print ('\\n Executing Test2') assert True PyTest Fixtures Like the XUnit style of setup and teardown functions, Test fixtures allow for re-use of code across tests by specifying functions that should be executed before the unit test runs. Specifying that a function is a test fixture is done by applying the \u201cpytest.fixture\u201d decorator to the function. Individual unit tests can specify they want to use that function by specifying it in their parameter list or by using the \u201cpytest.mark.usefixture\u201d decorator. The fixture can also set its autouse parameter to true which will cause all tests in the fixture\u2019s scope to automatically execute the fixture before the test executes. Lets look at an example. @pytest.fixture(): def math(): return Math() def test_Add(math): assert math.add(1,1) == 2 import pytest @pytest.fixture() def setup(): print ('\\n Setup') # below setup fixture gets called before test1 executes # This is because we passed setup as parameter to out test1() function def test1(setup): print ('Executing test1') assert True # below setup fixture doesn't gets called before test2 executes def test2(): print ('Executing test2') assert True # below setup fixture gets called before test1 executes # This is because we passed setup via decorator using below syntax. @pytest.mark.usefixtures(\"setup\") def test3(): print ('Executing test3') assert True Output test_fixture_example1.py::test1 Setup Executing test1 PASSED test_fixture_example1.py::test2 Executing test2 PASSED test_fixture_example1.py::test3 Setup Executing test3 PASSED Note - It would be cumbersome to pass fixture to each test manually. Instead, we use autouse functionality so that fixture gets called before each test. Eg - @pytest.fixture(autouse=True) def setup(): print ('\\n Setup') Test Fixture Teardown Often there is some type of teardown or cleanup that a test, class, or module need to perform after testing has been completed. Each test fixture can specify their own teardown code that should be executed. There are two methods of specifying a teardown code for a test fixture: The \u201c yield \u201d keyword and The request-context object\u2019s \u201c addfinalizer \u201d method. Test Fixture Teardown - Yield - The yield keyword is the simpler of the two options for teardown code. - The code after the yield is executed after the fixture goes out of scope. - The yield keyword is a replacement for return and any return values should be passed to it. @pytest.fixture(): def setup(): print(\u201cSetup!\u201d) yield print(\u201cTeardown!\u201d) Test Fixture Teardown - addfinalizer - The addfinalizer method of adding teardown code is a little more complicated but also a little more capable than the yield statement. - With the addfinalizer method one or more finalizer functions are added via the request-context\u2019s addfinalizer method. - One of the big differences between this method and the yield keyword method is that this method allows for multiple finalization functions to be specified. - Now lets take a look at some examples. @pytest.fixture(): def setup(request): print(\u201cSetup!\u201d) def teardown: print(\u201cTeardown!\u201d) request.addfinalizer(teardown) Example of 2 teardown methods import pytest @pytest.fixture() def setup1(): print ('\\n Setup 1') yield print ('\\n Teardown 1') @pytest.fixture() def setup2(request): print ('\\n Setup 2') def teardown_2_a(): print ('\\n Teardown 2A called') def teardown_2_b(): print ('\\n Teardown 2B called') request.addfinalizer(teardown_2_a) request.addfinalizer(teardown_2_b) @pytest.mark.usefixtures(\"setup1\") def test1(): print ('Executing test1') assert True @pytest.mark.usefixtures(\"setup2\") def test2(): print ('Executing test2') assert True Output test_fixture_example2.py::test1 Setup 1 Executing test1 PASSED Teardown 1 test_fixture_example2.py::test2 Setup 2 Executing test2 PASSED Teardown 2B called Teardown 2A called Test Fixtures Scope Which tests a test fixture applies to and how often it is run depends on the fixture\u2019s scope. Test fixtures have four different scopes: Function - Run the fixture once for each test Class - Run the fixture once for each class of tests Module - Run once when the module goes in scope Session - The fixture is run when pytest starts. By default, the scope is set to function and this specifies that the fixture should be called for all tests in the module. Class scope specifies the test fixture should be executed once per test class. Module scope specifies that the fixture should be executed once per module. Session scope specifies that the fixture should be executed once when PyTest starts. Eg - code 1 : test_fixture_example3_scope_of_fixtures.py import pytest @pytest.fixture(scope='session', autouse=True) def setupSession(): print (\"\\n Setup Session\") yield print(\"\\n Teardown Session\") @pytest.fixture(scope='module', autouse=True) def setupModule(): print (\"\\n Setup Module\") yield print(\"\\n Teardown Module\") @pytest.fixture(scope='function', autouse=True) def setupFunction(): print (\"\\n Setup Function\") yield print(\"\\n Teardown Function\") def test1(): print ('Executing Test 1') assert True def test2(): print ('Executing Test 2') assert True Eg - code 2 : test_fixture_example4_scope_of_fixtures.py import pytest @pytest.fixture(scope='module', autouse=True) def setupModule(): print (\"\\n Setup Module2\") yield print(\"\\n Teardown Module2\") @pytest.fixture(scope='class', autouse=True) def setupClass(): print (\"\\n Setup Class2\") yield print(\"\\n Teardown Class2\") @pytest.fixture(scope='function', autouse=True) def setupFunction(): print (\"\\n Setup Function2\") yield print(\"\\n Teardown Function2\") class TestClass: def test1(self): print ('Executing Test 1') assert True def test2(self): print ('Executing Test 2') assert True When both are together run for pytest, the output is -> test_fixture_example3_scope_of_fixtures.py::test1 Setup Session Setup Module Setup Function Executing Test 1 PASSED Teardown Function test_fixture_example3_scope_of_fixtures.py::test2 Setup Function Executing Test 2 PASSED Teardown Function Teardown Module test_fixture_example4_scope_of_fixtures.py::TestClass::test1 Setup Module2 Setup Class2 Setup Function2 Executing Test 1 PASSED Teardown Function2 test_fixture_example4_scope_of_fixtures.py::TestClass::test2 Setup Function2 Executing Test 2 PASSED Teardown Function2 Teardown Class2 Teardown Module2 Teardown Session Test Fixture Return Objects and Params PyTest Test Fixtures allow you to optionally return data from the fixture that can be used in the test. The optional params array argument in the fixture decorator can be used to specify one or more values that should be passed to the test. When a params argument has multiple values then the test will be called once with each value . Let's look at a working example. import pytest @pytest.fixture(params=[1,2,3]) def setup(request): return_val = request.param print ('\\n Setup return_val = {}'.format(return_val)) return return_val # Note: we didn't use @pytest.mark.usefixtures(\"setup\") here as we needed to access value of setup fixture. def test1(setup): print ('Executing test1') print ('Setup return value inside test function = ',format(setup)) assert True Output test_fixture_example5_return_objects_and_params.py::test1[1] Setup return_val = 1 Executing test1 Setup return value inside test function = 1 PASSED test_fixture_example5_return_objects_and_params.py::test1[2] Setup return_val = 2 Executing test1 Setup return value inside test function = 2 PASSED test_fixture_example5_return_objects_and_params.py::test1[3] Setup return_val = 3 Executing test1 Setup return value inside test function = 3 PASSED Using Assert and Testing Exceptions Using the assert Statement Pytest allows the use of the built in python assert statement for performing verifications in a unit test. The normal comparison operators can be used on all python data types: <, >,<=, >=, ==, and != Pytest expands on the messages that are reported for assert failures to provide more context in the test results. import pytest def test_IntAssert(): assert 1 == 1 def test_StrAssert(): assert \"str\" == \"str\" def test_floatAssert(): assert 1.0 == 1.0 def test_arrayAssert(): assert [1,2,3] == [1,2,3] def test_dictAssert(): assert {\"1\":1} == {\"1\":1} Output collected 6 items src/test/test_assert_example.py::test_IntAssert PASSED src/test/test_assert_example.py::test_StrAssert PASSED src/test/test_assert_example.py::test_floatAssert PASSED src/test/test_assert_example.py::test_arrayAssert PASSED src/test/test_assert_example.py::test_dictAssert PASSED src/test/test_exception_example.py::test_exception PASSED Comparing Floating Point Values - Validating floating point values can sometimes be difficult as internally the value is stored as a series of binary fractions. - Because of this some comparisons that we\u2019d expect to pass will fail. - Pytest provides the \u201capprox\u201d function which will validate that two floating point values are \u201capproximately\u201d the same value as each other to within a default tolerance of 1 time E to the -6 value. from pytest import approx # Failing Test def test_BadFloatCompare(): assert (0.1 + 0.2) == 0.3 # Passing Test def test_GoodFloatCompare(): val = 0.1 + 0.2 assert val == approx(0.3) Verifying Exceptions In some test cases we need to verify that a function raises an exception under certain conditions. Pytest provides the raises helper to perform this verification using the \u201cwith\u201d keyword. When the \u201craises\u201d helper is used the unit test will fail if the specified exception is not thrown in the code block after the \u201craises line. from pytest import raises def raisesValueException(): raise ValueError def test_exception(): with raises(ValueError): raisesValueException() PyTest Command Line Arguments Specifying What Tests Should Run By default, Pytest runs all tests that it finds in the current working directory and sub-directory using the naming conventions for automatic test discovery. There are several pytest command line arguments that can be specified to try and be more selective about which tests will be executed. moduleName - You can simply pass in the module name to execute only the unit tests in one particular module. DirectoryName - You can also simply pass in a directory path to have pytest run only the tests in that directory. You can use the \u201c-k\u201d option to specify an evaluation string based on keywords such as: module name, class name, and function name. You can use the \u201c-m\u201d option to specify that any tests that have a \u201cpytest.mark\u201d decorator that matches the specified expression string will be executed. Examples # 1 : DirectoryName pytest -v -s path_to_my_directory/ #2 : -k option : runs all test having keyword test1 inside them. pytest -v -s -k \"test1\" #3 : -k option : runs all test having keyword test1 or test2 inside them. pytest -v -s -k \"test1 or test2\" #4 : -m option define your tests by adding pytest.mark as shown below-> @pytest.mark.test1 def test1(): print ('executing test1') assert True @pytest.mark.test2 def test2(): print ('executing test2') assert True pytest -v -s -m \"test1 or test2\" Some additional command line arguments that can be very useful. - -v option specifies that verbose output from pytest should be enabled. - -q option specifies the opposite. It specifies that the tests should be run quietly (or minimal output). This can be helpful from a performance perspective when you\u2019re running 100\u2019s or 1000\u2019s of tests. - -s option specifies that PyTest should NOT capture the console output. - \u2014ignore option allows you to specify a path that should be ignore during test discovery. - \u2014maxfail option specifies that PyTest should stop after N number of test failures.","title":"PyTest overview"},{"location":"random/pytest/#creating-a-test","text":"Tests are python functions with \u201ctest\u201d at the beginning of the function name. Tests do verification of values using the standard python assert statement. Similar tests can be grouped together by including them in the same module or class # test_SomeFunction.py def test_SomeFunction(): assert 1 == 1","title":"Creating a Test"},{"location":"random/pytest/#test-discovery","text":"Pytest will automatically find your tests when you run it from the command line using several naming rules for the test files, test classes, and test functions. Test function names should begin with \u201ctest\u201d. Classes with tests in them should have the word \u201cTest\u201d with a capital T at the beginning of the class name. These classes should also have no \u201cinit\u201d method. The filenames for test modules should start with \u201ctest_\u201d or end with \u201c_test\u201d.","title":"Test Discovery"},{"location":"random/pytest/#xunit-style-setup-and-tear-down","text":"One key feature of all unit test frameworks is providing the ability to execute setup code before and after the tests. Pytest provides this capability with both XUnit style setup/teardown functions and with Pytest fixtures. The XUnit style setup and teardown functions allow you to execute code before and after: Test modules , Test Functions , Test Classes , and Test Methods in Test Classes. Using these setup and teardown functions can help reduce code duplication by letting you specify setup and teardown code once at each of the different levels as necessary rather than repeating the code in each individual unit test. This can help keep your code clean and manageable. Lets look at some examples to see how this works. def setup_module(): def teardown_module(): def setup_function(): def teardown_function(): def setup_class(): def teardown_class(): def setup_method(): def teardown_method(): To run below code use command : pytest -v -s class TestClass: @classmethod def setup_class(cls): print('\\n setup up TestClass') @classmethod def teardown_class(cls): print('\\n tear down TestClass') def setup_method(self, method): if method==self.test1: print ('\\n setting up test1') elif method==self.test2: print ('\\n setting up test2') else: print('\\n setting up unkown test') def teardown_method(self, method): if method==self.test1: print ('\\n tear down test1') elif method==self.test2: print ('\\n tear down test2') else: print('\\n tear down unkown test') def test1(self): print ('\\n Executing Test1') assert True def test2(self): print ('\\n Executing Test2') assert True","title":"XUNIT Style setup and tear down"},{"location":"random/pytest/#pytest-fixtures","text":"Like the XUnit style of setup and teardown functions, Test fixtures allow for re-use of code across tests by specifying functions that should be executed before the unit test runs. Specifying that a function is a test fixture is done by applying the \u201cpytest.fixture\u201d decorator to the function. Individual unit tests can specify they want to use that function by specifying it in their parameter list or by using the \u201cpytest.mark.usefixture\u201d decorator. The fixture can also set its autouse parameter to true which will cause all tests in the fixture\u2019s scope to automatically execute the fixture before the test executes. Lets look at an example. @pytest.fixture(): def math(): return Math() def test_Add(math): assert math.add(1,1) == 2 import pytest @pytest.fixture() def setup(): print ('\\n Setup') # below setup fixture gets called before test1 executes # This is because we passed setup as parameter to out test1() function def test1(setup): print ('Executing test1') assert True # below setup fixture doesn't gets called before test2 executes def test2(): print ('Executing test2') assert True # below setup fixture gets called before test1 executes # This is because we passed setup via decorator using below syntax. @pytest.mark.usefixtures(\"setup\") def test3(): print ('Executing test3') assert True Output test_fixture_example1.py::test1 Setup Executing test1 PASSED test_fixture_example1.py::test2 Executing test2 PASSED test_fixture_example1.py::test3 Setup Executing test3 PASSED Note - It would be cumbersome to pass fixture to each test manually. Instead, we use autouse functionality so that fixture gets called before each test. Eg - @pytest.fixture(autouse=True) def setup(): print ('\\n Setup')","title":"PyTest Fixtures"},{"location":"random/pytest/#test-fixture-teardown","text":"Often there is some type of teardown or cleanup that a test, class, or module need to perform after testing has been completed. Each test fixture can specify their own teardown code that should be executed. There are two methods of specifying a teardown code for a test fixture: The \u201c yield \u201d keyword and The request-context object\u2019s \u201c addfinalizer \u201d method. Test Fixture Teardown - Yield - The yield keyword is the simpler of the two options for teardown code. - The code after the yield is executed after the fixture goes out of scope. - The yield keyword is a replacement for return and any return values should be passed to it. @pytest.fixture(): def setup(): print(\u201cSetup!\u201d) yield print(\u201cTeardown!\u201d) Test Fixture Teardown - addfinalizer - The addfinalizer method of adding teardown code is a little more complicated but also a little more capable than the yield statement. - With the addfinalizer method one or more finalizer functions are added via the request-context\u2019s addfinalizer method. - One of the big differences between this method and the yield keyword method is that this method allows for multiple finalization functions to be specified. - Now lets take a look at some examples. @pytest.fixture(): def setup(request): print(\u201cSetup!\u201d) def teardown: print(\u201cTeardown!\u201d) request.addfinalizer(teardown) Example of 2 teardown methods import pytest @pytest.fixture() def setup1(): print ('\\n Setup 1') yield print ('\\n Teardown 1') @pytest.fixture() def setup2(request): print ('\\n Setup 2') def teardown_2_a(): print ('\\n Teardown 2A called') def teardown_2_b(): print ('\\n Teardown 2B called') request.addfinalizer(teardown_2_a) request.addfinalizer(teardown_2_b) @pytest.mark.usefixtures(\"setup1\") def test1(): print ('Executing test1') assert True @pytest.mark.usefixtures(\"setup2\") def test2(): print ('Executing test2') assert True Output test_fixture_example2.py::test1 Setup 1 Executing test1 PASSED Teardown 1 test_fixture_example2.py::test2 Setup 2 Executing test2 PASSED Teardown 2B called Teardown 2A called","title":"Test Fixture Teardown"},{"location":"random/pytest/#test-fixtures-scope","text":"Which tests a test fixture applies to and how often it is run depends on the fixture\u2019s scope. Test fixtures have four different scopes: Function - Run the fixture once for each test Class - Run the fixture once for each class of tests Module - Run once when the module goes in scope Session - The fixture is run when pytest starts. By default, the scope is set to function and this specifies that the fixture should be called for all tests in the module. Class scope specifies the test fixture should be executed once per test class. Module scope specifies that the fixture should be executed once per module. Session scope specifies that the fixture should be executed once when PyTest starts. Eg - code 1 : test_fixture_example3_scope_of_fixtures.py import pytest @pytest.fixture(scope='session', autouse=True) def setupSession(): print (\"\\n Setup Session\") yield print(\"\\n Teardown Session\") @pytest.fixture(scope='module', autouse=True) def setupModule(): print (\"\\n Setup Module\") yield print(\"\\n Teardown Module\") @pytest.fixture(scope='function', autouse=True) def setupFunction(): print (\"\\n Setup Function\") yield print(\"\\n Teardown Function\") def test1(): print ('Executing Test 1') assert True def test2(): print ('Executing Test 2') assert True Eg - code 2 : test_fixture_example4_scope_of_fixtures.py import pytest @pytest.fixture(scope='module', autouse=True) def setupModule(): print (\"\\n Setup Module2\") yield print(\"\\n Teardown Module2\") @pytest.fixture(scope='class', autouse=True) def setupClass(): print (\"\\n Setup Class2\") yield print(\"\\n Teardown Class2\") @pytest.fixture(scope='function', autouse=True) def setupFunction(): print (\"\\n Setup Function2\") yield print(\"\\n Teardown Function2\") class TestClass: def test1(self): print ('Executing Test 1') assert True def test2(self): print ('Executing Test 2') assert True When both are together run for pytest, the output is -> test_fixture_example3_scope_of_fixtures.py::test1 Setup Session Setup Module Setup Function Executing Test 1 PASSED Teardown Function test_fixture_example3_scope_of_fixtures.py::test2 Setup Function Executing Test 2 PASSED Teardown Function Teardown Module test_fixture_example4_scope_of_fixtures.py::TestClass::test1 Setup Module2 Setup Class2 Setup Function2 Executing Test 1 PASSED Teardown Function2 test_fixture_example4_scope_of_fixtures.py::TestClass::test2 Setup Function2 Executing Test 2 PASSED Teardown Function2 Teardown Class2 Teardown Module2 Teardown Session","title":"Test Fixtures Scope"},{"location":"random/pytest/#test-fixture-return-objects-and-params","text":"PyTest Test Fixtures allow you to optionally return data from the fixture that can be used in the test. The optional params array argument in the fixture decorator can be used to specify one or more values that should be passed to the test. When a params argument has multiple values then the test will be called once with each value . Let's look at a working example. import pytest @pytest.fixture(params=[1,2,3]) def setup(request): return_val = request.param print ('\\n Setup return_val = {}'.format(return_val)) return return_val # Note: we didn't use @pytest.mark.usefixtures(\"setup\") here as we needed to access value of setup fixture. def test1(setup): print ('Executing test1') print ('Setup return value inside test function = ',format(setup)) assert True Output test_fixture_example5_return_objects_and_params.py::test1[1] Setup return_val = 1 Executing test1 Setup return value inside test function = 1 PASSED test_fixture_example5_return_objects_and_params.py::test1[2] Setup return_val = 2 Executing test1 Setup return value inside test function = 2 PASSED test_fixture_example5_return_objects_and_params.py::test1[3] Setup return_val = 3 Executing test1 Setup return value inside test function = 3 PASSED","title":"Test Fixture Return Objects and Params"},{"location":"random/pytest/#using-assert-and-testing-exceptions","text":"","title":"Using Assert and Testing Exceptions"},{"location":"random/pytest/#using-the-assert-statement","text":"Pytest allows the use of the built in python assert statement for performing verifications in a unit test. The normal comparison operators can be used on all python data types: <, >,<=, >=, ==, and != Pytest expands on the messages that are reported for assert failures to provide more context in the test results. import pytest def test_IntAssert(): assert 1 == 1 def test_StrAssert(): assert \"str\" == \"str\" def test_floatAssert(): assert 1.0 == 1.0 def test_arrayAssert(): assert [1,2,3] == [1,2,3] def test_dictAssert(): assert {\"1\":1} == {\"1\":1} Output collected 6 items src/test/test_assert_example.py::test_IntAssert PASSED src/test/test_assert_example.py::test_StrAssert PASSED src/test/test_assert_example.py::test_floatAssert PASSED src/test/test_assert_example.py::test_arrayAssert PASSED src/test/test_assert_example.py::test_dictAssert PASSED src/test/test_exception_example.py::test_exception PASSED Comparing Floating Point Values - Validating floating point values can sometimes be difficult as internally the value is stored as a series of binary fractions. - Because of this some comparisons that we\u2019d expect to pass will fail. - Pytest provides the \u201capprox\u201d function which will validate that two floating point values are \u201capproximately\u201d the same value as each other to within a default tolerance of 1 time E to the -6 value. from pytest import approx # Failing Test def test_BadFloatCompare(): assert (0.1 + 0.2) == 0.3 # Passing Test def test_GoodFloatCompare(): val = 0.1 + 0.2 assert val == approx(0.3)","title":"Using the assert Statement"},{"location":"random/pytest/#verifying-exceptions","text":"In some test cases we need to verify that a function raises an exception under certain conditions. Pytest provides the raises helper to perform this verification using the \u201cwith\u201d keyword. When the \u201craises\u201d helper is used the unit test will fail if the specified exception is not thrown in the code block after the \u201craises line. from pytest import raises def raisesValueException(): raise ValueError def test_exception(): with raises(ValueError): raisesValueException()","title":"Verifying Exceptions"},{"location":"random/pytest/#pytest-command-line-arguments","text":"","title":"PyTest Command Line Arguments"},{"location":"random/pytest/#specifying-what-tests-should-run","text":"By default, Pytest runs all tests that it finds in the current working directory and sub-directory using the naming conventions for automatic test discovery. There are several pytest command line arguments that can be specified to try and be more selective about which tests will be executed. moduleName - You can simply pass in the module name to execute only the unit tests in one particular module. DirectoryName - You can also simply pass in a directory path to have pytest run only the tests in that directory. You can use the \u201c-k\u201d option to specify an evaluation string based on keywords such as: module name, class name, and function name. You can use the \u201c-m\u201d option to specify that any tests that have a \u201cpytest.mark\u201d decorator that matches the specified expression string will be executed. Examples # 1 : DirectoryName pytest -v -s path_to_my_directory/ #2 : -k option : runs all test having keyword test1 inside them. pytest -v -s -k \"test1\" #3 : -k option : runs all test having keyword test1 or test2 inside them. pytest -v -s -k \"test1 or test2\" #4 : -m option define your tests by adding pytest.mark as shown below-> @pytest.mark.test1 def test1(): print ('executing test1') assert True @pytest.mark.test2 def test2(): print ('executing test2') assert True pytest -v -s -m \"test1 or test2\" Some additional command line arguments that can be very useful. - -v option specifies that verbose output from pytest should be enabled. - -q option specifies the opposite. It specifies that the tests should be run quietly (or minimal output). This can be helpful from a performance perspective when you\u2019re running 100\u2019s or 1000\u2019s of tests. - -s option specifies that PyTest should NOT capture the console output. - \u2014ignore option allows you to specify a path that should be ignore during test discovery. - \u2014maxfail option specifies that PyTest should stop after N number of test failures.","title":"Specifying What Tests Should Run"}]}