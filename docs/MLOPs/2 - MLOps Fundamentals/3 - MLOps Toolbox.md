MLOps Toolbox

![img_2.png](images/img_2.png)
 
<br>

We will learn about the essential MLOps libraries in the data exploration and model training part. <br>

We can use different **Development Tools** such as Jupyter Notebooks, PyCharm or Visual Studio. These are some of the most used. <br>
Some other libraries are no longer used. For example Spyder. <br>
 <br>
We will usually use GitHub, GitLab or Bitbucket for the **Versioning of the code**. <br>
 <br>
For **Data Labeling**, we will use tools such as V7 or Label Box. <br>
V7 is a powerful automated annotation platform for computer vision. <br>
V7 combines dataset management, image annotation, video annotation and AutoML model training to complete labeling tasks automatically. <br>
 <br>
For the **Visualization and Exploration of data**, we can use visualization libraries such as bokeh, matplotlib or Seaborn. <br>
 <br>
As for **Feature Engineering**, this will depend on the type of model and the programming languages we use. <br>
Usually we use the scikit-learn Library to perform feature engineering with Python, but we also have other libraries such as the _featuretools_ or the _feast_ library. <br>
 <br>
Once it is done, we will move on to **Model Training**. <br>
The conventional libraries for training models are scikit-learn for Machine Learning and TensorFlow and Keras for deep learning. <br>
 <br>
Once the model is developed, we will move on to **Model Debugging**. <br>
For this we can use TensorBoard, interpretML or SHAP.
 <br>
 <br>
We would use libraries such as Optima, Tune or KerasTuner to **optimize this model**. <br>
 <br>
In ML ops, it is vital to **register the models**. <br>
Therefore, we would go on to perform the Model Track. <br>
One of the best known and newest book stores is ML Flow, but we also have others like Comet, ClearML and TensorBoard. <br>
 <br>
Once we have tracked the experiment and the versioning, we will move on to the **Model Packaging**. <br>
Here we can use Kubeflow, mlflow, ONNX or BentoML.  <br>
ONNX provides an open source format for AI models, both deep learning and traditional ML enabling interoperability between different frameworks. <br>
 <br>
Once the model was packaged, we would go to model serving. <br>
**Model serving** seeks to make the model accessible so the user can use it. <br>
We can use Amazon Sage Maker, FastAPI or BentoML. <br>
 <br>
Once we have deployed a model in production, we must **orchestrate the workflow**. <br>
To do this we can use Apache airflow, ZenML or Kubeflow. <br>
 <br>
For the **monitoring of the model**, we can use Seldon or Verta. <br>
To ensure that the model continues to work over time, we must monitor it. <br>
 <br>
As we had mentioned, it is not enough to version the model. <br>
You also have to version the data. <br>
The most commonly used library for **data versioning** is DVC. <br>
We also have the bookstores of Pachyderm, Quilt and Comet, among others. <br>
 <br>
Finally, we may want to **develop a model for an application process**. <br>
In this case, we can resort to streamlit or Dash. <br>
Both tools allow you to create machine learning web applications in Python with few lines of code. <br>
 <br>
Once this is finished, we also have the option to **develop a POC** of an application. <br>
It can be developed quickly with a streamlit library, a library specially designed for science and data to generate visual applications with very little code and integrates seamlessly with Python. <br>

